{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning for Text 2  \n",
    "&nbsp;\n",
    "\n",
    "\n",
    "Ayoub Bagheri, <a.bagheri@uu.nl>  \n",
    "\n",
    "\n",
    "<img src=\"img/uu_logo.png\" style=\"float: right;\" width=\"100\" height=\"100\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture‚Äôs Plan\n",
    "&nbsp;\n",
    "\n",
    "1. CNN\n",
    "2. Encoder-Decoder\n",
    "3. Attention\n",
    "4. Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNN for Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "&nbsp;\n",
    "\n",
    "- Convolutional Neural Networks, or Convolutional Networks, or CNNs, or ConvNets\n",
    "- For processing data with a **grid-like** or array topology\n",
    "    - 1-D grid: time-series data, sensor signal data\n",
    "    - 2-D grid: image data\n",
    "    - 3-D grid: video data\n",
    "\n",
    "- CNNs include four key ideas related to natural signals: \n",
    "    - **Local connections**\n",
    "    - **Shared weights**\n",
    "    - **Pooling**\n",
    "    - **Use of many layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNN Architecture\n",
    "&nbsp;\n",
    "\n",
    "- Intuition: Neural network with specialized connectivity structure\n",
    "    - Stacking multiple layers of feature extractors, low-level layers extract local features, and high-level layers extract learn global patterns.\n",
    "- There are a few distinct types of layers:\n",
    "    - **Convolutional Layer**: detecting local features through filters (discrete convolution)\n",
    "    - **Non-linear Layer**: normalization via Rectified Linear Unit (ReLU)\n",
    "    - **Pooling Layer**: merging similar features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building-blocks for CNNs\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (1) Convolutional Layer\n",
    "&nbsp;\n",
    "\n",
    "- The core layer of CNNs\n",
    "- Convolutional layer consists of a set of filters, $W_{kl}$\n",
    "- Each filter covers a spatially small portion of the input data, $Z_{i,j}$\n",
    "- Each filter is convolved across the dimensions of the input data, producing a multidimensional **feature map**.\n",
    "- As we convolve the filter, we are computing the dot product between the parameters of the filter and the input.\n",
    "- **Deep Learning algorithm**: During training, the network corrects errors and filters are **learned**, e.g., in Keras, by adjusting weights based on **Stochastic Gradient Descent**, **SGD** (stochastic approximation of GD using a randomly selected subset of the data).\n",
    "- The key architectural characteristics of the convolutional layer is **local connectivity** and **shared weights**.\n",
    "\n",
    "<img src=\"img/page 7.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Layer: Local Connectivity\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left; width:60%\">\n",
    "    <ul>\n",
    "        <li>Neurons in layer m are only connected to 3 adjacent neurons in the m-1 layer.</li>\n",
    "        <li>Neurons in layer m+1 have a similar connectivity with the layer below.</li>\n",
    "        <li>Each neuron is unresponsive to variations outside of its <em>receptive field</em> with respect to the input. </li>\n",
    "        <ul><li>Receptive field: small neuron collections which process portions of the input data.</li></ul>\n",
    "        <li>The architecture thus ensures that the learnt feature extractors produce the strongest response to a spatially local input pattern.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "<div style=\"float:left; width:40%\">\n",
    "    <img src=\"img/page 8.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Layer: Shared Weights\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left; width:60%\">\n",
    "    <ul>\n",
    "        <li>We show 3 hidden neurons belonging to the same feature map (the layer right above the input layer).</li>\n",
    "        <li>Weights of the same color are shared‚Äîconstrained to be identical.</li>\n",
    "        <li>Replicating neurons in this way allows for features to be detected regardless of their position in the input. </li>\n",
    "        <li>Additionally, <b>weight sharing increases learning efficiency</b> by greatly reducing the number of free parameters being learnt.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "<div style=\"float:left; width:40%\">\n",
    "    <img src=\"img/page 9.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution without padding\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 10.gif\">\n",
    "\n",
    "<img src=\"img/page 10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution with padding\n",
    "&nbsp;\n",
    "\n",
    "<center>Animation source: https://github.com/vdumoulin/conv_arithmetic</center>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "    <img src=\"img/page 11_1.gif\">\n",
    "    <center>4x4 input. 3x3 filter. Stride = 1. \n",
    "2x2 output.</center>\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right;width:50%\">\n",
    "    <img src=\"img/page 11_2.gif\">\n",
    "    <center>5x5 input. 3x3 filter. Stride = 1. \n",
    "5x5 output.</center>\n",
    "</div>\n",
    "\n",
    "<br> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (2) Non-linear Layer \n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 12_1.png\" width=\"250\">\n",
    "\n",
    "- Intuition: Increase the nonlinearity of the entire architecture without affecting the receptive fields of the convolution layer\n",
    "- A layer of neurons that applies the non-linear activation function, such as,\n",
    "    - **$f(x)=max‚Å°(0,x)$** - Rectified Linear Unit (ReLU); \n",
    "    \n",
    "    fast and most widely used in CNN\n",
    "    - $f(x)=\\text{tanh}x$\n",
    "    - $f(x)=|\\text{tanh}‚Å°ùë•|$\n",
    "    - $f(x)=(1+ùëí^{‚àíùë•})^{‚àí1}$ - sigmoid\n",
    "\n",
    "<center><img src=\"img/page 12_2.png\" width=\"300\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (3) Pooling Layer\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 13_1.png\" width=\"250\">\n",
    "\n",
    "- Intuition: to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting\n",
    "- Pooling partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum value of the features in that region.\n",
    "\n",
    "<center><img src=\"img/page 13_2.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling ( down sampling ) \n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 14.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other Layers\n",
    "&nbsp;\n",
    "\n",
    "- The convolution, non-linear, and pooling layers are typically used as a set. Multiple sets of the above three layers can appear in a CNN design.\n",
    "    - Input &rarr; <span style=\"color:green\">Conv. &rarr; Non-linear &rarr; Pooling</span> &rarr;  <span style=\"color:green\">Conv. &rarr; Non-linear &rarr; Pooling</span> &rarr; ‚Ä¶ &rarr; Output\n",
    "- Recent CNN architectures have 10-20 such layers.\n",
    "- After a few sets, the output is typically sent to one or two **fully connected layers**.\n",
    "    - A fully connected layer is a ordinary neural network layer as in other neural networks.\n",
    "    - Typical activation function is the sigmoid function.\n",
    "    - Output is typically class (classification) or real number (regression).\n",
    "\n",
    "<img src=\"img/page 15.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other Layers\n",
    "&nbsp;\n",
    "\n",
    "- The final layer of a CNN is determined by the research task.\n",
    "- Classification: Softmax Layer\n",
    "$$P(y=j|\\boldsymbol{x}) = \\frac{e^{w_j \\cdot x}}{\\sum_{k=1}^K{e^{w_k \\cdot x}}}$$\n",
    "    - The outputs are the probabilities of belonging to each class.\n",
    "- Regression: Linear Layer\n",
    "$$f(\\boldsymbol{x}) = \\boldsymbol{w} \\cdot \\boldsymbol{x}$$\n",
    "    - The output is a real number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementation for text in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "&nbsp;\n",
    "\n",
    "Main CNN idea for text:\n",
    "\n",
    "<span style=\"color:orange\">Compute vectors for n-grams</span> and group them afterwards\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "Example: ‚Äúthis takes too long‚Äù compute vectors for: \n",
    "\n",
    "This takes, takes too, too long, this takes too, takes too long, this takes too long\n",
    "\n",
    "<div style=\"float:left;width:60%\">\n",
    "    <img src=\"img/page 18.png\" width=\"450\">\n",
    "</div>\n",
    "<div style=\"float:right;width:40%\">\n",
    "    <img src=\"img/page 18.gif\" width=\"450\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNN for text classification\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 19.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNN with multiple filters\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 20.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python CNN Implementation\n",
    "&nbsp;\n",
    "\n",
    "- Prerequisites:\n",
    "    - Python 3.5+ (https://www.python.org/)\n",
    "    - TensorFlow (https://www.tensorflow.org/)\n",
    "    - Keras (https://keras.io/)\n",
    "        - **Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.**\n",
    "- Recommended:\n",
    "    - NumPy\n",
    "    - Scikit-Learn\n",
    "    - NLTK\n",
    "    - SciPy\n",
    "\n",
    "<img src=\"img/page 21.png\" width=‚Äú450‚Äù>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build a CNN in Keras\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- The `Sequential` model is used to build a linear stack of layers.\n",
    "- The following code shows how a typical CNN is built in Keras.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "![image.png](attachment:image.png)\n",
    "```\n",
    "\n",
    "Note:\n",
    "\n",
    "**Dense is the fully connected layer;**\n",
    "\n",
    "**Flatten is used after all CNN layers \n",
    "\n",
    "and before fully connected layer;\n",
    "\n",
    "**Conv2D is the 2D convolution layer;**\n",
    "\n",
    "**MaxPooling2D is the 2D max pooling layer;**\n",
    "\n",
    "**SGD is stochastic gradient descent algorithm.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build a CNN in Keras\n",
    "&nbsp;\n",
    "\n",
    "```\n",
    "(continued)\n",
    "\n",
    "model = Sequential()\n",
    "# We create an empty Sequential model and add layers onto it.\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100)))\n",
    "# We add a Conv2D layer with 32 filters, 3x3 each, followed by a detector layer ReLU.\n",
    "# This is the first layer we add to the model, so we need to specify the shape of the input. In this case we assume our input is a 100x100 matrix.\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# We add a MaxPooling2D layer with a 2x2 pooling size.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Build a CNN in Keras\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 23.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build a CNN in Keras\n",
    "&nbsp;\n",
    "\n",
    "```\n",
    "(continued)\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# We can add more Conv2D and MaxPooling2D layers onto the model.\n",
    "\n",
    "model.add(Flatten())\n",
    "# After all the desired CNN layers are added, add a Flatten layer.\n",
    "\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "# Add a fully connected layer followed by a detector layer with the sigmoid function.\n",
    "\n",
    "model.add(Dense(10, activation='softmax')\n",
    "# A softmax layer is added to achieve multiclass classification. In this example we have 10 classes.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Build a CNN in Keras\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 24.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build a CNN in Keras\n",
    "&nbsp;\n",
    "\n",
    "```\n",
    "(continued)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# Default SGD training parameters for correcting errors for filters\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "# Compile the model and use categorical crossentropy as the loss function, sgd as the optimizer\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "# Fit the model with x_train and y_train, batch_size and epochs can be set to other values\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)\n",
    "# Evaluate model performance using x_test and y_test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Build a CNN in Keras\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 25.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Encoder-Decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder-Decoder\n",
    "&nbsp;\n",
    "\n",
    "- **RNN**: input sequence is transformed into output sequence in a one-to-one fashion.\n",
    "- **Goal**: Develop an architecture capable of generating contextually appropriate, arbitrary length, output sequences\n",
    "- **Applications**: \n",
    "    - Machine translation, \n",
    "    - Summarization, \n",
    "    - Question answering,\n",
    "    - Dialogue modeling.\n",
    "\n",
    "<img src=\"img/page 27.png\" width=\"450\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple recurrent neural network illustrated as a feed-forward network\n",
    "&nbsp;\n",
    "\n",
    "**Most significant change: new set of weights, U**\n",
    "- connect the hidden layer from the previous time step to the current hidden layer. \n",
    "- determine how the network should make use of past context in calculating the output for the current input.\n",
    "\n",
    "<center><img src=\"img/page 28.png\" width=\"500\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple-RNN abstraction\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 29.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN Applications \n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 30.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentence Completion using an RNN\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 31.png\" width=\"600\">\n",
    "\n",
    "\n",
    "- **Trained Neural Language Model** can be used to generate novel sequences \n",
    "- Or to complete a given sequence (until end of sentence token <\\s> is generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/page 32.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extending (autoregressive) generation to Machine Translation\n",
    "&nbsp;\n",
    "\n",
    "- Translation as Sentence Completion!\n",
    "\n",
    "<img src=\"img/page 33.png\" width=\"700\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (simple) Encoder Decoder Networks\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 34.png\" width=\"700\">\n",
    "\n",
    "- Encoder generates a contextualized representation of the input (last state).\n",
    "- Decoder takes that state and autoregressively generates a sequence of outputs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Encoder Decoder Networks \n",
    "\n",
    "Abstracting away from these choices\n",
    "\n",
    "1. Encoder: accepts an input sequence, $x_{1:n}$ and generates a corresponding sequence of contextualized representations, $h_{1:n}$\n",
    "2. Context vector $c$:  function of $h_{1:n}$ and conveys the essence of the input to the decoder.\n",
    "3. Decoder: accepts $c$ as input and generates an arbitrary length sequence of hidden states $h_{1:m}$ from which a corresponding sequence of output states $y_{1:m}$ can be obtained.\n",
    "\n",
    "<center><img src=\"img/page 35.png\" width=\"500\"></center>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Popular architectural choices: Encoder\n",
    "&nbsp;\n",
    "\n",
    "Widely used encoder design: **stacked Bi-LSTMs** \n",
    "- Contextualized representations for each time step: **hidden states from top layers** from the forward and backward passes\n",
    "\n",
    "<center><img src=\"img/page 36.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decoder Basic Design\n",
    "&nbsp;\n",
    "\n",
    "- produce an output sequence an element at a time\n",
    "\n",
    "<center><img src=\"img/page 37.png\" width=\"600\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decoder Design <br> Enhancement\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 38.png\">\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decoder: How output y is chosen\n",
    "&nbsp;\n",
    "\n",
    "- **Sample soft-max** distribution (OK for generating novel output, not OK for e.g. MT or Summ)\n",
    "- **Most likely output** (doesn‚Äôt guarantee individual choices being made make sense together)\n",
    "\n",
    "<center><img src=\"img/page 39.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/page 40.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Flexible context: Attention\n",
    "&nbsp;\n",
    "\n",
    "**Context vector $c$**: function of **$h_{1:n}$** and conveys the essence of the input to the decoder.\n",
    "\n",
    "<center><img src=\"img/page 42.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Flexible context: Attention\n",
    "&nbsp;\n",
    "\n",
    "**Context vector $c$**: function of **$h_{1:n}$** and conveys the essence of the input to the decoder.\n",
    "\n",
    "**Flexible?**  \n",
    "- Different for each $h_i$\n",
    "- Flexibly combining the $h_j$ \n",
    "\n",
    "<center><img src=\"img/page 42.png\" width=\"500\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention (1): dynamically derived context\n",
    "&nbsp;\n",
    "\n",
    "- Replace static context vector with dynamic <span style=\"color:lightblue\">$c_i$</span>\n",
    "- derived from the encoder hidden states at each point <span style=\"color:lightblue\">$i$</span> during decoding\n",
    "\n",
    "**Ideas**:\n",
    "- should be a linear combination of those states \n",
    "$$c_i = \\sum_j{\\alpha_{ij}h^e_j}$$\n",
    "- $\\alpha_{ij} $ should depend on?\n",
    "\n",
    "<center><img src=\"img/page 43.png\" width=\"300\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention (2): computing $c_i$\n",
    "\n",
    "- Compute a vector of scores that capture the relevance of each encoder hidden state to the decoder state $h_{i-1}^d$\n",
    "$$score(h_{i-1}^d, h_j^e)$$\n",
    "\n",
    "- Just the similarity\n",
    "$$score(h_{i-1}^d, h_j^e) = h_{i-1}^d \\cdot h_j^e$$\n",
    "\n",
    "- Give network the ability to <span style=\"background-color:lightgray\">learn which aspects</span> of similarity between the decoder and encoder states are important to the current application.\n",
    "\n",
    "$$score(h_{i-1}^d, h_j^e) = h_{i-1}^d W_S h_j^e$$\n",
    "\n",
    "<center><img src=\"img/page 44.png\" width=\"300\"></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention (3): computing $c_i$ <br> From scores to weights\n",
    "&nbsp;\n",
    "\n",
    "- Create vector of weights  by normalizing scores\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_{ij} &= \\text{softmax}(score(h_{i-1}^d, h_j^e)\\ \\forall j \\in e) \\\\\n",
    "&= \\frac{exp(score(h_{i-1}^d, h_j^e))}{\\sum_k{exp(score(h_{i-1}^d, h_k^e))}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- **Goal achieved**: compute a fixed-length context vector for the current decoder state by taking a weighted average over all the encoder hidden states.\n",
    "\n",
    "<center><img src=\"img/page 45.png\" width=\"300\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention: Summary\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 46.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Explain Y. Goldberg different notation\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 47.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro to Encoder-Decoder and Attention (Goldberg‚Äôs notation)\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 48.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers (Attention is all you need 2017)\n",
    "&nbsp;\n",
    "\n",
    "- Just an introduction: These are two valuable resources to learn more details on the architecture and implementation\n",
    "\n",
    "- http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "\n",
    "- https://jalammar.github.io/illustrated-transformer/ (slides come from this source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## High-level architecture\n",
    "&nbsp;\n",
    "\n",
    "- Will only look at the ENCODER(s) part in detail\n",
    "\n",
    "<center><img src=\"img/page 51.png\" width=\"450\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/page 52.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Key property of Transformer**: word in each position flows through its own path in the encoder. \n",
    "- There are dependencies between these paths in the self-attention layer. \n",
    "- Feed-forward layer does not have those dependencies => various paths can be executed in parallel !\n",
    "\n",
    "<center><img src=\"img/page 53.png\" width=\"450\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visually clearer on two words\n",
    "&nbsp;\n",
    "\n",
    "- dependencies in self-attention layer. \n",
    "- No dependencies in Feed-forward layer \n",
    "\n",
    "<center><img src=\"img/page 54.png\" width=\"450\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-Attention\n",
    "&nbsp;\n",
    "\n",
    "While processing **each word** it allows to look at other positions in the input sequence for clues to build a better encoding for **this word**.\n",
    "\n",
    "**Step1: create three vectors** from each of the encoder‚Äôs input vectors: \n",
    "\n",
    "<span style=\"color:purple\">Query</span>, a <span style=\"color:orange\">Key</span>, <span style=\"color:lightblue\">Value</span>  (typically smaller dimension). \n",
    "\n",
    "by multiplying the embedding by three matrices that we **trained** during the training process.\n",
    "\n",
    "<center><img src=\"img/page 55.png\" width=\"400\"></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-Attention\n",
    "&nbsp;\n",
    "\n",
    "**Step 2: calculate a score** (like we have seen for regular attention!)¬† how much focus to place on other parts of the input sentence as we encode a word at a certain position.\n",
    "\n",
    "Take dot product of the¬†<span style=\"color:purple\">query vector</span>¬†with the¬†<span style=\"color:orange\">key vector</span> of the respective word we‚Äôre scoring. \n",
    "\n",
    "\n",
    "E.g., Processing the self-attention for word ‚ÄúThinking‚Äù in position¬†$\\text{#}1$, the first score would be the dot product of¬†<span style=\"color:purple\">q1</span>¬†and¬†<span style=\"color:orange\">k1</span>. The second score would be the dot product of¬†<span style=\"color:purple\">q1</span>¬†and¬†<span style=\"color:orange\">k2</span>.\n",
    "\n",
    "<center><img src=\"img/page 56.png\" width=\"400\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self Attention\n",
    "&nbsp;\n",
    "\n",
    "- **Step 3**¬†divide scores by the square root of the dimension of the <span style=\"color:orange\">key vectors</span>  (more stable gradients). \n",
    "- **Step 4** pass result through a softmax operation. (all positive and add up to 1)\n",
    "\n",
    "**Intuition**: softmax score determines how much each word will be expressed at this position. \n",
    "\n",
    "<center><img src=\"img/page 57.png\" width=\"400\"></center>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self Attention\n",
    "&nbsp;\n",
    "\n",
    "**Step6**¬†: sum up the weighted <span style=\"color:blue\">value vectors</span>. This produces <span style=\"color:pink\">the output of the self-attention layer</span> at this position\n",
    "\n",
    "<center><img src=\"img/page 58.png\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Self Attention\n",
    "&nbsp;\n",
    "\n",
    "**Step6**¬†: sum up the weighted <span style=\"color:blue\">value vectors</span>. This produces <span style=\"color:pink\">the output of the self-attention layer</span> at this position\n",
    "\n",
    "More details:\n",
    "- What we have seen for a word is done **for all words** (using matrices) \n",
    "- Need to **encode position** of words\n",
    "- And improved using a mechanism called ‚Äú**multi-headed**‚Äù attention\n",
    "\n",
    "(kind of like multiple filters for CNN)\n",
    "\n",
    "see https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "<center><img src=\"img/page 58.png\" width=\"200\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Decoder Side\n",
    "\n",
    "- Relies on most of the concepts on the encoder side\n",
    "- See animation on https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "<center><img src=\"img/page 59.png\" width=\"450\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Menti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary: what did we learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\text {Time for Practical 7!}$$ "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

---
title: "Deep Learning for Text 2"
author: "Ayoub Bagheri"
subtitle: "Applied Text Mining"
output:
  ioslides_presentation:
    logo: logo.png
    smaller: yes
    widescreen: no
---

```{r, include=FALSE}
library(knitr)
library(kableExtra)
```

## Lecture‚Äôs plan

1. Convolutional Neural Network (CNN)
2. Encoder-Decoder
3. Transformers

# Convolutional Neural Network

## Convolutional Neural Network (CNN)

- Convolutional Neural Networks, or Convolutional Networks, or CNNs, or ConvNets
- For processing data with a **grid-like** or array topology
    - 1-D grid: time-series data, sensor signal data
    - 2-D grid: image data
    - 3-D grid: video data

- CNNs include four key ideas related to natural signals: 
    - **Local connections**
    - **Shared weights**
    - **Pooling**
    - **Use of many layers**
    
## CNN architecture

- Intuition: Neural network with specialized connectivity structure
    - Stacking multiple layers of feature extractors, low-level layers extract local features, and high-level layers extract learn global patterns.
- There are a few distinct types of layers:
    - **Convolutional Layer**: detecting local features through filters (discrete convolution)
    - **Non-linear Layer**: normalization via Rectified Linear Unit (ReLU)
    - **Pooling Layer**: merging similar features
    
## Building-blocks for CNNs

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 6.png")
```

## (1) Convolutional layer

- The core layer of CNNs
- Convolutional layer consists of a set of filters, $W_{kl}$
- Each filter covers a spatially small portion of the input data, $Z_{i,j}$
- Each filter is convolved across the dimensions of the input data, producing a multidimensional **feature map**.
- As we convolve the filter, we are computing the dot product between the parameters of the filter and the input.
- **Deep Learning algorithm**: During training, the network corrects errors and filters are **learned**, e.g., in Keras, by adjusting weights based on **Stochastic Gradient Descent**, **SGD** (stochastic approximation of GD using a randomly selected subset of the data).
- The key architectural characteristics of the convolutional layer is **local connectivity** and **shared weights**.

```{r, echo=FALSE, out.width="30%", fig.align='center'}
include_graphics("img/page 7.png")
```

## Convolutional layer: Local connectivity

<div style="float:left; width:60%">
  <ul>
      <li>Neurons in layer m are only connected to 3 adjacent neurons in the m-1 layer.</li>
      <li>Neurons in layer m+1 have a similar connectivity with the layer below.</li>
      <li>Each neuron is unresponsive to variations outside of its <em>receptive field</em> with respect to the input. </li>
      <ul><li>Receptive field: small neuron collections which process portions of the input data.</li></ul>
      <li>The architecture thus ensures that the learnt feature extractors produce the strongest response to a spatially local input pattern.</li>
  </ul>
</div>
<div style="float:left; width:40%">
```{r, echo=FALSE, out.width="90%", fig.align='center'}
include_graphics("img/page 8.png")
```
</div>

## Convolutional layer: Shared weights

<div style="float:left; width:60%">
  <ul>
      <li>We show 3 hidden neurons belonging to the same feature map (the layer right above the input layer).</li>
      <li>Weights of the same color are shared‚Äîconstrained to be identical.</li>
      <li>Replicating neurons in this way allows for features to be detected regardless of their position in the input. </li>
      <li>Additionally, <b>weight sharing increases learning efficiency</b> by greatly reducing the number of free parameters being learnt.</li>
  </ul>
</div>
<div style="float:left; width:40%">
```{r, echo=FALSE, out.width="90%", fig.align='center'}
include_graphics("img/page 9.png")
```
</div>

## Convolution without padding

```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 10.gif")
```


```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 10.png")
```

## Convolution with padding

<center>Animation source: https://github.com/vdumoulin/conv_arithmetic</center>

<div style="float:left;width:50%">
```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 11_1.gif")
```
  <center>4x4 input. 3x3 filter. Stride = 1. 
2x2 output.</center>
</div>

<div style="float:right;width:50%">
```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 11_2.gif")
```
  <center>5x5 input. 3x3 filter. Stride = 1. 
5x5 output.</center>
</div>


## (2) Non-linear Layer 

```{r, echo=FALSE, out.width="40%", fig.align='center'}
include_graphics("img/page 12_1.png")
```

- Intuition: Increase the nonlinearity of the entire architecture without affecting the receptive fields of the convolution layer
- A layer of neurons that applies the non-linear activation function, such as,
    - **$f(x)=maxa(0,x)$** - Rectified Linear Unit (ReLU); 
    
    fast and most widely used in CNN
    - $f(x)=\text{tanh}x$
    - $f(x)=|\text{tanh}aùë•|$
    - $f(x)=(1+ùëí^{‚àíùë•})^{‚àí1}$ - sigmoid

```{r, echo=FALSE, out.width="50%", fig.align='center'}
include_graphics("img/page 12_2.png")
```

## (3) Pooling Layer

```{r, echo=FALSE, out.width="50%", fig.align='center'}
include_graphics("img/page 13_1.png")
```

- Intuition: to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting
- Pooling partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum value of the features in that region.

```{r, echo=FALSE, out.width="60%", fig.align='center'}
include_graphics("img/page 13_2.png")
```

## Pooling ( down sampling ) 

```{r, echo=FALSE, out.width="90%", fig.align='center'}
include_graphics("img/page 14.png")
```

## Other Layers

- The convolution, non-linear, and pooling layers are typically used as a set. Multiple sets of the above three layers can appear in a CNN design.
    - Input &rarr; <span style="color:green">Conv. &rarr; Non-linear &rarr; Pooling</span> &rarr;  <span style="color:green">Conv. &rarr; Non-linear &rarr; Pooling</span> &rarr; ‚Ä¶ &rarr; Output
- Recent CNN architectures have 10-20 such layers.
- After a few sets, the output is typically sent to one or two **fully connected layers**.
    - A fully connected layer is a ordinary neural network layer as in other neural networks.
    - Typical activation function is the sigmoid function.
    - Output is typically class (classification) or real number (regression).

```{r, echo=FALSE, out.width="50%", fig.align='center'}
include_graphics("img/page 15.png")
```

## Other Layers

- The final layer of a CNN is determined by the research task.
- Classification: Softmax Layer
$$P(y=j|\boldsymbol{x}) = \frac{e^{w_j \cdot x}}{\sum_{k=1}^K{e^{w_k \cdot x}}}$$
    - The outputs are the probabilities of belonging to each class.
- Regression: Linear Layer
$$f(\boldsymbol{x}) = \boldsymbol{w} \cdot \boldsymbol{x}$$
    - The output is a real number.

# CNN for Text 

## Convolutional Neural Networks (CNNs)

Main CNN idea for text:

<span style="color:orange">Compute vectors for n-grams</span> and group them afterwards

<br> <br>

Example: ‚Äúthis takes too long‚Äù compute vectors for: 

This takes, takes too, too long, this takes too, takes too long, this takes too long

<div style="float:left;width:60%">
  <img src="img/page 18.png" width="360">
</div>
<div style="float:right;width:40%">
  <img src="img/page 18.gif" width="360">
</div>

## CNN for text classification

```{r, echo=FALSE, out.width="90%", fig.align='center'}
include_graphics("img/page 19.png")
```

## CNN with multiple filters

```{r, echo=FALSE, out.width="90%", fig.align='center'}
include_graphics("img/page 20.png")
```

<!-- ## Python CNN Implementation -->

<!-- - Prerequisites: -->
<!--     - Python 3.5+ (https://www.python.org/) -->
<!--     - TensorFlow (https://www.tensorflow.org/) -->
<!--     - Keras (https://keras.io/) -->
<!--         - **Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.** -->
<!-- - Recommended: -->
<!--     - NumPy -->
<!--     - Scikit-Learn -->
<!--     - NLTK -->
<!--     - SciPy -->

<!-- ```{r, echo=FALSE, out.width="50%", fig.align='center'} -->
<!-- include_graphics("img/page 21.png") -->
<!-- ``` -->

## Build a CNN in Keras

<div style="float: left; width:60%">
- The `Sequential` model is used to build a linear stack of layers.
- The following code shows how a typical CNN is built in Keras.

```
import numpy as np
import keras
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.optimizers import SGD
![image.png](attachment:image.png)
```
</div>
<div style="float: right; width:40%">
Note:

**Dense is the fully connected layer;**

**Flatten is used after all CNN layers **

and before fully connected layer;

**Conv2D is the 2D convolution layer;**

**MaxPooling2D is the 2D max pooling layer;**

**SGD is stochastic gradient descent algorithm.**
</div>

<!-- ## Build a CNN in Keras -->

<!-- ```{r, echo=FALSE, out.width="100%", fig.align='center'} -->
<!-- include_graphics("img/page 23.png") -->
<!-- ``` -->

<!-- ## Build a CNN in Keras -->

<!-- ```{r, echo=FALSE, out.width="100%", fig.align='center'} -->
<!-- include_graphics("img/page 24.png") -->
<!-- ``` -->

<!-- ## Build a CNN in Keras -->

<!-- ```{r, echo=FALSE, out.width="100%", fig.align='center'} -->
<!-- include_graphics("img/page 25.png") -->
<!-- ``` -->

# Encoder-Decoder
 
## Encoder-Decoder

- **RNN**: input sequence is transformed into output sequence in a one-to-one fashion.
- **Goal**: Develop an architecture capable of generating contextually appropriate, arbitrary length, output sequences
- **Applications**: 
    - Machine translation, 
    - Summarization, 
    - Question answering,
    - Dialogue modeling.

```{r, echo=FALSE, out.width="50%", fig.align='center'}
include_graphics("img/page 27.png")
```

## Simple recurrent neural network illustrated as a feed-forward network
&nbsp;

**Most significant change: new set of weights, U**
- connect the hidden layer from the previous time step to the current hidden layer. 
- determine how the network should make use of past context in calculating the output for the current input.

```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 28.png")
```

## Simple-RNN abstraction

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 29.png")
```

<!-- ## RNN Applications -->

<!-- ```{r, echo=FALSE, out.width="100%", fig.align='center'} -->
<!-- include_graphics("img/page 30.png") -->
<!-- ``` -->

<!-- ## Sentence Completion using an RNN -->

<!-- ```{r, echo=FALSE, out.width="90%", fig.align='center'} -->
<!-- include_graphics("img/page 31.png") -->
<!-- ``` -->

<!-- - **Trained Neural Language Model** can be used to generate novel sequences  -->
<!-- - Or to complete a given sequence (until end of sentence token <\s> is generated) -->

<!-- ##  -->

<!-- ```{r, echo=FALSE, out.width="100%", fig.align='center'} -->
<!-- include_graphics("img/page 32.png") -->
<!-- ``` -->

<!-- ## Extending (autoregressive) generation to Machine Translation -->

<!-- - Translation as Sentence Completion! -->

<!-- ```{r, echo=FALSE, out.width="80%", fig.align='center'} -->
<!-- include_graphics("img/page 33.png") -->
<!-- ``` -->

## (simple) Encoder Decoder Networks
&nbsp;

```{r, echo=FALSE, out.width="90%", fig.align='center'}
include_graphics("img/page 34.png")
```

- Encoder generates a contextualized representation of the input (last state).
- Decoder takes that state and autoregressively generates a sequence of outputs

## General Encoder Decoder Networks 

Abstracting away from these choices

1. Encoder: accepts an input sequence, $x_{1:n}$ and generates a corresponding sequence of contextualized representations, $h_{1:n}$
2. Context vector $c$:  function of $h_{1:n}$ and conveys the essence of the input to the decoder.
3. Decoder: accepts $c$ as input and generates an arbitrary length sequence of hidden states $h_{1:m}$ from which a corresponding sequence of output states $y_{1:m}$ can be obtained.

```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 35.png")
```

## Popular architectural choices: Encoder

Widely used encoder design: **stacked Bi-LSTMs** 
- Contextualized representations for each time step: **hidden states from top layers** from the forward and backward passes

```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 36.png")
```

## Decoder Basic Design

- produce an output sequence an element at a time

```{r, echo=FALSE, out.width="80%", fig.align='center'}
include_graphics("img/page 37.png")
```

## Decoder Design <br> Enhancement

```{r, echo=FALSE, out.width="90%", fig.align='center'}
include_graphics("img/page 38.png")
```

## Decoder: How output y is chosen

- **Sample soft-max** distribution (OK for generating novel output, not OK for e.g. MT or Summ)
- **Most likely output** (doesn‚Äôt guarantee individual choices being made make sense together)

```{r, echo=FALSE, out.width="80%", fig.align='center'}
include_graphics("img/page 39.png")
```

##

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 40.png")
```

<!-- # Attention -->

<!-- ## Flexible context: Attention -->

<!-- **Context vector $c$**: function of **$h_{1:n}$** and conveys the essence of the input to the decoder. -->

<!-- ```{r, echo=FALSE, out.width="80%", fig.align='center'} -->
<!-- include_graphics("img/page 42.png") -->
<!-- ``` -->

<!-- ## Flexible context: Attention -->

<!-- **Context vector $c$**: function of **$h_{1:n}$** and conveys the essence of the input to the decoder. -->

<!-- **Flexible?**   -->
<!-- - Different for each $h_i$ -->
<!-- - Flexibly combining the $h_j$  -->

<!-- ```{r, echo=FALSE, out.width="80%", fig.align='center'} -->
<!-- include_graphics("img/page 42.png") -->
<!-- ``` -->

<!-- ## Attention (1): dynamically derived context -->

<!-- - Replace static context vector with dynamic <span style="color:lightblue">$c_i$</span> -->
<!-- - derived from the encoder hidden states at each point <span style="color:lightblue">$i$</span> during decoding -->

<!-- **Ideas**: -->
<!-- - should be a linear combination of those states  -->
<!-- $$c_i = \sum_j{\alpha_{ij}h^e_j}$$ -->
<!-- - $\alpha_{ij}$ should depend on? -->

<!-- ```{r, echo=FALSE, out.width="40%", fig.align='center'} -->
<!-- include_graphics("img/page 43.png") -->
<!-- ``` -->

<!-- ## Attention (2): computing $c_i$ -->

<!-- - Compute a vector of scores that capture the relevance of each encoder hidden state to the decoder state $h_{i-1}^d$ -->
<!-- $$score(h_{i-1}^d, h_j^e)$$ -->

<!-- - Just the similarity -->
<!-- $$score(h_{i-1}^d, h_j^e) = h_{i-1}^d \cdot h_j^e$$ -->

<!-- - Give network the ability to <span style="background-color:lightgray">learn which aspects</span> of similarity between the decoder and encoder states are important to the current application. -->

<!-- $$score(h_{i-1}^d, h_j^e) = h_{i-1}^d W_S h_j^e$$ -->

<!-- ```{r, echo=FALSE, out.width="40%", fig.align='center'} -->
<!-- include_graphics("img/page 44.png") -->
<!-- ``` -->

<!-- ## Attention (3): computing $c_i$ <br> From scores to weights -->

<!-- - Create vector of weights  by normalizing scores -->

<!-- $$ -->
<!-- \begin{align} -->
<!-- a_{ij} &= \text{softmax}(score(h_{i-1}^d, h_j^e)\ \forall j \in e) \\ -->
<!-- &= \frac{exp(score(h_{i-1}^d, h_j^e))}{\sum_k{exp(score(h_{i-1}^d, h_k^e))}} -->
<!-- \end{align} -->
<!-- $$ -->

<!-- - **Goal achieved**: compute a fixed-length context vector for the current decoder state by taking a weighted average over all the encoder hidden states. -->

<!-- ```{r, echo=FALSE, out.width="40%", fig.align='center'} -->
<!-- include_graphics("img/page 45.png") -->
<!-- ``` -->

<!-- ## Attention: Summary -->

<!-- ```{r, echo=FALSE, out.width="100%", fig.align='center'} -->
<!-- include_graphics("img/page 46.png") -->
<!-- ``` -->

<!-- ## Explain Y. Goldberg different notation -->

<!-- ```{r, echo=FALSE, out.width="100%", fig.align='center'} -->
<!-- include_graphics("img/page 47.png") -->
<!-- ``` -->

<!-- ## Intro to Encoder-Decoder and Attention (Goldberg‚Äôs notation) -->

<!-- ```{r, echo=FALSE, out.width="100%", fig.align='center'} -->
<!-- include_graphics("img/page 48.png") -->
<!-- ``` -->

# Transformers

## Transformers | (Attention is all you need)

- A transformer adopts an encoder-decoder architecture.

- Transformers were developed to solve the problem of sequence transduction, or neural machine translation. That means any task that transforms an input sequence to an output sequence. 

- These are two valuable resources to learn more details on the architecture and implementation

- http://nlp.seas.harvard.edu/2018/04/03/attention.html

- https://jalammar.github.io/illustrated-transformer/ (slides come from this source)


## Transformers

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/bertelmogpt.png")
```

<!-- ## High-level architecture -->

<!-- - Will only look at the ENCODER(s) part in detail -->

<!-- ```{r, echo=FALSE, out.width="80%", fig.align='center'} -->
<!-- include_graphics("img/page 51.png") -->
<!-- ``` -->

<!-- ##  -->

<!-- ```{r, echo=FALSE, out.width="100%", fig.align='center'} -->
<!-- include_graphics("img/page 52.png") -->
<!-- ``` -->

<!-- ##  -->

<!-- **Key property of Transformer**: word in each position flows through its own path in the encoder.  -->
<!-- - There are dependencies between these paths in the self-attention layer.  -->
<!-- - Feed-forward layer does not have those dependencies => various paths can be executed in parallel ! -->

<!-- ```{r, echo=FALSE, out.width="80%", fig.align='center'} -->
<!-- include_graphics("img/page 53.png") -->
<!-- ``` -->

<!-- ## Visually clearer on two words -->

<!-- - dependencies in self-attention layer.  -->
<!-- - No dependencies in Feed-forward layer  -->

<!-- ```{r, echo=FALSE, out.width="70%", fig.align='center'} -->
<!-- include_graphics("img/page 54.png") -->
<!-- ``` -->

<!-- ## Self-Attention -->

<!-- While processing **each word** it allows to look at other positions in the input sequence for clues to build a better encoding for **this word**. -->

<!-- **Step1: create three vectors** from each of the encoder‚Äôs input vectors:  -->

<!-- <span style="color:purple">Query</span>, a <span style="color:orange">Key</span>, <span style="color:lightblue">Value</span>  (typically smaller dimension).  -->

<!-- by multiplying the embedding by three matrices that we **trained** during the training process. -->

<!-- ```{r, echo=FALSE, out.width="50%", fig.align='center'} -->
<!-- include_graphics("img/page 55.png") -->
<!-- ``` -->

<!-- ## Self-Attention -->

<!-- **Step 2: calculate a score** (like we have seen for regular attention!)¬† how much focus to place on other parts of the input sentence as we encode a word at a certain position. -->

<!-- Take dot product of the <span style="color:purple">query vector</span> with the <span style="color:orange">key vector</span> of the respective word we‚Äôre scoring.  -->


<!-- E.g., Processing the self-attention for word ‚ÄúThinking‚Äù in position $\text{#}1$, the first score would be the dot product of <span style="color:purple">q1</span> and <span style="color:orange">k1</span>. The second score would be the dot product of <span style="color:purple">q1</span> and <span style="color:orange">k2</span>. -->

<!-- ```{r, echo=FALSE, out.width="50%", fig.align='center'} -->
<!-- include_graphics("img/page 56.png") -->
<!-- ``` -->

<!-- ## Self Attention -->

<!-- - **Step 3** divide scores by the square root of the dimension of the <span style="color:orange">key vectors</span>  (more stable gradients).  -->
<!-- - **Step 4** pass result through a softmax operation. (all positive and add up to 1) -->

<!-- **Intuition**: softmax score determines how much each word will be expressed at this position.  -->

<!-- ```{r, echo=FALSE, out.width="60%", fig.align='center'} -->
<!-- include_graphics("img/page 57.png") -->
<!-- ``` -->

<!-- ## Self Attention -->

<!-- **Step6** : sum up the weighted <span style="color:blue">value vectors</span>. This produces <span style="color:pink">the output of the self-attention layer</span> at this position -->

<!-- ```{r, echo=FALSE, out.width="70%", fig.align='center'} -->
<!-- include_graphics("img/page 57.png") -->
<!-- ``` -->

<!-- ## Self Attention -->

<!-- **Step6** : sum up the weighted <span style="color:blue">value vectors</span>. This produces <span style="color:pink">the output of the self-attention layer</span> at this position -->

<!-- More details: -->
<!-- - What we have seen for a word is done **for all words** (using matrices)  -->
<!-- - Need to **encode position** of words -->
<!-- - And improved using a mechanism called ‚Äú**multi-headed**‚Äù attention -->

<!-- (kind of like multiple filters for CNN) -->

<!-- see https://jalammar.github.io/illustrated-transformer/ -->

<!-- ```{r, echo=FALSE, out.width="30%", fig.align='center'} -->
<!-- include_graphics("img/page 58.png") -->
<!-- ``` -->

<!-- ## The Decoder Side -->

<!-- - Relies on most of the concepts on the encoder side -->
<!-- - See animation on https://jalammar.github.io/illustrated-transformer/ -->

<!-- ```{r, echo=FALSE, out.width="70%", fig.align='center'} -->
<!-- include_graphics("img/page 59.png") -->
<!-- ``` -->

# Summary

## Summary
- Convolutional Neural Network (CNN)
- Encoder-Decoder
- Transformers


# Time for Practical 7!

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "#### Applied Text Mining - Utrecht Summer School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the seventh practical of the course “Applied Text Mining”. \n",
    "\n",
    "In this practical, we will create models for neural machine translation. Today we are curious to see how a simple deep learning based model translates a sentence into its counterpart. See these examples:\n",
    "\n",
    "<img src=\"translation_example.png\">\n",
    "\n",
    "<img src=\"translation_example2.png\">\n",
    "\n",
    "The objective from this practical is to convert a Dutch sentence to its English counterpart using a Neural Machine Translation (NMT) system. We will implement this task by building a simple Sequence-to-Sequence model with the help of Keras library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will use the following libraries. Take care to have them installed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from numpy import array, argmax, random, take\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. **In this practical we will use a dataset of tab-delimited Bilingual Sentence Pairs from http://www.manythings.org/anki/. Use the following two functions (read_text and to_lines) and read the nld.txt dataset (provided in the data folder). This dataset contains phrases in Dutch with their translation in English. Convert the text sequences to an array and check the first items in your array.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read raw text file\n",
    "def read_text(filename):\n",
    "    # open the file\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a text into sentences\n",
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t') for i in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_text(\"nld-eng/nld.txt\")\n",
    "nld_eng = to_lines(data)\n",
    "nld_eng = array(nld_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54972, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nld_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Go.', 'Lopen!',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7764436 (LinguisticFusion)'],\n",
       "       ['Go.', 'Vooruit.',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7915821 (Elsofie)'],\n",
       "       ['Hi.', 'Hoi.',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #537889 (Dorenda)'],\n",
       "       ...,\n",
       "       ['Always use distilled water in steam irons because using ordinary water will cause a mineral build-up over time that will clog the steam holes.',\n",
       "        'Gebruik altijd gedistilleerd water in stoomstrijkijzers, want gewoon water zorgt voor mineraalophoping dat de stoomgaatjes na verloop van tijd verstopt.',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #3020388 (Delian) & #3037091 (Citrine)'],\n",
       "       [\"If you translate from your second language into your own native language, rather than the other way around, you're less likely to make mistakes.\",\n",
       "        'Als je vanuit je tweede taal naar je eigen moedertaal vertaalt, in plaats van andersom, maak je minder snel fouten.',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #1230823 (CK) & #8627687 (MarijnKp)'],\n",
       "       [\"If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\",\n",
       "        'Als iemand die je achtergrond niet kent zegt dat je klinkt als een moedertaalspreker betekent dat dat diegene waarschijnlijk iets in je spreken opgemerkt heeft dat hem deed realiseren dat je geen moedertaalspreker bent. Met andere woorden, je klinkt niet echt als een moedertaalspreker.',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #953936 (CK) & #1056762 (ReneeMona)']],\n",
       "      dtype='<U286')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nld_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a code like nld_eng = nld_eng[:1000,:], you can use a subset of data, for example the first 1000 sentence pairs, to reduce the training time of the model. Be aware that you will lose the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. **The maketrans() function is a function from the library str that is used to construct a transition table i.e specify a list of characters that need to be replaced in a string or the characters that need to be deleted from the string. To use this transition table, you can use the translate() function and apply that on a string. It is also possible to use these functions to remove the punctuations. Similar to the example below, apply the maketrans() function to remove punctuations from the nld_eng array.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The string before translating is : text mining\n",
      "The string after translating is : woow dining\n"
     ]
    }
   ],
   "source": [
    "# specify the list of characters that need to be replaced\n",
    "str1 = \"mtex\"\n",
    "\n",
    "# specify the list of characters with which the characters need to be replaced\n",
    "str2 = \"dwoo\"\n",
    "\n",
    "# specify the list of characters that needs to be deleted\n",
    "str3 = \"u\"\n",
    "\n",
    "# target string \n",
    "temp_str = \"text mining\"\n",
    "\n",
    "# using maketrans() to construct a translate table\n",
    "table = temp_str.maketrans(str1, str2, str3)\n",
    "  \n",
    "# Printing original string \n",
    "print (\"The string before translating is : \", end =\"\")\n",
    "print (temp_str)\n",
    "  \n",
    "# using translate() to make translations.\n",
    "print (\"The string after translating is : \", end =\"\")\n",
    "print (temp_str.translate(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is possible to replace one character with two or more. You need to supply a dict as argument to maketrans()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nld_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in nld_eng[:,0]]\n",
    "nld_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in nld_eng[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Go', 'Lopen',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7764436 (LinguisticFusion)'],\n",
       "       ['Go', 'Vooruit',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7915821 (Elsofie)'],\n",
       "       ['Hi', 'Hoi',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #537889 (Dorenda)'],\n",
       "       ...,\n",
       "       ['Always use distilled water in steam irons because using ordinary water will cause a mineral buildup over time that will clog the steam holes',\n",
       "        'Gebruik altijd gedistilleerd water in stoomstrijkijzers want gewoon water zorgt voor mineraalophoping dat de stoomgaatjes na verloop van tijd verstopt',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #3020388 (Delian) & #3037091 (Citrine)'],\n",
       "       ['If you translate from your second language into your own native language rather than the other way around youre less likely to make mistakes',\n",
       "        'Als je vanuit je tweede taal naar je eigen moedertaal vertaalt in plaats van andersom maak je minder snel fouten',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #1230823 (CK) & #8627687 (MarijnKp)'],\n",
       "       ['If someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker In other words you dont really sound like a native speaker',\n",
       "        'Als iemand die je achtergrond niet kent zegt dat je klinkt als een moedertaalspreker betekent dat dat diegene waarschijnlijk iets in je spreken opgemerkt heeft dat hem deed realiseren dat je geen moedertaalspreker bent Met andere woorden je klinkt niet echt als een moedertaalspreker',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #953936 (CK) & #1056762 (ReneeMona)']],\n",
       "      dtype='<U286')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nld_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. **Convert all words to their lowercase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "for i in range(len(nld_eng)):\n",
    "    nld_eng[i,0] = nld_eng[i,0].lower()    \n",
    "    nld_eng[i,1] = nld_eng[i,1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['go', 'lopen',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7764436 (LinguisticFusion)'],\n",
       "       ['go', 'vooruit',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7915821 (Elsofie)'],\n",
       "       ['hi', 'hoi',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #537889 (Dorenda)'],\n",
       "       ...,\n",
       "       ['always use distilled water in steam irons because using ordinary water will cause a mineral buildup over time that will clog the steam holes',\n",
       "        'gebruik altijd gedistilleerd water in stoomstrijkijzers want gewoon water zorgt voor mineraalophoping dat de stoomgaatjes na verloop van tijd verstopt',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #3020388 (Delian) & #3037091 (Citrine)'],\n",
       "       ['if you translate from your second language into your own native language rather than the other way around youre less likely to make mistakes',\n",
       "        'als je vanuit je tweede taal naar je eigen moedertaal vertaalt in plaats van andersom maak je minder snel fouten',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #1230823 (CK) & #8627687 (MarijnKp)'],\n",
       "       ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker',\n",
       "        'als iemand die je achtergrond niet kent zegt dat je klinkt als een moedertaalspreker betekent dat dat diegene waarschijnlijk iets in je spreken opgemerkt heeft dat hem deed realiseren dat je geen moedertaalspreker bent met andere woorden je klinkt niet echt als een moedertaalspreker',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #953936 (CK) & #1056762 (ReneeMona)']],\n",
       "      dtype='<U286')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nld_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. **What is the maximum length of a sentence in each of the Dutch and English sets? What about the average length?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists\n",
    "eng_l = []\n",
    "nld_l = []\n",
    "# populate the lists with sentence lengths\n",
    "for i in nld_eng[:,0]:\n",
    "    eng_l.append(len(i.split()))\n",
    "\n",
    "for i in nld_eng[:,1]:\n",
    "    nld_l.append(len(i.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_df = pd.DataFrame({'eng':eng_l, 'nld':nld_l})\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(eng_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(nld_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.853307138179437"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nld_l)\n",
    "import statistics\n",
    "statistics.mean(nld_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.619569962890199"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(eng_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coincidentally the maximum length of the Dutch sentences and that of the English phrases is equal to 44."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. **Using train_test_split from sklearn split the dataset into training (80%) and test (20%) sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(nld_eng, test_size=0.2, random_state = 321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. **Time to tokenize the sentences. The function below uses the Tokenizer function from Keras and fits that on its input text. Apply this function and find out about the vocabulary size for the Dutch and English sets. For this you can use the entire sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build a tokenizer\n",
    "def tokenization(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 9073\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = tokenization(nld_eng[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deutch Vocabulary Size: 12794\n"
     ]
    }
   ],
   "source": [
    "# prepare Dutch tokenizer\n",
    "nld_tokenizer = tokenization(nld_eng[:, 1])\n",
    "nld_vocab_size = len(nld_tokenizer.word_index) + 1\n",
    "print('Dutch Vocabulary Size: %d' % nld_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. **Write a function to convert tokens to sequences. Also perform sequence padding to a maximum sentence length. Input arguments to this function are tokenizer, maximum_length, and textlines, and its output will be sequences of tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, maximum_length, textlines):\n",
    "    # integer encode sequences\n",
    "    seq = tokenizer.texts_to_sequences(textlines)\n",
    "    # pad sequences with 0 values\n",
    "    seq = pad_sequences(seq, maxlen=maximum_length, padding='post')\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. **Convert your tokenized training data into sequences. Use a maximum length of 20 and name the dataframs train_X and train_Y.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_length = 20\n",
    "nld_length = 20\n",
    "# prepare training data\n",
    "train_X = encode_sequences(nld_tokenizer, nld_length, train[:, 1])\n",
    "train_Y = encode_sequences(eng_tokenizer, eng_length, train[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. **In the same way, convert your tokenized test data into sequences and name the dataframs test_X and test_Y.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "test_X = encode_sequences(nld_tokenizer, nld_length, test[:, 1])\n",
    "test_Y = encode_sequences(eng_tokenizer, eng_length, test[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. **Defince a Seq2Seq model architecture using an Embedding layer as the input layer, an LSTM layer as our encoder and another LSTM layer followed by a Dense layer as the decoder. Make this a function and name it build_model(). At least make the embedding_size and LSTMunits as input arguments for the function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build NMT model\n",
    "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, embedding_size, LSTMunits):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(in_vocab, embedding_size, input_length=in_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(LSTMunits))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(LSTMunits, return_sequences=True))\n",
    "    model.add(Dense(out_vocab, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11\\. **Create a model by calling the function with embedding_size = 300 and LSTMunits = 512.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(nld_vocab_size, eng_vocab_size, nld_length, eng_length, 300, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12\\. **Compile the model with the RMSprop optimizer and sparse_categorical_crossentropy for loss.**\n",
    "Note that we have used 'sparse_categorical_crossentropy' as the loss function as it allows us to use the target sequence as it is instead of one-hot encoded format. One-hot encoding the target sequences with such a huge vocabulary might consume your system's entire memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = optimizers.RMSprop(lr=0.001)\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13\\. **Fit the model with 5 epochs, validation_split of 0.2, and batch_size of 128**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "275/275 [==============================] - ETA: 0s - loss: 1.9605\n",
      "Epoch 00001: val_loss improved from inf to 1.73187, saving model to model.h1.09_jun_21\n",
      "INFO:tensorflow:Assets written to: model.h1.09_jun_21\\assets\n",
      "275/275 [==============================] - 441s 2s/step - loss: 1.9605 - val_loss: 1.7319\n",
      "Epoch 2/7\n",
      "275/275 [==============================] - ETA: 0s - loss: 1.6299\n",
      "Epoch 00002: val_loss improved from 1.73187 to 1.57127, saving model to model.h1.09_jun_21\n",
      "INFO:tensorflow:Assets written to: model.h1.09_jun_21\\assets\n",
      "275/275 [==============================] - 574s 2s/step - loss: 1.6299 - val_loss: 1.5713\n",
      "Epoch 3/7\n",
      "275/275 [==============================] - ETA: 0s - loss: 1.4751\n",
      "Epoch 00003: val_loss improved from 1.57127 to 1.45343, saving model to model.h1.09_jun_21\n",
      "INFO:tensorflow:Assets written to: model.h1.09_jun_21\\assets\n",
      "275/275 [==============================] - 615s 2s/step - loss: 1.4751 - val_loss: 1.4534\n",
      "Epoch 4/7\n",
      "275/275 [==============================] - ETA: 0s - loss: 1.3484\n",
      "Epoch 00004: val_loss improved from 1.45343 to 1.35369, saving model to model.h1.09_jun_21\n",
      "INFO:tensorflow:Assets written to: model.h1.09_jun_21\\assets\n",
      "275/275 [==============================] - 672s 2s/step - loss: 1.3484 - val_loss: 1.3537\n",
      "Epoch 5/7\n",
      "275/275 [==============================] - ETA: 0s - loss: 1.2401\n",
      "Epoch 00005: val_loss improved from 1.35369 to 1.27164, saving model to model.h1.09_jun_21\n",
      "INFO:tensorflow:Assets written to: model.h1.09_jun_21\\assets\n",
      "275/275 [==============================] - 647s 2s/step - loss: 1.2401 - val_loss: 1.2716\n",
      "Epoch 6/7\n",
      "275/275 [==============================] - ETA: 0s - loss: 1.1424\n",
      "Epoch 00006: val_loss improved from 1.27164 to 1.20789, saving model to model.h1.09_jun_21\n",
      "INFO:tensorflow:Assets written to: model.h1.09_jun_21\\assets\n",
      "275/275 [==============================] - 627s 2s/step - loss: 1.1424 - val_loss: 1.2079\n",
      "Epoch 7/7\n",
      " 47/275 [====>.........................] - ETA: 9:12 - loss: 1.0608"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-2965f96eb0f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m           \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m           callbacks=[checkpoint], verbose=1)\n\u001b[0m",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename = 'model.h1.09_jun_21'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), \n",
    "          epochs=7, batch_size=128, \n",
    "          validation_split = 0.2,\n",
    "          callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14\\. **Plot the loss for the training and validations sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-314735a9fdda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'validation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15\\. **Predict translations for the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-1f727fc9f0f2>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model.h1.09_jun_21')\n",
    "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you noticed preds are only indices of words, so we need to convert them to words to be able to read them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16\\. **Use the function get_word from below to convert an index to a word. Apply it on your predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions into text (English)\n",
    "preds_text = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], eng_tokenizer)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "             \n",
    "        else:\n",
    "            if(t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)            \n",
    "        \n",
    "    preds_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17\\. **Create a new dataframe with two columns where you show the actual text of the test set versus your predictions. Use the sample() function with your dataframe to randomly check some of the lines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i must refuse</td>\n",
       "      <td>i have to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>they are singers</td>\n",
       "      <td>theyre are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who switched off the light</td>\n",
       "      <td>who turned the  light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tom was really different</td>\n",
       "      <td>tom was really kind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do you need anything</td>\n",
       "      <td>do you need something</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>im fixing the clock</td>\n",
       "      <td>i am the  clock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i cant tell you when tom will get here</td>\n",
       "      <td>i cant tell you when come here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>he solved the problem with ease</td>\n",
       "      <td>he talked that to this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>im not suicidal</td>\n",
       "      <td>i dont have any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>please take another one</td>\n",
       "      <td>please dont a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>go ahead and ask tom</td>\n",
       "      <td>ask tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>do you want us to leave the room</td>\n",
       "      <td>do you want to  the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>you may have read this book already</td>\n",
       "      <td>you have read this   book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i have been to the us twice</td>\n",
       "      <td>i was in  the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>according to the tv it will rain tomorrow</td>\n",
       "      <td>will  the will next tomorrow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       actual  \\\n",
       "0                               i must refuse   \n",
       "1                            they are singers   \n",
       "2                  who switched off the light   \n",
       "3                    tom was really different   \n",
       "4                        do you need anything   \n",
       "5                         im fixing the clock   \n",
       "6      i cant tell you when tom will get here   \n",
       "7             he solved the problem with ease   \n",
       "8                             im not suicidal   \n",
       "9                     please take another one   \n",
       "10                       go ahead and ask tom   \n",
       "11           do you want us to leave the room   \n",
       "12        you may have read this book already   \n",
       "13                i have been to the us twice   \n",
       "14  according to the tv it will rain tomorrow   \n",
       "\n",
       "                                      predicted  \n",
       "0                    i have to                   \n",
       "1                  theyre are                    \n",
       "2          who turned the  light                 \n",
       "3           tom was really kind                  \n",
       "4         do you need something                  \n",
       "5                i am the  clock                 \n",
       "6   i cant tell you when come here               \n",
       "7         he talked that to this                 \n",
       "8               i dont have any                  \n",
       "9                please dont a                   \n",
       "10                    ask tom                    \n",
       "11            do you want to  the                \n",
       "12       you have read this   book               \n",
       "13                 i was in  the                 \n",
       "14   will  the will next tomorrow                "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i must refuse</td>\n",
       "      <td>i have to a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>they are singers</td>\n",
       "      <td>theyre they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who switched off the light</td>\n",
       "      <td>they the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tom was really different</td>\n",
       "      <td>tom is have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do you need anything</td>\n",
       "      <td>do you need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10990</th>\n",
       "      <td>i told you it was too soon</td>\n",
       "      <td>you   to the it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10991</th>\n",
       "      <td>whose book is this</td>\n",
       "      <td>this  is this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10992</th>\n",
       "      <td>get out</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10993</th>\n",
       "      <td>how did your interview go</td>\n",
       "      <td>how did you  this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10994</th>\n",
       "      <td>is eating healthy more expensive</td>\n",
       "      <td>the is to   me</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10995 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 actual           predicted\n",
       "0                         i must refuse       i have to a  \n",
       "1                      they are singers     theyre they    \n",
       "2            who switched off the light        they the    \n",
       "3              tom was really different      tom is have   \n",
       "4                  do you need anything      do you need   \n",
       "...                                 ...                 ...\n",
       "10990        i told you it was too soon     you   to the it\n",
       "10991                whose book is this     this  is this  \n",
       "10992                           get out            get     \n",
       "10993         how did your interview go  how did you  this \n",
       "10994  is eating healthy more expensive      the is to   me\n",
       "\n",
       "[10995 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>luckily the weather turned out fine</td>\n",
       "      <td>dont  to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>have you lost weight</td>\n",
       "      <td>do you like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>unfortunately she lives abroad</td>\n",
       "      <td>he was  in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9048</th>\n",
       "      <td>there was a pretty girl with black hair in the...</td>\n",
       "      <td>in a in  the days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6782</th>\n",
       "      <td>thats the way it goes</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8634</th>\n",
       "      <td>i think i was the one who suggested that</td>\n",
       "      <td>the   was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>he cured my illness</td>\n",
       "      <td>he was my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409</th>\n",
       "      <td>dont forget to tip the waiter</td>\n",
       "      <td>the   to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6093</th>\n",
       "      <td>is that a cow or a buffalo</td>\n",
       "      <td>my a  my  car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6187</th>\n",
       "      <td>tom probably doesnt know where mary is going t...</td>\n",
       "      <td>i when  to  him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>tom sold three refrigerators today</td>\n",
       "      <td>tom has three the years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>i like eating cake</td>\n",
       "      <td>i dont a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10853</th>\n",
       "      <td>youre timid</td>\n",
       "      <td>youre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5554</th>\n",
       "      <td>personal computers are very useful</td>\n",
       "      <td>his  are is very</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>tom worked</td>\n",
       "      <td>tom is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  actual  \\\n",
       "1542                 luckily the weather turned out fine   \n",
       "2595                                have you lost weight   \n",
       "1315                      unfortunately she lives abroad   \n",
       "9048   there was a pretty girl with black hair in the...   \n",
       "6782                               thats the way it goes   \n",
       "8634            i think i was the one who suggested that   \n",
       "2541                                 he cured my illness   \n",
       "2409                       dont forget to tip the waiter   \n",
       "6093                          is that a cow or a buffalo   \n",
       "6187   tom probably doesnt know where mary is going t...   \n",
       "9973                  tom sold three refrigerators today   \n",
       "3245                                  i like eating cake   \n",
       "10853                                        youre timid   \n",
       "5554                  personal computers are very useful   \n",
       "2397                                          tom worked   \n",
       "\n",
       "                      predicted  \n",
       "1542                dont  to     \n",
       "2595             do you like     \n",
       "1315               he was  in    \n",
       "9048          in a in  the days  \n",
       "6782                    is       \n",
       "8634                the   was    \n",
       "2541               he was my     \n",
       "2409                 the   to    \n",
       "6093              my a  my  car  \n",
       "6187            i when  to  him  \n",
       "9973   tom has three the years   \n",
       "3245                i dont a     \n",
       "10853                youre       \n",
       "5554          his  are is very   \n",
       "2397                 tom is      "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18\\. **You may not get very good results mainly because of the memory/time limit that you did not create a very efficient model. For this, we already trained a model with 30 epochs on the entire training dataset. Load this model with load_model('model.h1.09_jun_21') and repeat the steps in 15, 16 and 17 again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h1.09_jun_21')\n",
    "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

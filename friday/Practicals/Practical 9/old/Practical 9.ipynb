{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNyZcilgoIsB"
   },
   "source": [
    "\n",
    "# Practical 9: Applications of text mining and NLP\n",
    "Developed by Javier Garcia-Bernardo, Anastasia Giachanou. Updated by Pablo Mosteiro.\n",
    "\n",
    "**Applied Text Mining - Utrecht Summer School**\n",
    "\n",
    "In this practical you will be answering a research question or solving a problem. For that you will create a pipeline for classification or clustering.\n",
    "\n",
    "All the data is processed and can be found on the github repository.\n",
    "\n",
    "Here are some proposed research questions:\n",
    "\n",
    "## Classification\n",
    "\n",
    "#### RQ1: Identification of fake news, hate speech or spam + Interpretability of results:\n",
    "\n",
    "Data:\n",
    "\n",
    "- https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset or\n",
    "\n",
    "- https://github.com/aitor-garcia-p/hate-speech-dataset (https://paperswithcode.com/dataset/hate-speech) or\n",
    "\n",
    "- https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection\n",
    "\n",
    "Goal: Evaluate performance of different methods and interpret the results using LIME\n",
    "\n",
    "#### RQ2: Evaluate the importance of metadata. Create a classification system to identify the movie genre using and excluding metadata:\n",
    "\n",
    "Data: https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots\n",
    "\n",
    "Options:\n",
    "\n",
    "- Create two classifications systems, one using only metadata, one using only text. Stack them to create the best model: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html\n",
    "\n",
    "- Use the functional API of Keras to create one model that handles both types of inputs: https://pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
    "\n",
    "Goal: Evaluate performance and interpret the results using LIME\n",
    "\n",
    "## Clustering:\n",
    "\n",
    "#### RQ3: Create a recommendation system for movies based on their plot:\n",
    "\n",
    "Data: https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots\n",
    "\n",
    "Output: What are the closest movies to \"The Shawshank Redemption\", \"Goodfellas\", and \"Harry Potter and the Sorcerer's Stone\"?\n",
    "\n",
    "#### RQ4: Cluster headlines using word embeddings:\n",
    "\n",
    "- Data: https://www.ims.uni-stuttgart.de/en/research/resources/corpora/goodnewseveryone/ (https://aclanthology.org/2020.lrec-1.194.pdf)\n",
    "\n",
    "- Do the clusters correlate to emotions or media sources?\n",
    "You can come up with your own research question using any dataset on text analysis, e.g. from:\n",
    "\n",
    "- UCI repository: https://archive.ics.uci.edu/ml/datasets.php?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table\n",
    "\n",
    "- Papers with code repository: https://paperswithcode.com/datasets?mod=texts&page=1\n",
    "\n",
    "- Kaggle (code examples are often included): https://www.kaggle.com/datasets?tags=13204-NLP (but given the time restrictions, choosing one of the above is recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3qWZ38aEKbu"
   },
   "outputs": [],
   "source": [
    "# path to the data\n",
    "path_data = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5n8R8NRoFN25",
    "outputId": "97e11b7e-711f-4727-b038-27b796e6714b"
   },
   "outputs": [],
   "source": [
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Interpretable AI\n",
    "!pip install lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# data_rq1_fake = pd.read_csv(\"rq1_fake_news.csv.gzip\",sep=\"\\t\",compression=\"gzip\")\n",
    "# data_rq1_hate_speech = pd.read_csv(\"rq1_hate_speech.csv.gzip\",sep=\"\\t\",compression=\"gzip\")\n",
    "# data_rq1_youtube = pd.read_csv(\"rq1_youtube.csv.gzip\",sep=\"\\t\",compression=\"gzip\")\n",
    "# data_rq2_3 = pd.read_csv(\"rq2_3_wiki_movie_plots.csv.gzip\",sep=\"\\t\",compression=\"gzip\")\n",
    "# data_rq4 = pd.read_csv(\"rq4_gne-release-v1.0.csv.gzip\",sep=\"\\t\",compression=\"gzip\")\n",
    "# data_rq1_fake.shape, data_rq1_hate_speech.shape, data_rq1_youtube.shape, data_rq2_3.shape, data_rq4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKwY5lCupVZR"
   },
   "source": [
    "\n",
    "### RQ1: Identification of hate speech\n",
    "Data on hate speech: https://github.com/aitor-garcia-p/hate-speech-dataset (https://paperswithcode.com/dataset/hate-speech)\n",
    "Data on fake vs real news: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
    "Data on youtube spam messages: https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection\n",
    "We provide code for the first dataset. Your goal is to improve the classifier by using a more advanced method\n",
    "\n",
    "Data: Dataset of hate speech annotated on Internet forum posts in English at sentence-level. The source forum in Stormfront, a large online community of white nacionalists. A total of 10,568 sentence have been been extracted from Stormfront and classified as conveying hate speech or not\n",
    "\n",
    "#### Step 1: Read data and create train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "E83rPp9YFow6",
    "outputId": "1461a915-52c8-4693-9c34-13b4eb1503f2"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{path_data}/rq1_hate_speech.csv.gzip\",sep=\"\\t\",compression=\"gzip\", index_col=0)\n",
    "df[\"label\"] = df[\"label\"].map({\"hate\": 1, \"noHate\": 0})\n",
    "df = df[[\"text\",\"label\"]]\n",
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwmTq7mWFo0i"
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"].values, df[\"label\"].values, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvOlejYZpoAN"
   },
   "source": [
    "### Step 2: Create pipeline and hyperparameter tuning\n",
    "\n",
    "Create a pipeline that vectorizes the text and transform it using TF-IDF, and classifies the news titles using LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBux7Uu8Fo3b",
    "outputId": "9b687632-8f32-40a6-b8f0-7f0b22fb3344"
   },
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english',  #remove stopwords\n",
    "                                   lowercase=True, #convert to lowercase\n",
    "                                   token_pattern=r'(?u)\\b[A-Za-z][A-Za-z]+\\b')), #tokens of at least 2 characters\n",
    "    ('clf', LogisticRegression(max_iter=10000, dual=False, solver=\"saga\")) #logistic regression\n",
    "])\n",
    "\n",
    "\n",
    "# Parameters to hyptertune\n",
    "param_grid = dict(vectorizer__ngram_range=[(1,1), (1,2), (1,3)], # creation of n-grams\n",
    "                  vectorizer__min_df=[1, 10, 100], # minimum support for words\n",
    "                  clf__C=[0.1, 1, 10, 100], # regularization\n",
    "                  clf__penalty=[\"l2\",\"l1\"]) # type of regularization\n",
    "\n",
    "# Run a grid search using cross-validation to find the best parameters\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, verbose=True, n_jobs=-1)\n",
    "\n",
    "# to speed it up we find the hyperparameters using a sample, and fit on the entire datast later\n",
    "grid_search.fit(X_train[:1000], y_train[:1000])\n",
    "\n",
    "# best parameters, score and estimator\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "-gfPpHfwFo6q",
    "outputId": "6be4b5c0-ccd0-4181-d69f-3b291fb99b2f"
   },
   "outputs": [],
   "source": [
    "# print resutls\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results.sort_values(by=\"mean_test_score\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UoSHT5nFo-p"
   },
   "outputs": [],
   "source": [
    "# Use the best parameters in the pipe and fit with the entire dataset\n",
    "pipe = pipe.set_params(**grid_search.best_params_)\n",
    "clf_best = pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7Ik8HHZGdNI",
    "outputId": "a848e5db-0c50-4ee5-b009-37c4581de7d2"
   },
   "outputs": [],
   "source": [
    "# print vocabulary size\n",
    "print(len(clf_best[\"vectorizer\"].get_feature_names_out()))\n",
    "\n",
    "#vocabulary\n",
    "#clf_best[\"vectorizer\"].vocabulary_\n",
    "\n",
    "# the best score achieved\n",
    "print(clf_best.score(X_train, y_train))\n",
    "# the best score achieved\n",
    "print(clf_best.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "oDS9Jb2xGdQQ",
    "outputId": "ca42f4fc-ec1a-4791-8b4e-5bdc8f63aa29"
   },
   "outputs": [],
   "source": [
    "# Add predicitons to dataframe\n",
    "df[\"predicted\"] = clf_best.predict(df[\"text\"])\n",
    "df[\"predicted_prob_hate\"] = clf_best.predict_proba(df[\"text\"])[:,1]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WArko5Xpr8B6"
   },
   "source": [
    "### Step 3: Interpretation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzdeXkcbsFK_"
   },
   "source": [
    "### Interpretation of coefficients in the linear model\n",
    "\n",
    "We can use the coefficients of the Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTmHg9w3GdTi"
   },
   "outputs": [],
   "source": [
    "# Extract the coeficients from the omdel\n",
    "coefs = pd.DataFrame([clf_best[\"vectorizer\"].get_feature_names_out(),\n",
    "                      clf_best[\"clf\"].coef_[0]]).T\n",
    "coefs.columns = [\"gram\",\"coef\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "-ce_pmVmGdWZ",
    "outputId": "2ae141ad-af95-443b-c759-7626cdb33155"
   },
   "outputs": [],
   "source": [
    "# top words influencing hate\n",
    "display(coefs.sort_values(by=\"coef\", ascending=False).head(10))\n",
    "\n",
    "# top words influencing non-hate\n",
    "display(coefs.sort_values(by=\"coef\", ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01mwhvCpsV7r"
   },
   "source": [
    "### Interpretation of coefficients using LIME (Local Interpretable Model-Agnostic Explanations)\n",
    "\n",
    "LIME modifies the text to understand the impact of each word to the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0PZqdYnGdZo",
    "outputId": "35761a19-3194-4b6e-8582-5764ce96b267"
   },
   "outputs": [],
   "source": [
    "# Find some extreme examples\n",
    "df_confused = df.loc[df[\"label\"] != df[\"predicted\"]]\n",
    "pred_hate_not_hate = (df_confused.loc[df_confused[\"label\"]==0].sort_values(by=\"predicted_prob_hate\").tail(1).values[0][0])\n",
    "pred_not_hate_hate = df_confused.loc[df_confused[\"label\"]==1].sort_values(by=\"predicted_prob_hate\").head(1).values[0][0]\n",
    "\n",
    "print(\"Here\")\n",
    "\n",
    "less_hate = df.sort_values(by=\"predicted_prob_hate\").head(1).values[0][0]\n",
    "most_hate = df.sort_values(by=\"predicted_prob_hate\").tail(1).values[0][0]\n",
    "\n",
    "pred_50_50 = \"She says the class is out of control and the kids are unteachable , and the black administration does not support her \"\n",
    "\n",
    "print(\"Least hate: \", less_hate)\n",
    "print(\"Most hate: \", most_hate)\n",
    "print(\"Predicted very hate but not hateful: \", pred_hate_not_hate)\n",
    "print(\"Predicted very innocuous but hateful: \", pred_not_hate_hate)\n",
    "print(\"Predicted 50/50: \", pred_50_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YAP_fhccGr4x",
    "outputId": "53934245-0561-4891-eb76-cc605f1fb987"
   },
   "outputs": [],
   "source": [
    "# start the explainer\n",
    "explainer = LimeTextExplainer(class_names = [\"Innocuous\", \"Hateful\"], bow=False)\n",
    "\n",
    "# shows the explanation for our example instances\n",
    "for text in [less_hate, most_hate, pred_hate_not_hate, pred_not_hate_hate, pred_50_50]:\n",
    "    exp = explainer.explain_instance(text,\n",
    "                                     clf_best.predict_proba,\n",
    "                                     num_features = 10,\n",
    "                                    num_samples = 1000)\n",
    "    exp.show_in_notebook(text=text)\n",
    "    print(exp.as_list())\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "RhtIshx-Gr8C",
    "outputId": "e1afd218-1c12-48a2-fbf3-9847a7fd32e4"
   },
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(\"I believe Dutch people have inferior food and they should be colonized by Belgium\",\n",
    "                                 clf_best.predict_proba,\n",
    "                                 num_features = 10,\n",
    "                                num_samples = 1000)\n",
    "exp.show_in_notebook(text=text)\n",
    "print(exp.as_list())\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKm_SWHItMh7"
   },
   "source": [
    "\n",
    "## Now it's your turn.\n",
    "\n",
    "Either:\n",
    "\n",
    "*   Adapt RQ1 using different models (e.g. a CNN, as shown below) or data (either the ones described under RQ1, or any other)\n",
    "\n",
    "*   Or start on a different RQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKAyvgjqGddR",
    "outputId": "345c6e74-7d99-4db5-b114-795a270e4a2f"
   },
   "outputs": [],
   "source": [
    "!pip install scikeras\n",
    "!pip install Keras-Preprocessing\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import Sequential\n",
    "from keras import layers, utils\n",
    "\n",
    "def plot_history(history, val=0):\n",
    "    acc = history['accuracy']\n",
    "    if val == 1:\n",
    "        val_acc = history['val_accuracy'] # we can add a validation set in our fit function with nn\n",
    "    loss = history['loss']\n",
    "    if val == 1:\n",
    "        val_loss = history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training accuracy')\n",
    "    if val == 1:\n",
    "        plt.plot(x, val_acc, 'r', label='Validation accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    if val == 1:\n",
    "        plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CWhEPbYLHNd"
   },
   "outputs": [],
   "source": [
    "## CREATE MODEL\n",
    "def create_model(num_filters=64, kernel_size=3, embedding_dim=50, maxlen=100, num_classes=2):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    x = tensorflow.ones((1, maxlen))\n",
    "    y = model(x)\n",
    "    return model\n",
    "\n",
    "## CLASS FOR PREPROCESSING (needed to work with pipelines)\n",
    "class preprocessing():\n",
    "    def __init__(self, num_words=20000, maxlen=100):\n",
    "        self.maxlen = maxlen\n",
    "        self.tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.tokenizer.fit_on_texts(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_ = self.tokenizer.texts_to_sequences(X)\n",
    "        return pad_sequences(X_, padding='post', maxlen=self.maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "YhVbqIANLznX",
    "outputId": "48f1f3d0-5410-4cde-b79f-a1a7114648d3"
   },
   "outputs": [],
   "source": [
    "## PROCESS DATA\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"].values, df[\"label\"].values, test_size=0.33, random_state=42)\n",
    "\n",
    "# Encode the list of newsgroups into categorical integer values\n",
    "y_train = utils.to_categorical(y_train)\n",
    "y_test = utils.to_categorical(y_test)\n",
    "\n",
    "## CREATE PIPELINE\n",
    "# Use the best parameters in the pipe and fit with the entire dataset\n",
    "pipe_preproc = Pipeline([\n",
    "    (\"preproc\", preprocessing())])\n",
    "\n",
    "pipe_est = Pipeline([\n",
    "    ('clf', KerasClassifier(model=create_model,\n",
    "                        epochs = 10,\n",
    "                        batch_size=64,\n",
    "                        verbose=True,\n",
    "                        num_filters=32 )) #logistic regression\n",
    "])\n",
    "\n",
    "pipe_preproc.fit(X_train)\n",
    "X_train_p = pipe_preproc.transform(X_train)\n",
    "X_test_p = pipe_preproc.transform(X_test)\n",
    "vocab_size = len(pipe_preproc[\"preproc\"].tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "\n",
    "# test it works\n",
    "pipe_est.fit(X_train_p[:500], y_train[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "FJS5dwbUMFyr",
    "outputId": "66d5e2be-f623-41f9-dac3-5f34b6ec8075"
   },
   "outputs": [],
   "source": [
    "pipe_est[\"clf\"].model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W76VDUFsMF1Q",
    "outputId": "bafa7d05-0646-4a1a-db00-08da40103920"
   },
   "outputs": [],
   "source": [
    "## HYPERPARAMETER TUNING\n",
    "param_grid = dict(clf__model__num_filters=[32, 64, 128],\n",
    "                  clf__model__kernel_size=[3, 5, 7],\n",
    "                  clf__model__embedding_dim=[50, 100],\n",
    "                 clf__verbose=[False])\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=pipe_est,\n",
    "                          param_distributions=param_grid,\n",
    "                          cv=5,\n",
    "                          n_jobs=-1,\n",
    "                          verbose=True,\n",
    "                         n_iter=10)\n",
    "\n",
    "grid.fit(X_train_p[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjBPjH-HMF33"
   },
   "outputs": [],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhjY2W6BMF6u"
   },
   "outputs": [],
   "source": [
    "# Use the best parameters in the pipe and fit with the entire dataset\n",
    "clf_best = grid.best_estimator_\n",
    "clf_best = pipe_est.fit(X_train_p, y_train,\n",
    "                   clf__validation_data=(X_test_p, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DyWLn3hMF9t"
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "plot_history(clf_best[\"clf\"].history_, val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55PL3ge1MccE"
   },
   "outputs": [],
   "source": [
    "# Use the best epochs\n",
    "clf_best = grid.best_estimator_\n",
    "clf_best = pipe_est.fit(X_train_p, y_train,\n",
    "                   clf__validation_data=(X_test_p, y_test),\n",
    "                   clf__epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XcEFuO2McfC"
   },
   "outputs": [],
   "source": [
    "# Find some extreme examples\n",
    "less_hate = \"- YouTube\"\n",
    "most_hate = \"Look what happens when Whites leave black Countries alone to do what they do naturally The blacks in White Countries today should be on their knees thanking Whites for trying to civilize them\"\n",
    "pred_hate_not_hate = \"Too many whites think they deserve what negroes dish out because of guilt .\"\n",
    "pred_not_hate_hate = \"https://www.stormfront.org/forum/t1020784/ https : //www.stormfront.org/forum/t102 ... ghlight = sweden https : //www.stormfront.org/forum/t102 ... ghlight = sweden https : //www.stormfront.org/forum/t101 ... ghlight = sweden https : //www.stormfront.org/forum/t101 ... ghlight = sweden https : //www.stormfront.org/forum/t100 ... ghlight = sweden https : //www.stormfront.org/forum/t100 ... ghlight = sweden https : //www.stormfront.org/forum/t100 ... ghlight = sweden God save them .....\"\n",
    "pred_50_50 = \"She says the class is out of control and the kids are unteachable , and the black administration does not support her \"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Least hate: \", less_hate)\n",
    "print(\"Most hate: \", most_hate)\n",
    "print(\"Predicted very hate but not hate: \", pred_hate_not_hate)\n",
    "print(\"Predicted non hate but hate: \", pred_not_hate_hate)\n",
    "print(\"Predicted 50/50: \", pred_50_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bT0wT2d4MciC"
   },
   "outputs": [],
   "source": [
    "# start the explainer\n",
    "explainer = LimeTextExplainer(class_names = [\"Innocuous\", \"Hate\"], bow=False)\n",
    "\n",
    "# relying on global objects (not too nice)\n",
    "def create_proba(text):\n",
    "    t = pipe_preproc.transform(text)\n",
    "    return clf_best.predict_proba(t)\n",
    "\n",
    "\n",
    "# shows the explanation for our example instances\n",
    "for text in [less_hate, most_hate, pred_hate_not_hate, pred_not_hate_hate, pred_50_50]:\n",
    "    exp = explainer.explain_instance(text,\n",
    "                                     create_proba,\n",
    "                                     num_features = 10,\n",
    "                                    num_samples = 1000)\n",
    "    exp.show_in_notebook(text=text)\n",
    "    print(exp.as_list())\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxhDwWIENRAM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

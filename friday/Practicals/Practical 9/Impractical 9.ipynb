{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 9: LLMs pre-training, prompting, & learning from human feedback\n",
    "\n",
    "Dong Nguyen\n",
    "\n",
    "Applied Text Mining - Utrecht Summer School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkCzAb_QEvGH"
   },
   "source": [
    "# Settings\n",
    "To run this notebook, use GPU or TPU. In Google Colab, select T4. ('**Change runtime type**').\n",
    "\n",
    "We're going to use the Hugging Face Transformers library, which is a very popular Python library/platform for working with language models.\n",
    "See more at https://huggingface.co/docs/transformers/en/index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUYA7ofkdoP6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rK_pQutA0JTe"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets==2.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OD5GVRme1H4J"
   },
   "source": [
    "# Phi-3-mini-4k-instruct\n",
    "\n",
    "The model below loads in a pre-trained LLM (Phi-3-mini-4k-instruct; 3.8B).\n",
    "\n",
    "Take a look at https://huggingface.co/microsoft/Phi-3-mini-4k-instruct to read more about Phi-3-mini-4k-instruct.\n",
    "\n",
    "\n",
    "**Tip:** Run the code below (which can take a few - 10 minutes), and look at the\n",
    "webpage in between.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBMtRZN50Sd0"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",  # store the model on GPU\n",
    "    torch_dtype=\"auto\",  # automatically determines the best data type\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQQFRMGx0qLE"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z071ubdYjCYT"
   },
   "source": [
    "Let's prompt the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkdVU1H53r4z"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Where is Utrecht?\"}\n",
    "]\n",
    "\n",
    "output = generator(messages)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYCXDVjl43L4"
   },
   "source": [
    "**Experiment with the following:**\n",
    "\n",
    "* `return_full_text` controls whether the input prompt is returned as well.\n",
    "   Experiment with `True` and `False`.\n",
    "* `max_new_tokens` The number of maximum tokens to generate. Experiment with different values.\n",
    "* Different prompts. Experiment with both factual and more subjective questions.\n",
    "* Experiment with deterministic generation (`do_sample=False`) and non-deterministic generation (`do_sample=True`). When you do sample, you can also set the temperature parameter. Try out different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMNpeYAgO4Oj"
   },
   "source": [
    "# System message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp8zXDiXO56K"
   },
   "source": [
    "With the system message we can set the overall behavior of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xDyPyF_O-Fr"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Respond as if you're a 15-year old girl named Lisa, who loves thrillers.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is your favorite movie?\"}\n",
    "]\n",
    "print(generator(messages)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w542bMEgPfcT"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You're a 50-year-old man named Dave, who has a dry sense of humor and loves sci-fi movies.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is your favorite movie?\"}\n",
    "]\n",
    "print(generator(messages)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zecGdfDzR1LG"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a high school teacher.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain photosynthesis to 13 year old. \"}\n",
    "]\n",
    "print(generator(messages)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWfq0eJFRi_V"
   },
   "source": [
    "**Experiment with the following:**\n",
    "\n",
    "* Experiment with different prompts and system messages, to simulate certain personas or to steer the behavior of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI347kRt-Cys"
   },
   "source": [
    "# Simulate a chat history\n",
    "\n",
    "We can input a list of system / user messages to simulate a longer history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNJKzhTBj0Pp"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who wrote 'Pride and Prejudice'?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Jane Austen wrote 'Pride and Prejudice'.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What else did she write?\"}\n",
    "]\n",
    "print(generator(messages)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnwAYWWim6WE"
   },
   "source": [
    "**Exercise**: Experiment with a few more examples where context can make a difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD2GwShPne4-"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8x540YCrapy8"
   },
   "source": [
    "We're going to experiment with sentiment classification, and load in the SST2 dataset, which contains sentences from movie reviews (negative=0 and positive=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9iGYUbvnzh1"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sentiment dataset, only the first 10 instances\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"validation[:10]\")\n",
    "\n",
    "# Pipeline for zero-shot prompting\n",
    "classification_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model= model,\n",
    "    tokenizer= tokenizer,\n",
    "    max_new_tokens= 50,\n",
    "    do_sample= False,\n",
    "    return_full_text = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toymaCaNa_Gr"
   },
   "source": [
    "Print the first two instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhO1BRimaihX"
   },
   "outputs": [],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoER9w9IoHyv"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Format and run examples\n",
    "for example in dataset:\n",
    "    text = example[\"sentence\"]\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Is the sentence below Positive or Negative? Only answer with Positive or Negative.\n",
    "\n",
    "### Text:\n",
    "\"{text}\"\n",
    "\n",
    "### Sentiment:\"\"\"\n",
    "    messages = [\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    "\n",
    "\n",
    "    output = classification_generator(messages)[0]['generated_text']\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Sentiment: {output}\")\n",
    "    print(\"---\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fO3V34nPd1kT"
   },
   "source": [
    "**Exercise**\n",
    "Experiment with different prompts, for example, you can ask for an explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S93kbD4t8v32"
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nbo_KUSETL_S"
   },
   "source": [
    "To get a sense of the tokenizer used, you can print the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toZsbx1O8cTI"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer(\"Where is Utrecht?\")\n",
    "\n",
    "print(tokens)\n",
    "print(tokenizer.convert_ids_to_tokens(tokens['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tB64xnjf8618"
   },
   "source": [
    "**Exercise**\n",
    "Experiment with uncommon words, misspelled words, dialect words, or word that don't exist.\n",
    "\n",
    "For example:\n",
    "* *I like this so muhc* vs *I like this so muhc*\n",
    "* *This is so coooooool*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjH7HLFqTPp3"
   },
   "source": [
    "Print a subset of the tokens in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0rjkK9q9B4O"
   },
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Sort the vocabulary by token ID to get the \"first\" tokens\n",
    "sorted_vocab = sorted(vocab.items(), key=lambda item: item[1])\n",
    "\n",
    "# Print some tokens\n",
    "for token, token_id in sorted_vocab[1000:1050]:\n",
    "    print(f\"{token_id:>3}: {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQLOy3xbZdL2"
   },
   "source": [
    "# If you have the time: experiment with another model\n",
    "\n",
    "You can experiment with the `HuggingFaceTB/SmolLM3-3B` model,\n",
    "which was very recently released https://huggingface.co/HuggingFaceTB/SmolLM3-3B.\n",
    "Note that *extended thinking* is enabled by default, whichgenerates the output with a reasoning trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKESYtW4BSEk"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# if you run into out of memory error, you can either restart the notebook\n",
    "# and just load this model, explicitly delete the previous model\n",
    "# from memory\n",
    "\n",
    "# del model\n",
    "# del tokenizer\n",
    "# del generator\n",
    "#gc.collect()\n",
    "#torch.cuda.empty_cache()\n",
    "#print(torch.cuda.memory_allocated())\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "smol_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolLM3-3B\",\n",
    "    device_map=\"cuda\",  # store the model on GPU\n",
    "    torch_dtype=\"auto\",  # automatically determines the best data type\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "smol_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM3-3B\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8PwTC7ZUNjh"
   },
   "outputs": [],
   "source": [
    "smol_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=smol_model,\n",
    "    tokenizer=smol_tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True, ## apply sampling\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How are you?\"}\n",
    "]\n",
    "\n",
    "print(smol_generator(messages))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB_ukytgMJ3e"
      },
      "source": [
        "# Practical 4: Text Clustering and Topic Modeling\n",
        "#### Ayoub Bagheri\n",
        "<img src=\"img/uu_logo.png\" alt=\"logo\" align=\"right\" title=\"UU\" width=\"50\" height=\"20\" />\n",
        "\n",
        "#### Applied Text Mining - Utrecht Summer School"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2e5YQ5qMJ3g"
      },
      "source": [
        "In this practical, we are going to apply different clustering algorithms and a topic modeling approach on sport news articles and cluster them into different groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7hS5r12MJ3i"
      },
      "source": [
        "Today we will use the following libraries. Take care to have them installed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6-6XC06LMJ3i"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# for reproducibility\n",
        "random_state = 321"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_p1D58JMJ3j"
      },
      "source": [
        "### Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF7F1dU5MJ3j"
      },
      "source": [
        "1\\. **Here we are going to use a another set of the BBC news articles, the BBC Sport dataset. This dataset provided for use as benchmarks for machine learning research. The BBC Sport dataset consists of 737 documents from the BBC Sport website corresponding to sports news articles from 2004-2005 in five topical areas: athletics, cricket, football, rugby, tennis. Upload the `bbcsport-fulltext.zip` file and extract it using the code below. Convert the resulting object to a dataframe.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlpK7vbqMJ3m"
      },
      "source": [
        "2\\. **For text clustering and topic modeling, we will ignore the outcome variable (labels) but we will use them while evaluating models. Create a copied dataframe removing the outcome variable.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZpnZU-SMJ3n"
      },
      "source": [
        "3\\. **Apply the following pre-processing steps and convert the data to a document-term matrix with term frequencies:**\n",
        "- convert to lowercase\n",
        "- remove stopwords\n",
        "- remove numbers\n",
        "- extract uni- and bi-grams\n",
        "- remove terms that occur in less than 2 documents\n",
        "- remove one-letter terms, e.g.'a', or 's'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAD2-LM5MJ3o"
      },
      "outputs": [],
      "source": [
        "# You can check the vocabulary of your vectorizer with the following line\n",
        "# tfidf_vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJm9SSQDMJ3o"
      },
      "source": [
        "### K-Means clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sxdIGvfMJ3o"
      },
      "source": [
        "4\\. **Use the MiniBatchKMeans function from the sklearn package, and a K-Means clustering algorithm with 5 clusters.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcNZMv0zMJ3p"
      },
      "source": [
        "5\\. **What are the top terms in each cluster?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0c_3kx3MJ3p"
      },
      "source": [
        "Based on these 10 top terms we can manually label our clustering output as:\n",
        "- cluster 0: cricket\n",
        "- cluster 1: tennis\n",
        "- cluster 2: football\n",
        "- cluster 3: athletics\n",
        "- cluster 4: rugby"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyxclqc2MJ3q"
      },
      "source": [
        "6\\. **Visualize the output of the K-Means clustering: first apply a PCA method to transform the high-dimensional feature space into 2 dimensions, and then plot the points using a scatter plot.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq5vCl9MMJ3q"
      },
      "source": [
        "7\\. **Evaluate the quality of the K-Means clustering with the `sklearn` metrics for clustering: `homogeneity_score`, `completeness_score`, `v_measure_score`, `adjusted_rand_score`, `silhouette_score`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJXqzb7mMJ3q"
      },
      "source": [
        "Evaluation for unsupervised learning algorithms is a bit difficult and requires human judgement but there are some metrics which you might use. There are two kinds of metrics you can use depending on whether or not you have the labels.\n",
        "\n",
        "If you have a labelled dataset you can use metrics that give you an idea of how good your clustering model is. For this purpose you can use the `sklearn.metrics` module, for example `homogeneity_score` is one of the possible metrics. As per the documentation, the score ranges between 0 and 1 where 1 stands for perfectly homogeneous labeling.\n",
        "\n",
        "If you do not have labels for your dataset, then you can still evaluate your clustering model with some metrics. One of them is the `silhouette_score`. From the `sklearn`'s documentation: The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) / max(a,b). To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of. The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doUmqEeCMJ3r"
      },
      "source": [
        "8\\. **Apply the K-Means clustering method on a range of 3 to 7 clusters, and calculate the squared loss obtained in each clustering. Apply the Elbow method to find the optimal k. (Tip: use the `cls.inertia_` for the squared loss)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy9tSwvXMJ3s"
      },
      "source": [
        "9\\. **Use the following two news articles as your test data, and predict cluster labels for your new dataset with the best value for K and the K-Means algorithm.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CEOiENjEMJ3s"
      },
      "outputs": [],
      "source": [
        "documents = ['Frank de Boer out as Oranje manager after early Euro 2020 exit Dutch menâ€™s football team coach.',\n",
        "             'The time has come for Nadal to be selective in the events that he should and should not play. This is where he can start the difficulty. After a rigorous participation of the clay season, Rafael Nadal definitely wants to conserve his energies for as long as possible.']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5DR-hCqMJ3t"
      },
      "source": [
        "### Hierarchical clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjW8yG3sMJ3t"
      },
      "source": [
        "10\\. **Hierarchical clustering is a type of unsupervised machine learning algorithm used to cluster unlabeled data points. Similar to the K-Means clustering, hierarchical clustering groups the data points with similar characteristics together. Apply a hierarchical clustering with the ward linkage on the BBC Sport dataset. Fit the model with 5 clusters and check the predicted labels.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN4sZYsOMJ3t"
      },
      "source": [
        "11\\. **Plot a dendrogram for your hierarchical clustering model using the function below. To do this, you need to fit the model again without assigning the number of clusters.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsQebCcuMJ3u"
      },
      "source": [
        "### Topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaDRCjQqMJ3u"
      },
      "source": [
        "Topic modeling is another unsupervised method for text mining applications where we want to get an idea of what topics we have in our dataset. A topic is a collection of words that describe the overall theme. For example, in case of news articles, you might think of topics as the categories in the dataset. Just like clustering algorithms, there are some algorithms that need you to specify the number of topics you want to extract from the dataset and some that automatically determine the number of topics. Here, we are going to use the Latent Dirichlet Allocation (LDA) method for topic modeling. You can check sklearn's documentation for more details about LDA from the `sklearn.decomposition` module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOE5yIi5MJ3u"
      },
      "source": [
        "12\\. **One of the mainly used approaches for topic modeling is Latent Dirichlet Allocation (LDA). The LDA is based upon two general assumptions:**\n",
        "\n",
        "- Documents exhibit multiple topics\n",
        "- A topic is a distribution over a fixed vocabulary\n",
        "\n",
        "**Train a LDA model from the `sklearn` package for topic modeling with 5 components.****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAZvpTaeMJ3v"
      },
      "source": [
        "We used LDA to create topics along with the probability distribution for each word in our vocabulary for each topic. The parameter `n_components` specifies the number of categories, or topics, that we want our text to be divided into."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg-Ty2hDMJ3v"
      },
      "source": [
        "13\\. **Print the 10 words with highest probabilities for all the five topics.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H14AbtBMJ3v"
      },
      "source": [
        "You can also use the following function for this purpose:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xKlyxLFwMJ3v"
      },
      "outputs": [],
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    topic_dict = {}\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
        "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
        "    return pd.DataFrame(topic_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQx4WBe4MJ3w"
      },
      "source": [
        "14\\. **Transform the learned topics into your data. Check the shape of the output. What can be the use of this output?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18CguCjoMJ3w"
      },
      "source": [
        "This output can be used as our new observations and features for further tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2jyh0V_MJ3x"
      },
      "source": [
        "15\\. **Use the `score` function for LDA to calculate the log likelihood for your data. Compare two LDA models with 5 and 10 topics.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VOm1V5HMJ3x"
      },
      "source": [
        "Many procedures use the log of the likelihood, rather than the likelihood itself, because it is easier to work with. The log likelihood (i.e., the log of the likelihood) will always be negative, with higher values (closer to zero) indicating a better fitting model. Here this belongs to the model with 5 topics."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

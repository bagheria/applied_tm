{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 3: Feature Selection & Dimension Reduction\n",
    "#### Ayoub Bagheri\n",
    "<img src=\"img/uu_logo.png\" alt=\"logo\" align=\"right\" title=\"UU\" width=\"50\" height=\"20\" />\n",
    "\n",
    "#### Applied Text Mining - Utrecht Summer School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical, we are going to learn about feature selection and dimension reduction methods for text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will use the following libraries. Take care to have them installed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. **Here we are going to use a news article data set, originating from BBC News (http://mlg.ucd.ie/datasets/bbc.html). This data set provided for use as benchmarks for machine learning research. The BBC data set consists of 2225 documents and 5 categories: business, entertainment, politics, sport, and tech. Your first task is to load the dataset so that you can proceed. Do not forget to import the necessary dependencies, you are going to need. You can import the other ones as you go along.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chris Evans back on the market\\n\\nBroadcaster ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Giggs handed Wales leading role\\n\\nRyan Giggs ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wales silent on Grand Slam talk\\n\\nRhys Willia...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kenya lift Chepkemei's suspension\\n\\nKenya's a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lee to create new film superhero\\n\\nComic book...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Chris Evans back on the market\\n\\nBroadcaster ...      1\n",
       "1  Giggs handed Wales leading role\\n\\nRyan Giggs ...      3\n",
       "2  Wales silent on Grand Slam talk\\n\\nRhys Willia...      3\n",
       "3  Kenya lift Chepkemei's suspension\\n\\nKenya's a...      3\n",
       "4  Lee to create new film superhero\\n\\nComic book...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for reproducibility\n",
    "random_state = 321 \n",
    "\n",
    "DATA_DIR = \"data/bbc\"\n",
    "data = load_files(DATA_DIR, encoding=\"utf-8\", decode_error=\"replace\", random_state=random_state)\n",
    "df = pd.DataFrame(list(zip(data['data'], data['target'])), columns=['text', 'label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. **Print the unique target names in your data and check the number of articles in each category. Then split your data into training (80%) and test (20%) sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels, counts = np.unique(df['label'], return_counts=True) # np.unique(data.target, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business', 'entertainment', 'politics', 'sport', 'tech']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'business': 510, 'entertainment': 386, 'politics': 417, 'sport': 511, 'tech': 401}\n"
     ]
    }
   ],
   "source": [
    "print(dict(zip(data.target_names, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. **Use the CountVectorizer from sklearn and convert the text data into a document-term matrix. What is the difference between CountVectorizer and tfidfVectorizer(use_idf=False)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1780, 23908)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # for bag of words feature extraction\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#tokenizer to remove unwanted elements from out data like symbols\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "# If you have memory issues, reduce the max_features value so you can continue with the practical\n",
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "                             tokenizer=None,\n",
    "                             stop_words='english',\n",
    "                             ngram_range=(1, 2),\n",
    "                             analyzer='word',\n",
    "                             min_df=3,\n",
    "                             max_features=None)\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model and learns the vocabulary; \n",
    "# second, it transforms our data into feature vectors. \n",
    "# The input to fit_transform should be a list of strings.\n",
    "bbc_dtm = vectorizer.fit_transform(X_train)\n",
    "print(bbc_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference is that the TfidfVectorizer() returns floats while the CountVectorizer() returns ints. And that’s to be expected – as explained in the documentation quoted above, TfidfVectorizer() assigns a score while CountVectorizer() counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. **Print top 20 frequent words in the training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['said', 'mr', 'year', 'people', 'new', 'time', 'world',\n",
       "       'government', 'uk', 'years', 'best', 'just', 'told', 'film',\n",
       "       'make', 'game', 'like', 'music', 'labour', '000'], dtype='<U27')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = np.argsort(np.asarray(bbc_dtm.sum(axis=0)).ravel())[::-1]\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "feature_names[importance[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also sort the counts based on a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1770</th>\n",
       "      <th>1771</th>\n",
       "      <th>1772</th>\n",
       "      <th>1773</th>\n",
       "      <th>1774</th>\n",
       "      <th>1775</th>\n",
       "      <th>1776</th>\n",
       "      <th>1777</th>\n",
       "      <th>1778</th>\n",
       "      <th>1779</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>retailers</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>figures</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retail</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>december</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christmas</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ons</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bank england</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1780 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "retailers        0     0     7     0     0     0     0     0     0     0  ...   \n",
       "figures          0     0     7     0     0     0     0     1     0     0  ...   \n",
       "sales            0     0     6     0     0     0     0     0     0     1  ...   \n",
       "retail           0     0     6     0     0     0     0     0     0     0  ...   \n",
       "december         0     0     5     0     0     1     1     0     0     0  ...   \n",
       "christmas        0     0     5     0     0     0     0     0     0     0  ...   \n",
       "ons              0     0     4     0     0     0     0     0     0     0  ...   \n",
       "worst            0     0     4     0     0     0     0     0     0     0  ...   \n",
       "said             6     3     4     2     0     8     2     2     0     4  ...   \n",
       "bank england     0     0     3     0     0     0     0     0     0     0  ...   \n",
       "\n",
       "              1770  1771  1772  1773  1774  1775  1776  1777  1778  1779  \n",
       "retailers        0     0     0     0     0     0     0     0     0     0  \n",
       "figures          0     0     0     0     2     0     0     0     0     0  \n",
       "sales            0     0     0     4     0     0     0     0     0     0  \n",
       "retail           0     0     0     0     0     0     0     0     0     0  \n",
       "december         0     0     0     1     0     0     0     1     0     0  \n",
       "christmas        0     0     0     0     0     0     0     0     0     0  \n",
       "ons              0     0     0     0     0     0     0     0     0     0  \n",
       "worst            0     0     0     0     0     0     0     0     0     0  \n",
       "said             3     6     0     5     5     3     2     4     1     1  \n",
       "bank england     0     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[10 rows x 1780 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = pd.DataFrame(bbc_dtm.toarray(),\n",
    "                      columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Show us the top 10 most common words in document 2\n",
    "counts.T.sort_values(by=2, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. **From the feature selection library in sklearn load the SelectKBest function and apply it on the BBC dataset using the chi-squared method. Extract top 20 features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1780x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4428 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch2 = SelectKBest(chi2, k=20)\n",
    "ch2.fit_transform(bbc_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_chi = [feature_names[i] for i\n",
    "                         in ch2.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best',\n",
       " 'blair',\n",
       " 'brown',\n",
       " 'computer',\n",
       " 'digital',\n",
       " 'election',\n",
       " 'film',\n",
       " 'government',\n",
       " 'labour',\n",
       " 'minister',\n",
       " 'mobile',\n",
       " 'mr',\n",
       " 'mr blair',\n",
       " 'music',\n",
       " 'net',\n",
       " 'party',\n",
       " 'people',\n",
       " 'software',\n",
       " 'technology',\n",
       " 'users']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names_chi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. **Extract the 20 top features according to the mutual information feature selection method. Do you get the same list of words as compared to the chi-squared method?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1780x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6350 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info = SelectKBest(mutual_info_classif, k=20)\n",
    "mutual_info.fit_transform(bbc_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blair',\n",
       " 'coach',\n",
       " 'election',\n",
       " 'film',\n",
       " 'firm',\n",
       " 'game',\n",
       " 'government',\n",
       " 'labour',\n",
       " 'market',\n",
       " 'minister',\n",
       " 'mr',\n",
       " 'music',\n",
       " 'party',\n",
       " 'people',\n",
       " 'said',\n",
       " 'secretary',\n",
       " 'technology',\n",
       " 'tory',\n",
       " 'users',\n",
       " 'win']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names_mutual_info = [feature_names[i] for i\n",
    "                         in mutual_info.get_support(indices=True)]\n",
    "feature_names_mutual_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can build a classifier and train it using the output of these feature selection techniques. We are not going to do this right now, but if you are interested you can transform your training and test set using the selected features and continue with your classifier! Here are some tips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = mutual_info.fit_transform(bbc_dtm, y_train)\n",
    "# X_test = mutual_info.transform(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. **One of the functions for embedded feature selection is the SelectFromModel function in sklearn. Use this function with L1 norm SVM and check how many non-zero coefficients left in the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the matrix before applying the embedded feature selection: (1780, 23908)\n",
      "shape of the matrix before applying the embedded feature selection: (1780, 156)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the matrix before applying the embedded feature selection:\", bbc_dtm.shape)\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False)\n",
    "model = SelectFromModel(lsvc).fit(bbc_dtm, y_train) # you can add threshold=0.18 as another argument to select features that have an importance of more than 0.18\n",
    "X_new = model.transform(bbc_dtm)\n",
    "print(\"shape of the matrix before applying the embedded feature selection:\", X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LinearSVC(C=0.01, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    loss='squared_hinge', max_iter=1000,\n",
       "                                    multi_class='ovr', penalty='l1',\n",
       "                                    random_state=None, tol=0.0001, verbose=0),\n",
       "                max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        , -0.08074181,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also check the coefficient values\n",
    "model.estimator_.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. **What are the top features according to the SVM model? Tip: Use the function model.get_support() to find these features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, ..., False, False, False])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by SelectFromModel:  ['000' '2004' 'airlines' 'album' 'analysts' 'apple' 'arsenal' 'athens'\n",
      " 'athletics' 'award' 'ballet' 'ban' 'band' 'bank' 'bbc' 'best' 'bid'\n",
      " 'blair' 'blog' 'book' 'britain' 'british' 'broadband' 'brown' 'business'\n",
      " 'champion' 'chart' 'chelsea' 'chief' 'children' 'china' 'club' 'coach'\n",
      " 'comedy' 'committee' 'companies' 'company' 'computer' 'content' 'council'\n",
      " 'countries' 'cup' 'data' 'deal' 'deutsche' 'digital' 'dollar' 'doping'\n",
      " 'drugs' 'economic' 'economy' 'education' 'election' 'england' 'european'\n",
      " 'euros' 'film' 'final' 'financial' 'firm' 'firms' 'fraud' 'game' 'games'\n",
      " 'gaming' 'glazer' 'good' 'government' 'great' 'group' 'growth' 'half'\n",
      " 'high' 'home' 'howard' 'hunting' 'iaaf' 'including' 'information'\n",
      " 'injury' 'internet' 'ireland' 'jones' 'just' 'labour' 'like' 'liverpool'\n",
      " 'lord' 'mail' 'make' 'market' 'match' 'microsoft' 'million' 'minister'\n",
      " 'mobile' 'mps' 'mr' 'music' 'musical' 'net' 'new' 'nintendo' 'number'\n",
      " 'oil' 'old' 'olympic' 'online' 'party' 'people' 'plans' 'play' 'players'\n",
      " 'police' 'president' 'prices' 'public' 'rights' 'roddick' 'rugby' 'said'\n",
      " 'sales' 'says' 'season' 'secretary' 'series' 'service' 'services' 'set'\n",
      " 'shares' 'singer' 'site' 'software' 'sony' 'spam' 'star' 'stars' 'state'\n",
      " 'team' 'technology' 'time' 'trade' 'tv' 'uk' 'united' 'use' 'users'\n",
      " 'using' 'video' 'virus' 'web' 'win' 'won' 'world' 'year' 'year old']\n"
     ]
    }
   ],
   "source": [
    "print(\"Features selected by SelectFromModel: \", feature_names[model.get_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. **Create a pipeline with the tfidf representation and a random forest classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('feature_extraction', TfidfTransformer()),\n",
    "    ('classification', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. **Fit the pipeline on the training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vectorizer',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, vocabulary=None)),\n",
       "  ('feature_extraction',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('classification',\n",
       "   RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=None, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False))],\n",
       " 'verbose': False,\n",
       " 'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 'feature_extraction': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'classification': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'vectorizer__analyzer': 'word',\n",
       " 'vectorizer__binary': False,\n",
       " 'vectorizer__decode_error': 'strict',\n",
       " 'vectorizer__dtype': numpy.int64,\n",
       " 'vectorizer__encoding': 'utf-8',\n",
       " 'vectorizer__input': 'content',\n",
       " 'vectorizer__lowercase': True,\n",
       " 'vectorizer__max_df': 1.0,\n",
       " 'vectorizer__max_features': None,\n",
       " 'vectorizer__min_df': 1,\n",
       " 'vectorizer__ngram_range': (1, 1),\n",
       " 'vectorizer__preprocessor': None,\n",
       " 'vectorizer__stop_words': None,\n",
       " 'vectorizer__strip_accents': None,\n",
       " 'vectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vectorizer__tokenizer': None,\n",
       " 'vectorizer__vocabulary': None,\n",
       " 'feature_extraction__norm': 'l2',\n",
       " 'feature_extraction__smooth_idf': True,\n",
       " 'feature_extraction__sublinear_tf': False,\n",
       " 'feature_extraction__use_idf': True,\n",
       " 'classification__bootstrap': True,\n",
       " 'classification__ccp_alpha': 0.0,\n",
       " 'classification__class_weight': None,\n",
       " 'classification__criterion': 'gini',\n",
       " 'classification__max_depth': None,\n",
       " 'classification__max_features': 'auto',\n",
       " 'classification__max_leaf_nodes': None,\n",
       " 'classification__max_samples': None,\n",
       " 'classification__min_impurity_decrease': 0.0,\n",
       " 'classification__min_impurity_split': None,\n",
       " 'classification__min_samples_leaf': 1,\n",
       " 'classification__min_samples_split': 2,\n",
       " 'classification__min_weight_fraction_leaf': 0.0,\n",
       " 'classification__n_estimators': 100,\n",
       " 'classification__n_jobs': None,\n",
       " 'classification__oob_score': False,\n",
       " 'classification__random_state': None,\n",
       " 'classification__verbose': 0,\n",
       " 'classification__warm_start': False}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.fit(X_train, y_train)\n",
    "clf1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11\\. **Use the pipeline to predict the outcome variable on your test set. Evaluate the performance of the pipeline using the classification_report function on the test subset. How do you analyze your results?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.94      0.95      0.94        92\n",
      "entertainment       1.00      0.90      0.95        84\n",
      "     politics       0.90      0.96      0.93        77\n",
      "        sport       0.99      0.99      0.99       111\n",
      "         tech       0.94      0.96      0.95        81\n",
      "\n",
      "     accuracy                           0.96       445\n",
      "    macro avg       0.95      0.95      0.95       445\n",
      " weighted avg       0.96      0.96      0.96       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = clf1.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred1, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12\\. **Create your second pipeline with the tfidf representation and a random forest classifier with the addition of an embedded feature selection using the SVM classification method with L1 penalty. Fit the pipeline on your training set and test it with the test set. How does the performance change?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('feature_extraction', TfidfTransformer()),\n",
    "    ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n",
    "    ('classification', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabula...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vectorizer',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, vocabulary=None)),\n",
       "  ('feature_extraction',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('feature_selection',\n",
       "   SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False,\n",
       "                                       fit_intercept=True, intercept_scaling=1,\n",
       "                                       loss='squared_hinge', max_iter=1000,\n",
       "                                       multi_class='ovr', penalty='l1',\n",
       "                                       random_state=None, tol=0.0001, verbose=0),\n",
       "                   max_features=None, norm_order=1, prefit=False, threshold=None)),\n",
       "  ('classification',\n",
       "   RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=None, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False))],\n",
       " 'verbose': False,\n",
       " 'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 'feature_extraction': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'feature_selection': SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     loss='squared_hinge', max_iter=1000,\n",
       "                                     multi_class='ovr', penalty='l1',\n",
       "                                     random_state=None, tol=0.0001, verbose=0),\n",
       "                 max_features=None, norm_order=1, prefit=False, threshold=None),\n",
       " 'classification': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'vectorizer__analyzer': 'word',\n",
       " 'vectorizer__binary': False,\n",
       " 'vectorizer__decode_error': 'strict',\n",
       " 'vectorizer__dtype': numpy.int64,\n",
       " 'vectorizer__encoding': 'utf-8',\n",
       " 'vectorizer__input': 'content',\n",
       " 'vectorizer__lowercase': True,\n",
       " 'vectorizer__max_df': 1.0,\n",
       " 'vectorizer__max_features': None,\n",
       " 'vectorizer__min_df': 1,\n",
       " 'vectorizer__ngram_range': (1, 1),\n",
       " 'vectorizer__preprocessor': None,\n",
       " 'vectorizer__stop_words': None,\n",
       " 'vectorizer__strip_accents': None,\n",
       " 'vectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vectorizer__tokenizer': None,\n",
       " 'vectorizer__vocabulary': None,\n",
       " 'feature_extraction__norm': 'l2',\n",
       " 'feature_extraction__smooth_idf': True,\n",
       " 'feature_extraction__sublinear_tf': False,\n",
       " 'feature_extraction__use_idf': True,\n",
       " 'feature_selection__estimator__C': 1.0,\n",
       " 'feature_selection__estimator__class_weight': None,\n",
       " 'feature_selection__estimator__dual': False,\n",
       " 'feature_selection__estimator__fit_intercept': True,\n",
       " 'feature_selection__estimator__intercept_scaling': 1,\n",
       " 'feature_selection__estimator__loss': 'squared_hinge',\n",
       " 'feature_selection__estimator__max_iter': 1000,\n",
       " 'feature_selection__estimator__multi_class': 'ovr',\n",
       " 'feature_selection__estimator__penalty': 'l1',\n",
       " 'feature_selection__estimator__random_state': None,\n",
       " 'feature_selection__estimator__tol': 0.0001,\n",
       " 'feature_selection__estimator__verbose': 0,\n",
       " 'feature_selection__estimator': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       " 'feature_selection__max_features': None,\n",
       " 'feature_selection__norm_order': 1,\n",
       " 'feature_selection__prefit': False,\n",
       " 'feature_selection__threshold': None,\n",
       " 'classification__bootstrap': True,\n",
       " 'classification__ccp_alpha': 0.0,\n",
       " 'classification__class_weight': None,\n",
       " 'classification__criterion': 'gini',\n",
       " 'classification__max_depth': None,\n",
       " 'classification__max_features': 'auto',\n",
       " 'classification__max_leaf_nodes': None,\n",
       " 'classification__max_samples': None,\n",
       " 'classification__min_impurity_decrease': 0.0,\n",
       " 'classification__min_impurity_split': None,\n",
       " 'classification__min_samples_leaf': 1,\n",
       " 'classification__min_samples_split': 2,\n",
       " 'classification__min_weight_fraction_leaf': 0.0,\n",
       " 'classification__n_estimators': 100,\n",
       " 'classification__n_jobs': None,\n",
       " 'classification__oob_score': False,\n",
       " 'classification__random_state': None,\n",
       " 'classification__verbose': 0,\n",
       " 'classification__warm_start': False}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.90      0.93      0.91        92\n",
      "entertainment       0.97      0.93      0.95        84\n",
      "     politics       0.93      0.88      0.91        77\n",
      "        sport       0.97      0.98      0.98       111\n",
      "         tech       0.93      0.96      0.95        81\n",
      "\n",
      "     accuracy                           0.94       445\n",
      "    macro avg       0.94      0.94      0.94       445\n",
      " weighted avg       0.94      0.94      0.94       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred2, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13\\. **Create your third and forth pipelines with the tfidf representation, a chi2 feature selection (with 20 and 200 features for clf3 and clf4, respectively), and a random forest classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('feature_extraction', TfidfTransformer()),\n",
    "    ('feature_selection', SelectKBest(chi2, k=20)),\n",
    "    ('classification', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vectorizer',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, vocabulary=None)),\n",
       "  ('feature_extraction',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('feature_selection',\n",
       "   SelectKBest(k=20, score_func=<function chi2 at 0x0000017C21589798>)),\n",
       "  ('classification',\n",
       "   RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=None, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False))],\n",
       " 'verbose': False,\n",
       " 'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 'feature_extraction': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'feature_selection': SelectKBest(k=20, score_func=<function chi2 at 0x0000017C21589798>),\n",
       " 'classification': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'vectorizer__analyzer': 'word',\n",
       " 'vectorizer__binary': False,\n",
       " 'vectorizer__decode_error': 'strict',\n",
       " 'vectorizer__dtype': numpy.int64,\n",
       " 'vectorizer__encoding': 'utf-8',\n",
       " 'vectorizer__input': 'content',\n",
       " 'vectorizer__lowercase': True,\n",
       " 'vectorizer__max_df': 1.0,\n",
       " 'vectorizer__max_features': None,\n",
       " 'vectorizer__min_df': 1,\n",
       " 'vectorizer__ngram_range': (1, 1),\n",
       " 'vectorizer__preprocessor': None,\n",
       " 'vectorizer__stop_words': None,\n",
       " 'vectorizer__strip_accents': None,\n",
       " 'vectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vectorizer__tokenizer': None,\n",
       " 'vectorizer__vocabulary': None,\n",
       " 'feature_extraction__norm': 'l2',\n",
       " 'feature_extraction__smooth_idf': True,\n",
       " 'feature_extraction__sublinear_tf': False,\n",
       " 'feature_extraction__use_idf': True,\n",
       " 'feature_selection__k': 20,\n",
       " 'feature_selection__score_func': <function sklearn.feature_selection._univariate_selection.chi2(X, y)>,\n",
       " 'classification__bootstrap': True,\n",
       " 'classification__ccp_alpha': 0.0,\n",
       " 'classification__class_weight': None,\n",
       " 'classification__criterion': 'gini',\n",
       " 'classification__max_depth': None,\n",
       " 'classification__max_features': 'auto',\n",
       " 'classification__max_leaf_nodes': None,\n",
       " 'classification__max_samples': None,\n",
       " 'classification__min_impurity_decrease': 0.0,\n",
       " 'classification__min_impurity_split': None,\n",
       " 'classification__min_samples_leaf': 1,\n",
       " 'classification__min_samples_split': 2,\n",
       " 'classification__min_weight_fraction_leaf': 0.0,\n",
       " 'classification__n_estimators': 100,\n",
       " 'classification__n_jobs': None,\n",
       " 'classification__oob_score': False,\n",
       " 'classification__random_state': None,\n",
       " 'classification__verbose': 0,\n",
       " 'classification__warm_start': False}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.fit(X_train, y_train)\n",
    "clf3.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.64      0.47      0.54        92\n",
      "entertainment       0.80      0.57      0.67        84\n",
      "     politics       0.78      0.73      0.75        77\n",
      "        sport       0.63      0.98      0.76       111\n",
      "         tech       0.90      0.80      0.85        81\n",
      "\n",
      "     accuracy                           0.72       445\n",
      "    macro avg       0.75      0.71      0.71       445\n",
      " weighted avg       0.74      0.72      0.71       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred3 = clf3.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred3, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('feature_extraction', TfidfTransformer()),\n",
    "    ('feature_selection', SelectKBest(chi2, k=200)),\n",
    "    ('classification', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vectorizer',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, vocabulary=None)),\n",
       "  ('feature_extraction',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('feature_selection',\n",
       "   SelectKBest(k=200, score_func=<function chi2 at 0x0000017C21589798>)),\n",
       "  ('classification',\n",
       "   RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=None, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False))],\n",
       " 'verbose': False,\n",
       " 'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 'feature_extraction': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'feature_selection': SelectKBest(k=200, score_func=<function chi2 at 0x0000017C21589798>),\n",
       " 'classification': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'vectorizer__analyzer': 'word',\n",
       " 'vectorizer__binary': False,\n",
       " 'vectorizer__decode_error': 'strict',\n",
       " 'vectorizer__dtype': numpy.int64,\n",
       " 'vectorizer__encoding': 'utf-8',\n",
       " 'vectorizer__input': 'content',\n",
       " 'vectorizer__lowercase': True,\n",
       " 'vectorizer__max_df': 1.0,\n",
       " 'vectorizer__max_features': None,\n",
       " 'vectorizer__min_df': 1,\n",
       " 'vectorizer__ngram_range': (1, 1),\n",
       " 'vectorizer__preprocessor': None,\n",
       " 'vectorizer__stop_words': None,\n",
       " 'vectorizer__strip_accents': None,\n",
       " 'vectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vectorizer__tokenizer': None,\n",
       " 'vectorizer__vocabulary': None,\n",
       " 'feature_extraction__norm': 'l2',\n",
       " 'feature_extraction__smooth_idf': True,\n",
       " 'feature_extraction__sublinear_tf': False,\n",
       " 'feature_extraction__use_idf': True,\n",
       " 'feature_selection__k': 200,\n",
       " 'feature_selection__score_func': <function sklearn.feature_selection._univariate_selection.chi2(X, y)>,\n",
       " 'classification__bootstrap': True,\n",
       " 'classification__ccp_alpha': 0.0,\n",
       " 'classification__class_weight': None,\n",
       " 'classification__criterion': 'gini',\n",
       " 'classification__max_depth': None,\n",
       " 'classification__max_features': 'auto',\n",
       " 'classification__max_leaf_nodes': None,\n",
       " 'classification__max_samples': None,\n",
       " 'classification__min_impurity_decrease': 0.0,\n",
       " 'classification__min_impurity_split': None,\n",
       " 'classification__min_samples_leaf': 1,\n",
       " 'classification__min_samples_split': 2,\n",
       " 'classification__min_weight_fraction_leaf': 0.0,\n",
       " 'classification__n_estimators': 100,\n",
       " 'classification__n_jobs': None,\n",
       " 'classification__oob_score': False,\n",
       " 'classification__random_state': None,\n",
       " 'classification__verbose': 0,\n",
       " 'classification__warm_start': False}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4.fit(X_train, y_train)\n",
    "clf4.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.87      0.91      0.89        92\n",
      "entertainment       0.97      0.89      0.93        84\n",
      "     politics       0.93      0.88      0.91        77\n",
      "        sport       0.97      0.99      0.98       111\n",
      "         tech       0.93      0.98      0.95        81\n",
      "\n",
      "     accuracy                           0.93       445\n",
      "    macro avg       0.93      0.93      0.93       445\n",
      " weighted avg       0.94      0.93      0.93       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred4 = clf4.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred4, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14\\. **We can change the learner by simply plugging a different classifier object into our pipeline. Create your fifth pipeline with L1 norm SVM for the feature selection method and naive Bayes for the classifier. Compare your results on the test set with the previous pipelines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('feature_extraction', TfidfTransformer()),\n",
    "    ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n",
    "    ('classification', MultinomialNB(alpha=0.01))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vectorizer',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, vocabulary=None)),\n",
       "  ('feature_extraction',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('feature_selection',\n",
       "   SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False,\n",
       "                                       fit_intercept=True, intercept_scaling=1,\n",
       "                                       loss='squared_hinge', max_iter=1000,\n",
       "                                       multi_class='ovr', penalty='l1',\n",
       "                                       random_state=None, tol=0.0001, verbose=0),\n",
       "                   max_features=None, norm_order=1, prefit=False, threshold=None)),\n",
       "  ('classification',\n",
       "   MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))],\n",
       " 'verbose': False,\n",
       " 'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 'feature_extraction': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'feature_selection': SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     loss='squared_hinge', max_iter=1000,\n",
       "                                     multi_class='ovr', penalty='l1',\n",
       "                                     random_state=None, tol=0.0001, verbose=0),\n",
       "                 max_features=None, norm_order=1, prefit=False, threshold=None),\n",
       " 'classification': MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True),\n",
       " 'vectorizer__analyzer': 'word',\n",
       " 'vectorizer__binary': False,\n",
       " 'vectorizer__decode_error': 'strict',\n",
       " 'vectorizer__dtype': numpy.int64,\n",
       " 'vectorizer__encoding': 'utf-8',\n",
       " 'vectorizer__input': 'content',\n",
       " 'vectorizer__lowercase': True,\n",
       " 'vectorizer__max_df': 1.0,\n",
       " 'vectorizer__max_features': None,\n",
       " 'vectorizer__min_df': 1,\n",
       " 'vectorizer__ngram_range': (1, 1),\n",
       " 'vectorizer__preprocessor': None,\n",
       " 'vectorizer__stop_words': None,\n",
       " 'vectorizer__strip_accents': None,\n",
       " 'vectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vectorizer__tokenizer': None,\n",
       " 'vectorizer__vocabulary': None,\n",
       " 'feature_extraction__norm': 'l2',\n",
       " 'feature_extraction__smooth_idf': True,\n",
       " 'feature_extraction__sublinear_tf': False,\n",
       " 'feature_extraction__use_idf': True,\n",
       " 'feature_selection__estimator__C': 1.0,\n",
       " 'feature_selection__estimator__class_weight': None,\n",
       " 'feature_selection__estimator__dual': False,\n",
       " 'feature_selection__estimator__fit_intercept': True,\n",
       " 'feature_selection__estimator__intercept_scaling': 1,\n",
       " 'feature_selection__estimator__loss': 'squared_hinge',\n",
       " 'feature_selection__estimator__max_iter': 1000,\n",
       " 'feature_selection__estimator__multi_class': 'ovr',\n",
       " 'feature_selection__estimator__penalty': 'l1',\n",
       " 'feature_selection__estimator__random_state': None,\n",
       " 'feature_selection__estimator__tol': 0.0001,\n",
       " 'feature_selection__estimator__verbose': 0,\n",
       " 'feature_selection__estimator': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       " 'feature_selection__max_features': None,\n",
       " 'feature_selection__norm_order': 1,\n",
       " 'feature_selection__prefit': False,\n",
       " 'feature_selection__threshold': None,\n",
       " 'classification__alpha': 0.01,\n",
       " 'classification__class_prior': None,\n",
       " 'classification__fit_prior': True}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf5.fit(X_train, y_train)\n",
    "clf5.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.96      0.93      0.95        92\n",
      "entertainment       1.00      0.94      0.97        84\n",
      "     politics       0.95      0.99      0.97        77\n",
      "        sport       1.00      1.00      1.00       111\n",
      "         tech       0.93      0.98      0.95        81\n",
      "\n",
      "     accuracy                           0.97       445\n",
      "    macro avg       0.97      0.97      0.97       445\n",
      " weighted avg       0.97      0.97      0.97       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred5 = clf5.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred5, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15\\. **Dimensionality reduction methods such as PCA and SVD can be used to project the data into a lower dimensional space. If you run PCA with your text data, you might end up with the message \"PCA does not support sparse input. See TruncatedSVD for a possible alternative.\" Therefore, we will use the Truncated SVD function from the sklearn package and we want to find out how much of the variance in the BBC data set is explained with different components. For this, first create a tfidf matrix and use that to make a co-occurrence matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TFIDF vectorizer: (1780, 26739)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "X = tfidf_vect.fit_transform(X_train)\n",
    "Xc = (X.T * X) # this is co-occurrence matrix in sparse csr format\n",
    "Xc.setdiag(0) # sometimes you want to fill same word cooccurence to 0\n",
    "print(\"Shape of the TFIDF vectorizer:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.00024418 0.         ... 0.         0.         0.        ]\n",
      " [0.00024418 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(Xc.todense()) # print out matrix in dense format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'moya': 16209,\n",
       " 'emotional': 8651,\n",
       " 'at': 2701,\n",
       " 'davis': 6926,\n",
       " 'cup': 6713,\n",
       " 'win': 26258,\n",
       " 'carlos': 4732,\n",
       " 'described': 7350,\n",
       " 'spain': 22477,\n",
       " 'victory': 25605,\n",
       " 'as': 2586,\n",
       " 'the': 24062,\n",
       " 'highlight': 11819,\n",
       " 'of': 17001,\n",
       " 'his': 11872,\n",
       " 'career': 4712,\n",
       " 'after': 1753,\n",
       " 'he': 11591,\n",
       " 'beat': 3292,\n",
       " 'andy': 2196,\n",
       " 'roddick': 20643,\n",
       " 'to': 24311,\n",
       " 'end': 8715,\n",
       " 'usa': 25352,\n",
       " 'challenge': 5012,\n",
       " 'in': 12538,\n",
       " 'seville': 21583,\n",
       " 'made': 14933,\n",
       " 'up': 25285,\n",
       " 'for': 10089,\n",
       " 'missing': 15914,\n",
       " '2000': 428,\n",
       " 'through': 24179,\n",
       " 'injury': 12794,\n",
       " 'by': 4484,\n",
       " 'beating': 3295,\n",
       " 'give': 10798,\n",
       " 'hosts': 12081,\n",
       " 'an': 2147,\n",
       " 'unassailable': 24929,\n",
       " 'lead': 14223,\n",
       " 'have': 11560,\n",
       " 'woken': 26365,\n",
       " 'so': 22302,\n",
       " 'many': 15140,\n",
       " 'nights': 16676,\n",
       " 'dreaming': 8137,\n",
       " 'this': 24124,\n",
       " 'day': 6934,\n",
       " 'said': 20934,\n",
       " 'all': 1955,\n",
       " 'my': 16350,\n",
       " 'energy': 8752,\n",
       " 'has': 11525,\n",
       " 'been': 3342,\n",
       " 'focused': 10027,\n",
       " 'on': 17097,\n",
       " 'today': 24317,\n",
       " 'what': 26113,\n",
       " 'lived': 14610,\n",
       " 'do': 7916,\n",
       " 'not': 16800,\n",
       " 'think': 24111,\n",
       " 'will': 26230,\n",
       " 'live': 14608,\n",
       " 'again': 1760,\n",
       " 'only': 17111,\n",
       " 'other': 17277,\n",
       " 'title': 24300,\n",
       " 'came': 4575,\n",
       " 'two': 24844,\n",
       " 'years': 26599,\n",
       " 'ago': 1797,\n",
       " 'valencia': 25425,\n",
       " 'when': 26127,\n",
       " 'they': 24099,\n",
       " 'australia': 2816,\n",
       " 'and': 2180,\n",
       " 'nicknamed': 16649,\n",
       " 'charly': 5091,\n",
       " 'admitted': 1643,\n",
       " 'is': 13176,\n",
       " 'dream': 8135,\n",
       " 'was': 25919,\n",
       " 'bit': 3648,\n",
       " 'nervous': 16561,\n",
       " 'outset': 17350,\n",
       " 'some': 22373,\n",
       " 'people': 17913,\n",
       " 'that': 24054,\n",
       " 'am': 2046,\n",
       " 'obsessed': 16950,\n",
       " 'but': 4452,\n",
       " 'it': 13220,\n",
       " 'better': 3533,\n",
       " 'way': 25965,\n",
       " 'helps': 11711,\n",
       " 'me': 15467,\n",
       " 'reach': 19543,\n",
       " 'goals': 10896,\n",
       " 'if': 12350,\n",
       " 'really': 19578,\n",
       " 'incredible': 12608,\n",
       " 'get': 10709,\n",
       " 'winning': 26291,\n",
       " 'point': 18391,\n",
       " 'something': 22380,\n",
       " 'spanish': 22488,\n",
       " 'captain': 4680,\n",
       " 'jordi': 13487,\n",
       " 'arrese': 2538,\n",
       " 'played': 18300,\n",
       " 'great': 11085,\n",
       " 'game': 10498,\n",
       " 'opportunity': 17167,\n",
       " 'hasn': 11529,\n",
       " 'let': 14373,\n",
       " 'us': 25351,\n",
       " 'down': 8069,\n",
       " 'had': 11313,\n",
       " 'lost': 14750,\n",
       " 'three': 24158,\n",
       " 'times': 24259,\n",
       " 'him': 11839,\n",
       " 'waiting': 25820,\n",
       " 'be': 3266,\n",
       " 'position': 18525,\n",
       " 'also': 2021,\n",
       " 'remarkable': 20007,\n",
       " 'performance': 17946,\n",
       " 'rafael': 19368,\n",
       " 'nadal': 16373,\n",
       " 'who': 26173,\n",
       " 'opening': 17137,\n",
       " 'singles': 22020,\n",
       " 'aged': 1765,\n",
       " '18': 273,\n",
       " '185': 290,\n",
       " 'days': 6938,\n",
       " 'mallorcan': 15052,\n",
       " 'became': 3314,\n",
       " 'youngest': 26642,\n",
       " 'player': 18301,\n",
       " 'finish': 9808,\n",
       " 'year': 26595,\n",
       " 'afterwards': 1757,\n",
       " 'coach': 5541,\n",
       " 'patrick': 17763,\n",
       " 'mcenroe': 15416,\n",
       " 'wants': 25871,\n",
       " 'rest': 20254,\n",
       " 'team': 23835,\n",
       " 'play': 18294,\n",
       " 'more': 16105,\n",
       " 'tennis': 23954,\n",
       " 'clay': 5410,\n",
       " 'hone': 11992,\n",
       " 'their': 24071,\n",
       " 'skills': 22086,\n",
       " 'surface': 23428,\n",
       " 'help': 11705,\n",
       " 'these': 24098,\n",
       " 'guys': 11285,\n",
       " 'even': 9083,\n",
       " 'slow': 22183,\n",
       " 'hard': 11459,\n",
       " 'courts': 6453,\n",
       " 'learn': 14258,\n",
       " 'how': 12124,\n",
       " 'mix': 15944,\n",
       " 'things': 24110,\n",
       " 'little': 14603,\n",
       " 'smarter': 22214,\n",
       " 'tactically': 23661,\n",
       " 'obviously': 16963,\n",
       " 'unrealistic': 25202,\n",
       " 'say': 21091,\n",
       " 'we': 25974,\n",
       " 're': 19541,\n",
       " 'going': 10915,\n",
       " 'just': 13586,\n",
       " 'start': 22840,\n",
       " 'playing': 18304,\n",
       " 'constantly': 6090,\n",
       " 'with': 26325,\n",
       " 'schedule': 21155,\n",
       " 'certainly': 4972,\n",
       " 'can': 4602,\n",
       " 'put': 19204,\n",
       " 'work': 26420,\n",
       " 'appropriate': 2415,\n",
       " 'time': 24253,\n",
       " 'couple': 6433,\n",
       " 'events': 9088,\n",
       " 'against': 1761,\n",
       " 'are': 2471,\n",
       " 'best': 3513,\n",
       " 'stuff': 23180,\n",
       " 'left': 14290,\n",
       " 'frustrated': 10358,\n",
       " 'losing': 14746,\n",
       " 'both': 3970,\n",
       " 'olympic': 17085,\n",
       " 'stadium': 22755,\n",
       " 'tough': 24434,\n",
       " 'because': 3315,\n",
       " 'felt': 9634,\n",
       " 'like': 14494,\n",
       " 'whole': 26176,\n",
       " 'one': 17101,\n",
       " 'top': 24372,\n",
       " 'courters': 6446,\n",
       " 'world': 26443,\n",
       " 'american': 2093,\n",
       " 'chances': 5034,\n",
       " 'didn': 7525,\n",
       " 'convert': 6237,\n",
       " 'them': 24074,\n",
       " 'bottom': 3980,\n",
       " 'line': 14538,\n",
       " 'were': 26082,\n",
       " 'than': 24045,\n",
       " 'weekend': 26030,\n",
       " 'out': 17291,\n",
       " 'took': 24362,\n",
       " 'care': 4710,\n",
       " 'business': 4443,\n",
       " 'simple': 21979,\n",
       " 'bryan': 4294,\n",
       " 'twins': 24837,\n",
       " 'keep': 13697,\n",
       " 'hopes': 12031,\n",
       " 'alive': 1954,\n",
       " 'united': 25136,\n",
       " 'states': 22857,\n",
       " 'kept': 13737,\n",
       " 'final': 9767,\n",
       " 'saturday': 21060,\n",
       " 'doubles': 8052,\n",
       " 'rubber': 20789,\n",
       " 'leaving': 14271,\n",
       " 'ahead': 1816,\n",
       " 'into': 13020,\n",
       " 'masters': 15295,\n",
       " 'champions': 5026,\n",
       " 'mike': 15763,\n",
       " 'bob': 3819,\n",
       " 'thrashed': 24148,\n",
       " 'juan': 13520,\n",
       " 'ferrero': 9661,\n",
       " 'tommy': 24345,\n",
       " 'robredo': 20617,\n",
       " 'front': 10335,\n",
       " 'partisan': 17694,\n",
       " 'crowd': 6627,\n",
       " 'would': 26467,\n",
       " 'given': 10801,\n",
       " 'outclassed': 17297,\n",
       " 'sunday': 23345,\n",
       " 'reverse': 20377,\n",
       " 'takes': 23690,\n",
       " 'before': 3349,\n",
       " 'faces': 9388,\n",
       " 'mardy': 15169,\n",
       " 'fish': 9863,\n",
       " 'feels': 9613,\n",
       " 'good': 10942,\n",
       " 'don': 7990,\n",
       " 'tomorrow': 24346,\n",
       " 'those': 24139,\n",
       " 'another': 2284,\n",
       " 'shot': 21827,\n",
       " 'go': 10889,\n",
       " 'sleep': 22141,\n",
       " 'added': 1586,\n",
       " 'confident': 5959,\n",
       " 'first': 9854,\n",
       " 'match': 15301,\n",
       " 'then': 24079,\n",
       " 'anything': 2330,\n",
       " 'happen': 11443,\n",
       " 'chose': 5247,\n",
       " 'old': 17063,\n",
       " 'epic': 8890,\n",
       " 'over': 17372,\n",
       " 'friday': 10310,\n",
       " 'replaced': 20115,\n",
       " 'former': 10147,\n",
       " 'number': 16872,\n",
       " 'pair': 17541,\n",
       " 'depth': 7320,\n",
       " 'teams': 23838,\n",
       " '26': 544,\n",
       " 'won': 26382,\n",
       " 'four': 10196,\n",
       " 'matches': 15304,\n",
       " 'quickly': 19286,\n",
       " 'silenced': 21952,\n",
       " 'huge': 12154,\n",
       " 'racing': 19335,\n",
       " 'set': 21552,\n",
       " 'love': 14767,\n",
       " 'spaniards': 22487,\n",
       " 'twice': 24831,\n",
       " 'surrendered': 23457,\n",
       " 'breaks': 4110,\n",
       " 'serve': 21540,\n",
       " 'second': 21363,\n",
       " 'bryans': 4295,\n",
       " 'broke': 4230,\n",
       " 'served': 21541,\n",
       " 'dropped': 8178,\n",
       " 'third': 24117,\n",
       " 'unflappable': 25085,\n",
       " 'brothers': 4256,\n",
       " 'powered': 18590,\n",
       " 'impressive': 12516,\n",
       " 'upset': 25322,\n",
       " 'hinted': 11857,\n",
       " 'further': 10431,\n",
       " 'dissatisfaction': 7813,\n",
       " 'defeat': 7086,\n",
       " 'difficult': 7548,\n",
       " 'players': 18302,\n",
       " 'everything': 9103,\n",
       " 'calculated': 4534,\n",
       " 'very': 25557,\n",
       " 'surprised': 23450,\n",
       " 'named': 16392,\n",
       " 'hardly': 11472,\n",
       " 'badly': 2989,\n",
       " 'right': 20505,\n",
       " 'christmas': 5259,\n",
       " 'sales': 20955,\n",
       " 'worst': 26460,\n",
       " 'since': 22004,\n",
       " '1981': 395,\n",
       " 'uk': 24885,\n",
       " 'retail': 20299,\n",
       " 'fell': 9626,\n",
       " 'december': 7011,\n",
       " 'failing': 9417,\n",
       " 'meet': 15522,\n",
       " 'expectations': 9245,\n",
       " 'making': 15020,\n",
       " 'counts': 6429,\n",
       " 'month': 16078,\n",
       " 'rise': 20551,\n",
       " 'november': 16846,\n",
       " 'office': 17021,\n",
       " 'national': 16450,\n",
       " 'statistics': 22866,\n",
       " 'ons': 17113,\n",
       " 'revised': 20391,\n",
       " 'annual': 2270,\n",
       " '2004': 434,\n",
       " 'rate': 19496,\n",
       " 'growth': 11187,\n",
       " 'from': 10333,\n",
       " 'estimated': 9008,\n",
       " 'retailers': 20301,\n",
       " 'already': 2019,\n",
       " 'reported': 20129,\n",
       " 'poor': 18464,\n",
       " 'figures': 9739,\n",
       " 'clothing': 5508,\n",
       " 'non': 16751,\n",
       " 'specialist': 22528,\n",
       " 'stores': 23031,\n",
       " 'hit': 11883,\n",
       " 'internet': 12982,\n",
       " 'showing': 21851,\n",
       " 'any': 2325,\n",
       " 'significant': 21943,\n",
       " 'according': 1473,\n",
       " 'last': 14144,\n",
       " 'endured': 8741,\n",
       " 'tougher': 24436,\n",
       " '23': 502,\n",
       " 'previously': 18780,\n",
       " 'plunged': 18359,\n",
       " 'echoed': 8379,\n",
       " 'earlier': 8318,\n",
       " 'caution': 4873,\n",
       " 'bank': 3095,\n",
       " 'england': 8778,\n",
       " 'governor': 10998,\n",
       " 'mervyn': 15649,\n",
       " 'king': 13820,\n",
       " 'read': 19553,\n",
       " 'too': 24361,\n",
       " 'much': 16240,\n",
       " 'analysts': 2160,\n",
       " 'positive': 18530,\n",
       " 'gloss': 10864,\n",
       " 'pointing': 18395,\n",
       " 'seasonally': 21345,\n",
       " 'adjusted': 1620,\n",
       " 'showed': 21847,\n",
       " 'comparable': 5784,\n",
       " '2003': 433,\n",
       " 'jump': 13560,\n",
       " 'roughly': 20738,\n",
       " 'recent': 19643,\n",
       " 'averages': 2874,\n",
       " 'although': 2032,\n",
       " 'below': 3423,\n",
       " 'serious': 21530,\n",
       " 'booms': 3916,\n",
       " 'seen': 21408,\n",
       " '1990s': 406,\n",
       " 'volume': 25755,\n",
       " 'outperformed': 17338,\n",
       " 'measures': 15486,\n",
       " 'actual': 1562,\n",
       " 'spending': 22577,\n",
       " 'indication': 12640,\n",
       " 'consumers': 6120,\n",
       " 'looking': 14718,\n",
       " 'bargains': 3139,\n",
       " 'cutting': 6771,\n",
       " 'prices': 18784,\n",
       " 'however': 12131,\n",
       " 'reports': 20134,\n",
       " 'high': 11807,\n",
       " 'street': 23094,\n",
       " 'weakness': 25981,\n",
       " 'sector': 21380,\n",
       " 'morrisons': 16126,\n",
       " 'woolworths': 26412,\n",
       " 'house': 12107,\n",
       " 'fraser': 10249,\n",
       " 'marks': 15209,\n",
       " 'spencer': 22575,\n",
       " 'big': 3578,\n",
       " 'food': 10067,\n",
       " 'festive': 9676,\n",
       " 'period': 17957,\n",
       " 'disappointing': 7647,\n",
       " 'british': 4195,\n",
       " 'consortium': 6081,\n",
       " 'survey': 23468,\n",
       " 'found': 10188,\n",
       " '10': 55,\n",
       " 'yet': 26614,\n",
       " 'including': 12582,\n",
       " 'hmv': 11903,\n",
       " 'monsoon': 16067,\n",
       " 'jessops': 13387,\n",
       " 'body': 3832,\n",
       " 'shop': 21803,\n",
       " 'tesco': 23995,\n",
       " 'well': 26064,\n",
       " 'investec': 13068,\n",
       " 'chief': 5182,\n",
       " 'economist': 8393,\n",
       " 'philip': 18088,\n",
       " 'shaw': 21688,\n",
       " 'did': 7521,\n",
       " 'expect': 9242,\n",
       " 'immediate': 12433,\n",
       " 'effect': 8447,\n",
       " 'interest': 12958,\n",
       " 'rates': 19498,\n",
       " 'weak': 25975,\n",
       " 'indicated': 12637,\n",
       " 'night': 16671,\n",
       " 'you': 26638,\n",
       " 'accurate': 1498,\n",
       " 'impression': 12513,\n",
       " 'trading': 24502,\n",
       " 'until': 25255,\n",
       " 'about': 1381,\n",
       " 'easter': 8349,\n",
       " 'mr': 16225,\n",
       " 'our': 17285,\n",
       " 'view': 25618,\n",
       " 'its': 13233,\n",
       " 'powder': 18585,\n",
       " 'dry': 8200,\n",
       " 'wait': 25816,\n",
       " 'see': 21393,\n",
       " 'picture': 18146,\n",
       " 'jones': 13479,\n",
       " 'medals': 15499,\n",
       " 'must': 16332,\n",
       " 'guilty': 11239,\n",
       " 'anti': 2302,\n",
       " 'doping': 8028,\n",
       " 'agency': 1771,\n",
       " 'wada': 25800,\n",
       " 'dick': 7509,\n",
       " 'pound': 18576,\n",
       " 'says': 21095,\n",
       " 'marion': 15191,\n",
       " 'should': 21829,\n",
       " 'stripped': 23135,\n",
       " 'her': 11735,\n",
       " 'taking': 23691,\n",
       " 'banned': 3108,\n",
       " 'substances': 23243,\n",
       " 'victor': 25599,\n",
       " 'conte': 6136,\n",
       " 'balco': 3030,\n",
       " 'laboratories': 14012,\n",
       " 'claims': 5350,\n",
       " 'sprinter': 22686,\n",
       " 'regularly': 19872,\n",
       " 'used': 25362,\n",
       " 'drugs': 8189,\n",
       " 'enhance': 8782,\n",
       " 'she': 21692,\n",
       " 'asked': 2618,\n",
       " 'there': 24089,\n",
       " 'timescale': 24260,\n",
       " 'could': 6399,\n",
       " 'taken': 23686,\n",
       " 'issue': 13212,\n",
       " 'under': 24987,\n",
       " 'international': 12977,\n",
       " 'committee': 5754,\n",
       " 'ioc': 13106,\n",
       " 'rules': 20826,\n",
       " 'athletes': 2706,\n",
       " 'caught': 4865,\n",
       " 'within': 26336,\n",
       " 'event': 9087,\n",
       " 'five': 9879,\n",
       " 'olympics': 17086,\n",
       " 'denies': 7251,\n",
       " 'using': 25373,\n",
       " 'take': 23684,\n",
       " 'legal': 14295,\n",
       " 'action': 1546,\n",
       " 'allegations': 1965,\n",
       " 'firm': 9849,\n",
       " 'centre': 4952,\n",
       " 'wide': 26189,\n",
       " 'reaching': 19546,\n",
       " 'investigation': 13074,\n",
       " 'continued': 6171,\n",
       " 'indeed': 12619,\n",
       " 'disappointment': 7649,\n",
       " 'lot': 14752,\n",
       " 'eminem': 8639,\n",
       " 'secret': 21369,\n",
       " 'gig': 10750,\n",
       " 'venue': 25525,\n",
       " 'revealed': 20359,\n",
       " 'rapper': 19473,\n",
       " 'intimate': 13013,\n",
       " 'london': 14696,\n",
       " 'following': 10054,\n",
       " 'show': 21839,\n",
       " 'river': 20570,\n",
       " 'thames': 24044,\n",
       " 'star': 22822,\n",
       " 'songs': 22388,\n",
       " 'showcasing': 21845,\n",
       " 'label': 14007,\n",
       " 'shady': 21618,\n",
       " 'records': 19697,\n",
       " 'islington': 13197,\n",
       " 'academy': 1422,\n",
       " 'performed': 17948,\n",
       " 'hms': 11902,\n",
       " 'belfast': 3385,\n",
       " 'which': 26135,\n",
       " 'docked': 7922,\n",
       " 'where': 26129,\n",
       " 'filmed': 9753,\n",
       " 'bbc': 3259,\n",
       " 'pops': 18473,\n",
       " 'arrived': 2547,\n",
       " 'appearance': 2366,\n",
       " 'mtv': 16237,\n",
       " 'europe': 9046,\n",
       " 'music': 16318,\n",
       " 'awards': 2900,\n",
       " 'rome': 20675,\n",
       " 'rap': 19467,\n",
       " 'acts': 1561,\n",
       " 'may': 15365,\n",
       " 'appear': 2365,\n",
       " 'include': 12579,\n",
       " 'stat': 22850,\n",
       " 'quo': 19309,\n",
       " 'proof': 18980,\n",
       " 'dj': 7905,\n",
       " 'green': 11097,\n",
       " 'lantern': 14115,\n",
       " 'swift': 23568,\n",
       " 'obie': 16918,\n",
       " 'trice': 24646,\n",
       " 'latest': 14152,\n",
       " 'album': 1901,\n",
       " 'soared': 22307,\n",
       " 'chart': 5095,\n",
       " 'sale': 20954,\n",
       " 'record': 19690,\n",
       " 'shops': 21808,\n",
       " 'encore': 8701,\n",
       " 'now': 16851,\n",
       " 'topper': 24377,\n",
       " 'sides': 21910,\n",
       " 'atlantic': 2717,\n",
       " 'debut': 6993,\n",
       " 'fourth': 10200,\n",
       " 'outsold': 17356,\n",
       " 'rivals': 20569,\n",
       " 'released': 19958,\n",
       " 'early': 8321,\n",
       " 'effort': 8457,\n",
       " 'combat': 5681,\n",
       " 'physical': 18119,\n",
       " 'online': 17110,\n",
       " 'piracy': 18210,\n",
       " 'includes': 12581,\n",
       " 'track': 24483,\n",
       " 'mosh': 16135,\n",
       " 'tirade': 24286,\n",
       " 'president': 18734,\n",
       " 'bush': 4439,\n",
       " 'presence': 18716,\n",
       " 'troops': 24701,\n",
       " 'iraq': 13130,\n",
       " 'criticised': 6591,\n",
       " 'april': 2426,\n",
       " 'led': 14282,\n",
       " '12': 119,\n",
       " 'viewers': 25621,\n",
       " 'complain': 5825,\n",
       " 'lewd': 14395,\n",
       " 'offensive': 17014,\n",
       " 'complaints': 5830,\n",
       " 'grabbing': 11009,\n",
       " 'crotch': 6623,\n",
       " 'upheld': 25301,\n",
       " 'performer': 17949,\n",
       " 'tone': 24348,\n",
       " 'act': 1543,\n",
       " 'rehearsal': 19885,\n",
       " 'ignored': 12368,\n",
       " 'request': 20160,\n",
       " 'during': 8269,\n",
       " 'broadcast': 4212,\n",
       " 'statement': 22854,\n",
       " 'gestures': 10708,\n",
       " 'part': 17678,\n",
       " 'culture': 6699,\n",
       " 'gone': 10933,\n",
       " 'beyond': 3550,\n",
       " 'expected': 9246,\n",
       " 'loses': 14745,\n",
       " 'customer': 6757,\n",
       " 'details': 7414,\n",
       " 'america': 2092,\n",
       " 'computer': 5878,\n",
       " 'tapes': 23751,\n",
       " 'containing': 6134,\n",
       " 'account': 1476,\n",
       " 'million': 15790,\n",
       " 'customers': 6758,\n",
       " 'federal': 9596,\n",
       " 'employees': 8670,\n",
       " 'several': 21577,\n",
       " 'members': 15569,\n",
       " 'senate': 21457,\n",
       " 'among': 2118,\n",
       " 'affected': 1725,\n",
       " 'vulnerable': 25791,\n",
       " 'identity': 12324,\n",
       " 'theft': 24070,\n",
       " 'sources': 22442,\n",
       " 'stolen': 23009,\n",
       " 'plane': 18261,\n",
       " 'baggage': 2999,\n",
       " 'handlers': 11410,\n",
       " 'gave': 10581,\n",
       " 'no': 16718,\n",
       " 'disappeared': 7643,\n",
       " 'probably': 18851,\n",
       " 'misused': 15933,\n",
       " 'accounts': 1484,\n",
       " 'being': 3375,\n",
       " 'monitoring': 16055,\n",
       " 'holders': 11942,\n",
       " 'notified': 16821,\n",
       " 'unusual': 25263,\n",
       " 'activity': 1556,\n",
       " 'detected': 7421,\n",
       " 'officials': 17027,\n",
       " 'went': 26079,\n",
       " 'while': 26138,\n",
       " 'shipped': 21765,\n",
       " 'back': 2945,\n",
       " 'data': 6905,\n",
       " 'law': 14191,\n",
       " 'authorities': 2837,\n",
       " 'done': 7999,\n",
       " 'robust': 20620,\n",
       " 'thorough': 24134,\n",
       " 'neither': 16547,\n",
       " 'nor': 16761,\n",
       " 'make': 15012,\n",
       " 'lightly': 14489,\n",
       " 'believe': 3395,\n",
       " 'alexandra': 1923,\n",
       " 'tower': 24460,\n",
       " 'spokeswoman': 22635,\n",
       " 'north': 16780,\n",
       " 'carolina': 4747,\n",
       " 'based': 3194,\n",
       " 'told': 24329,\n",
       " 'magazine': 14950,\n",
       " 'evidence': 9110,\n",
       " 'criminal': 6569,\n",
       " 'service': 21545,\n",
       " 'whose': 26185,\n",
       " 'brief': 4159,\n",
       " 'investigations': 13075,\n",
       " 'financial': 9779,\n",
       " 'crime': 6567,\n",
       " 'loss': 14747,\n",
       " 'new': 16598,\n",
       " 'york': 26633,\n",
       " 'senator': 21458,\n",
       " 'charles': 5086,\n",
       " 'schumer': 21200,\n",
       " 'commercial': 5735,\n",
       " 'whether': 26134,\n",
       " 'terrorism': 23990,\n",
       " 'or': 17193,\n",
       " 'complicated': 5845,\n",
       " 'background': 2960,\n",
       " 'checks': 5132,\n",
       " 'hired': 11866,\n",
       " 'increasingly': 12607,\n",
       " 'sensitive': 21480,\n",
       " 'positions': 18529,\n",
       " 'democrat': 7218,\n",
       " 'vermont': 25544,\n",
       " 'colleague': 5632,\n",
       " 'pat': 17737,\n",
       " 'leahy': 14237,\n",
       " 'credit': 6540,\n",
       " 'card': 4701,\n",
       " 'tracy': 24493,\n",
       " 'schmaler': 21171,\n",
       " '900': 1245,\n",
       " '000': 1,\n",
       " 'military': 15781,\n",
       " 'civilian': 5336,\n",
       " 'staff': 22757,\n",
       " 'defence': 7096,\n",
       " 'department': 7273,\n",
       " 'pentagon': 17910,\n",
       " 'spokesman': 22632,\n",
       " 'bollywood': 3860,\n",
       " 'dvd': 8281,\n",
       " 'fraudster': 10256,\n",
       " 'jailed': 13279,\n",
       " 'major': 15005,\n",
       " 'distributor': 7854,\n",
       " 'pirated': 18212,\n",
       " 'dvds': 8282,\n",
       " 'films': 9760,\n",
       " 'sent': 21485,\n",
       " 'prison': 18826,\n",
       " 'jayanti': 13336,\n",
       " 'amarishi': 2054,\n",
       " 'buhecha': 4339,\n",
       " 'cambridge': 4572,\n",
       " 'trademark': 24496,\n",
       " 'offences': 17008,\n",
       " 'sentenced': 21488,\n",
       " 'harrow': 11510,\n",
       " 'crown': 6632,\n",
       " 'court': 6442,\n",
       " 'tuesday': 24763,\n",
       " 'per': 17925,\n",
       " 'illegal': 12381,\n",
       " 'trade': 24494,\n",
       " 'called': 4552,\n",
       " 'biggest': 3580,\n",
       " 'pirates': 18213,\n",
       " 'sentencing': 21491,\n",
       " 'judge': 13528,\n",
       " 'phonographic': 18106,\n",
       " 'industry': 12685,\n",
       " 'bpi': 4037,\n",
       " 'worked': 26422,\n",
       " 'case': 4796,\n",
       " 'operation': 17149,\n",
       " 'launched': 14171,\n",
       " '2002': 432,\n",
       " 'received': 19638,\n",
       " 'activities': 1555,\n",
       " 'lasted': 14145,\n",
       " 'seven': 21571,\n",
       " 'heavy': 11652,\n",
       " 'penalty': 17887,\n",
       " 'enormous': 8805,\n",
       " 'damage': 6834,\n",
       " 'caused': 4868,\n",
       " 'legitimate': 14310,\n",
       " 'fake': 9439,\n",
       " 'manufactured': 15134,\n",
       " 'pakistan': 17549,\n",
       " 'malaysia': 15027,\n",
       " 'sold': 22347,\n",
       " 'wholesale': 26179,\n",
       " 'traded': 24495,\n",
       " 'conterfeit': 6154,\n",
       " 'stopped': 23023,\n",
       " 'car': 4693,\n",
       " 'standards': 22802,\n",
       " 'officers': 17023,\n",
       " 'uncovered': 24976,\n",
       " 'faked': 9440,\n",
       " 'inlay': 12804,\n",
       " 'cards': 4708,\n",
       " 'printed': 18814,\n",
       " 'registered': 19856,\n",
       " 'trademarks': 24497,\n",
       " 'despite': 7392,\n",
       " 'arrested': 2540,\n",
       " 'bailed': 3012,\n",
       " 'home': 11973,\n",
       " 'lock': 14665,\n",
       " 'contain': 6131,\n",
       " 'counterfeit': 6414,\n",
       " 'suspended': 23489,\n",
       " 'sued': 23289,\n",
       " 'employers': 8672,\n",
       " 'dealing': 6964,\n",
       " 'copies': 6284,\n",
       " 'classic': 5390,\n",
       " 'mohabbatein': 16009,\n",
       " 'film': 9752,\n",
       " 'distributors': 7855,\n",
       " 'hailed': 11322,\n",
       " 'conviction': 6247,\n",
       " 'boost': 3921,\n",
       " 'suffers': 23297,\n",
       " '40': 771,\n",
       " 'suffered': 23294,\n",
       " 'mainstream': 14997,\n",
       " 'productions': 18892,\n",
       " 'welcomed': 26060,\n",
       " 'news': 16613,\n",
       " 'sentence': 21487,\n",
       " 'warned': 25899,\n",
       " 'plenty': 18332,\n",
       " 'active': 1551,\n",
       " 'counterfeiters': 6415,\n",
       " 'organisation': 17220,\n",
       " 'director': 7616,\n",
       " 'david': 6923,\n",
       " 'martin': 15244,\n",
       " 'problem': 18858,\n",
       " 'simply': 21988,\n",
       " 'disappear': 7641,\n",
       " 'others': 17278,\n",
       " 'place': 18245,\n",
       " 'vital': 25710,\n",
       " 'efforts': 8459,\n",
       " 'field': 9712,\n",
       " 'german': 10696,\n",
       " 'goes': 10912,\n",
       " 'germany': 10699,\n",
       " 'economy': 8395,\n",
       " 'shrank': 21858,\n",
       " 'months': 16080,\n",
       " 'upsetting': 25324,\n",
       " 'sustained': 23503,\n",
       " 'recovery': 19707,\n",
       " 'confounded': 5978,\n",
       " 'expansion': 9237,\n",
       " 'quarter': 19252,\n",
       " 'contraction': 6184,\n",
       " 'estimate': 9007,\n",
       " 'zero': 26706,\n",
       " 'putting': 19208,\n",
       " 'standstill': 22810,\n",
       " 'july': 13557,\n",
       " 'onward': 17123,\n",
       " 'reliant': 19970,\n",
       " 'exports': 9311,\n",
       " 'unemployment': 25065,\n",
       " 'impending': 12465,\n",
       " 'cuts': 6768,\n",
       " 'welfare': 26063,\n",
       " 'mean': 15472,\n",
       " 'money': 16044,\n",
       " 'themselves': 24078,\n",
       " 'companies': 5780,\n",
       " 'volkswagen': 25752,\n",
       " 'daimlerchrysler': 6818,\n",
       " 'siemens': 21922,\n",
       " 'spent': 22579,\n",
       " 'talks': 23707,\n",
       " 'unions': 25129,\n",
       " 'trimming': 24669,\n",
       " 'jobs': 13431,\n",
       " 'costs': 6383,\n",
       " 'destatis': 7397,\n",
       " 'rising': 20555,\n",
       " 'outweighed': 17369,\n",
       " 'continuing': 6173,\n",
       " 'domestic': 7973,\n",
       " 'demand': 7202,\n",
       " 'relentless': 19964,\n",
       " 'value': 25441,\n",
       " 'euro': 9040,\n",
       " 'competitiveness': 5814,\n",
       " 'products': 18895,\n",
       " 'overseas': 17424,\n",
       " 'depress': 7312,\n",
       " 'prospects': 19018,\n",
       " 'nation': 16449,\n",
       " 'eurozone': 9056,\n",
       " 'senior': 21469,\n",
       " 'setting': 21560,\n",
       " 'european': 9047,\n",
       " 'central': 4946,\n",
       " 'beginning': 3356,\n",
       " 'talk': 23702,\n",
       " 'threat': 24151,\n",
       " 'inflation': 12729,\n",
       " 'prompting': 18972,\n",
       " 'fears': 9582,\n",
       " 'ecb': 8373,\n",
       " 'mandate': 15082,\n",
       " 'fight': 9729,\n",
       " 'boosting': 3923,\n",
       " 'threaten': 24152,\n",
       " 'tautou': 23806,\n",
       " 'da': 6800,\n",
       " 'vinci': 25655,\n",
       " 'french': 10293,\n",
       " 'actress': 1559,\n",
       " 'audrey': 2802,\n",
       " 'amelie': 2083,\n",
       " 'female': 9635,\n",
       " 'adaptation': 1574,\n",
       " 'code': 5577,\n",
       " 'movie': 16203,\n",
       " 'version': 25551,\n",
       " 'dan': 6852,\n",
       " 'brown': 4259,\n",
       " 'selling': 21442,\n",
       " 'novel': 16841,\n",
       " 'directed': 7609,\n",
       " 'ron': 20681,\n",
       " 'howard': 12125,\n",
       " 'stars': 22837,\n",
       " 'tom': 24337,\n",
       " 'hanks': 11427,\n",
       " 'cracking': 6487,\n",
       " 'partner': 17698,\n",
       " 'various': 25471,\n",
       " 'newspapers': 16618,\n",
       " 'currently': 6734,\n",
       " 'starring': 22836,\n",
       " 'long': 14704,\n",
       " 'engagement': 8764,\n",
       " 'jean': 13345,\n",
       " 'pierre': 18160,\n",
       " 'jeunet': 13393,\n",
       " 'responsible': 20251,\n",
       " 'directing': 7610,\n",
       " '2001': 429,\n",
       " 'starred': 22835,\n",
       " 'role': 20656,\n",
       " 'critically': 6589,\n",
       " 'acclaimed': 1452,\n",
       " 'dirty': 7625,\n",
       " 'pretty': 18763,\n",
       " 'oscar': 17263,\n",
       " 'preferring': 18674,\n",
       " 'name': 16391,\n",
       " 'hollywood': 11965,\n",
       " 'kate': 13661,\n",
       " 'beckinsale': 3320,\n",
       " 'widely': 26191,\n",
       " 'tipped': 24282,\n",
       " 'possibility': 18538,\n",
       " 'alongside': 2012,\n",
       " 'vanessa': 25453,\n",
       " 'paradis': 17622,\n",
       " 'juliette': 13556,\n",
       " 'binoche': 3611,\n",
       " 'thriller': 24165,\n",
       " 'upon': 25313,\n",
       " '17': 249,\n",
       " 'centred': 4953,\n",
       " 'global': 10846,\n",
       " 'conspiracy': 6083,\n",
       " 'surrounding': 23464,\n",
       " ...}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16\\. **Run the SVD function with different values for components: 1, 2, 4, 5, 10, 15, 20, 50, 100. Plot the explained variance ratio for each component of Truncated SVD.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components = 1 and explained variance = 0.8302335701200921\n",
      "Number of components = 2 and explained variance = 0.91650936322083\n",
      "Number of components = 4 and explained variance = 0.9291569440678302\n",
      "Number of components = 5 and explained variance = 0.934486055411578\n",
      "Number of components = 10 and explained variance = 0.9477460316797044\n",
      "Number of components = 15 and explained variance = 0.9529730120135838\n",
      "Number of components = 20 and explained variance = 0.9560370191126871\n",
      "Number of components = 50 and explained variance = 0.9645716958622763\n",
      "Number of components = 100 and explained variance = 0.9711054139797352\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_comp = [1, 2, 4, 5, 10, 15, 20, 50, 100] # list containing different values of components\n",
    "explained = [] # explained variance ratio for each component of Truncated SVD\n",
    "for x in n_comp:\n",
    "    svd = TruncatedSVD(n_components=x, random_state=321)\n",
    "    svd.fit(Xc)\n",
    "    explained.append(svd.explained_variance_ratio_.sum())\n",
    "    print(\"Number of components = %r and explained variance = %r\"%(x,svd.explained_variance_ratio_.sum()))\n",
    "\n",
    "plt.plot(n_comp, explained)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.title(\"Plot of Number of components v/s explained variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17\\. **How many components are needed to explain at least 95% of the variance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the selected values, it seems 15 components are needed to explain 95% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18\\. **Use these components and train a SVM model on the BBC dataset. Make a pipeline for your model. Compare your results on the test set with the previous pipelines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf6 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('feature_extraction', TfidfTransformer()),\n",
    "    ('feature_selection', TruncatedSVD(n_components=15, random_state=321)),\n",
    "    ('classification', LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabula...\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('feature_selection',\n",
       "                 TruncatedSVD(algorithm='randomized', n_components=15, n_iter=5,\n",
       "                              random_state=321, tol=0.0)),\n",
       "                ('classification',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf6.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.91      0.92      0.92        92\n",
      "entertainment       1.00      0.94      0.97        84\n",
      "     politics       0.91      0.92      0.92        77\n",
      "        sport       0.99      1.00      1.00       111\n",
      "         tech       0.94      0.96      0.95        81\n",
      "\n",
      "     accuracy                           0.95       445\n",
      "    macro avg       0.95      0.95      0.95       445\n",
      " weighted avg       0.95      0.95      0.95       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred6 = clf6.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred6, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19\\. **Can you make a plot of accuracies of your 5 pipelines?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

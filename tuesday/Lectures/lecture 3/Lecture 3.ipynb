{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature selection in text  \n",
    "&nbsp;\n",
    "\n",
    "\n",
    "Ayoub Bagheri, <a.bagheri@uu.nl>  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/uu_logo.png\" style=\"float: right;\" width=\"100\" height=\"100\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture’s Plan\n",
    "&nbsp;\n",
    "\n",
    "1. How to do feature selection for text data?\n",
    "\n",
    "2. Is PCA a FS method for text?\n",
    "\n",
    "3. Other methods?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An illustration of VS model \n",
    "&nbsp;\n",
    "\n",
    "- All documents are projected into this concept space\n",
    "\n",
    "<img src=\"img/page 3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Selection:  What\n",
    "&nbsp;\n",
    "\n",
    "<center><span style=\"color:red\">You have some data, and you want to use it to build a classifier, so that you can predict something\n",
    " (e.g. email spam classification)\n",
    "</span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Selection:  What\n",
    "&nbsp;\n",
    "\n",
    "<center><span style=\"color:blue\">You have some data, and you want to use it to build a classifier, so that you can predict something (e.g. email spam classification)\n",
    "</span></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><span style=\"color:red\">The data has 10,000 fields (features)</span></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Selection:  What\n",
    "&nbsp;\n",
    "\n",
    "<center><span style=\"color:red\">You have some data, and you want to use it to build a classifier, so that you can predict something (e.g. email spam classification)\n",
    "</span></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><span style=\"color:blue\">The data has 10,000 fields (features)</span></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><span style=\"color:brown\">you need to cut it down to 1,000 fields before you try machine learning. Which 1,000? </span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Selection:  What\n",
    "&nbsp;\n",
    "\n",
    "<center><span style=\"color:blue\">You have some data, and you want to use it to build a classifier, so that you can predict something (e.g. email spam classification)\n",
    "</span></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><span style=\"color:red\">The data has 10,000 fields (features)</span></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><span style=\"color:blue\">you need to cut it down to 1,000 fields before you try machine learning. Which 1,000? </span></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><span style=\"color:red\">The process of choosing the 1,000 fields to use is called Feature Selection</span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Selection:  Why?\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Selection:  Why?\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 9.png\" width=\"700\">\n",
    "\n",
    "<p style=\"float:right\">From http://elpub.scix.net/data/works/att/02-28.content.pdf</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why accuracy reduces:\n",
    "&nbsp;\n",
    "\n",
    "- Note: suppose the best feature set has 20 features. If you *add* another 5 features, typically the accuracy of machine learning may reduce. But you still have the original 20 features!! Why does this happen???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Noise / Explosion\n",
    "&nbsp;\n",
    "\n",
    "- The additional features typically add *noise*. Machine learning will pick up on spurious correlations, that might be true in the training set, but not in the test set.\n",
    "\n",
    "- For some ML methods, more features means more *parameters* to learn (more NN weights, more decision tree nodes, etc…) – the increased space of possibilities is more difficult to search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature selection\n",
    "&nbsp;\n",
    "\n",
    "- Why we need FS:\n",
    "    1. to improve performance (in terms of speed, predictive power, simplicity of the model).\n",
    "\n",
    "    2. to visualize the data for model selection.\n",
    "\n",
    "    3. to reduce dimensionality and remove noise.\n",
    "\n",
    "- *Feature Selection* is a process that chooses an optimal subset of features according to a certain criterion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature selection\n",
    "&nbsp;\n",
    "\n",
    "- Reasons for performing FS may include:\n",
    "    - removing irrelevant data.\n",
    "    - increasing predictive accuracy of learned models.\n",
    "    - reducing the cost of the data.\n",
    "    - improving learning efficiency, such as reducing storage requirements and computational cost.\n",
    "    - reducing the complexity of the resulting model description, improving the understanding of the data and the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature selection for text\n",
    "&nbsp;\n",
    "\n",
    "- Feature selection is the process of selecting a specific subset of the terms of the training set and using only them in the classification algorithm.\n",
    "- high dimensionality of text features\n",
    "- Select the most informative features for model training\n",
    "    - Reduce noise in feature representation\n",
    "    - Improve final classification performance\n",
    "    - Improve training/testing efficiency\n",
    "        - Less time complexity\n",
    "        - Fewer training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\textbf {Feature Selection Methods}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Selection Methods\n",
    "&nbsp;\n",
    "\n",
    "<ul>\n",
    "<li><span style=\"color:blue; font-weight:bold\">Thousands to millions of low level features</span>: select the most relevant one to build <span style=\"color:blue; font-weight:bold\">better, faster, and easier to understand</span> learning machines.</li>\n",
    "</ul>\n",
    "\n",
    "<img src=\"img/page 16_1.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Feature Selection Methods\n",
    "&nbsp;\n",
    "\n",
    "<ul>\n",
    "<li><span style=\"color:blue; font-weight:bold\">Thousands to millions of low level features</span>: select the most relevant one to build <span style=\"color:blue; font-weight:bold\">better, faster, and easier to understand</span> learning machines.</li>\n",
    "</ul>\n",
    "\n",
    "<img src=\"img/page 16_2.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other criteria\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Other criteria\n",
    "&nbsp;\n",
    "\n",
    "<center>A choice of feature selection ranking methods           depending on the nature of:</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Other criteria\n",
    "&nbsp;\n",
    "\n",
    "<center>A choice of feature selection ranking methods           depending on the nature of:</center>\n",
    "\n",
    "<ul>\n",
    "    <li><u>the variables and the target</u> (binary, categorical, continuous)\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Other criteria\n",
    "&nbsp;\n",
    "\n",
    "<center>A choice of feature selection ranking methods           depending on the nature of:</center>\n",
    "\n",
    "<ul>\n",
    "    <li><u>the variables and the target</u> (binary, categorical, continuous)\n",
    "</li>\n",
    "    <li><u>the problem</u> (dependencies between variables, linear/non-linear relationships between variables and target)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Other criteria\n",
    "&nbsp;\n",
    "\n",
    "<center>A choice of feature selection ranking methods           depending on the nature of:</center>\n",
    "\n",
    "<ul>\n",
    "    <li><u>the variables and the target</u> (binary, categorical, continuous)\n",
    "</li>\n",
    "    <li><u>the problem</u> (dependencies between variables, linear/non-linear relationships between variables and target)</li>\n",
    "    <li><u>the available data</u> (number of examples and number of variables, noise in data)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Other criteria\n",
    "&nbsp;\n",
    "\n",
    "<center>A choice of feature selection ranking methods           depending on the nature of:</center>\n",
    "\n",
    "<ul>\n",
    "    <li><u>the variables and the target</u> (binary, categorical, continuous)\n",
    "</li>\n",
    "    <li><u>the problem</u> (dependencies between variables, linear/non-linear relationships between variables and target)</li>\n",
    "    <li><u>the available data</u>(number of examples and number of variables, noise in data)</li>\n",
    "    <li><u>the available tabulated statistics</u></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Filters, Wrappers, and Embedded methods\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 18.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Menti\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\textbf {Wrapper methods}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature selection methods\n",
    "&nbsp;\n",
    "\n",
    "- Wrapper method\n",
    "    - Find the best subset of features for a particular classification method\n",
    "    \n",
    "<img src=\"img/page 21.png\" width=\"550\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Selection - Wrappers\n",
    "&nbsp;\n",
    "\n",
    "- Optimizes for a specific learning algorithm\n",
    "- The feature subset selection algorithm is a \"wrapper\" around the learning algorithm\n",
    "    1. Pick a feature subset and pass it in to learning algorithm\n",
    "    2. Create training/test set based on the feature subset\n",
    "    3. Train the learning algorithm with the training set\n",
    "    4. Find accuracy (objective) with validation set\n",
    "    5. Repeat for all feature subsets and pick the feature subset which led to the highest predictive accuracy (or other objective)\n",
    "- Basic approach is simple\n",
    "- Variations are based on how to select the feature subsets, since there are an exponential number of subsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Selection - Wrappers\n",
    "&nbsp;\n",
    "\n",
    "- Exhaustive Search - Exhausting\n",
    "- Forward Search – $O(n^2 \\cdot \\text{learning/testing time})$ - Greedy\n",
    "    1. Score each feature by itself and add the best feature to the initially empty set FS (FS will be our final Feature Set)\n",
    "    2. Try each subset consisting of the current FS plus one remaining feature and add the best feature to FS\n",
    "    3. Continue until stop getting significant improvement (over a window)\n",
    "- Backward Search – $O(n^2 \\cdot \\text{learning/testing time})$ - Greedy\n",
    "    1. Score the initial complete FS \n",
    "    2. Try each subset consisting of the current FS minus one feature in FS and drop the feature from FS causing least decrease in accuracy\n",
    "    3. Continue until begin to get significant decreases in accuracy\n",
    "- Branch and Bound and other heuristic approaches available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature selection methods\n",
    "&nbsp;\n",
    "\n",
    "- Wrapper method\n",
    "    - Search in the whole space of feature groups\n",
    "        - Sequential forward selection or genetic search to speed up the search\n",
    "        \n",
    "<img src=\"img/page 24.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature selection methods\n",
    "&nbsp;\n",
    "\n",
    "- Wrapper method\n",
    "    - Consider all possible dependencies among the features\n",
    "    - Impractical for text categorization\n",
    "        - Cannot deal with large feature set\n",
    "        - A NP-complete problem\n",
    "            - No direct relation between feature subset selection and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wrappers for feature selection\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 26.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Search Strategies\n",
    "&nbsp;\n",
    "\n",
    "- **Exhaustive search**.\n",
    "- **Simulated annealing, genetic algorithms**.\n",
    "- **Beam search**: keep k best path at each step. \n",
    "- **Greedy search**: forward selection or backward elimination.\n",
    "- **PTA(l,r)**: plus l , take away r – at each step, run SFS l times then SBS r times.\n",
    "- **Floating search** (SFFS and SBFS): One step of SFS (resp. SBS), then SBS (resp. SFS) as long as we find better subsets than those of the same size obtained so far. Any time, if a better subset of the same size was already found, switch abruptly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Forward Selection (wrapper)\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 28.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Forward Selection (embedded)\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 29.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backward Elimination (wrapper)\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 30.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backward Elimination (embedded)\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 31.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scaling Factors\n",
    "&nbsp;\n",
    "\n",
    "<b>Idea</b>: Transform a discrete space into a continuous space.\n",
    "\n",
    "<img src=\"img/page 32.png\" with=\"450\">\n",
    "\n",
    "<ul>\n",
    "    <li>Discrete indicators of feature presence: $\\sigma_i \\in \\{0,1\\}$</li>\n",
    "    <li>Continuous scaling factors: $\\sigma_i \\in \\text{IR}$</li>\n",
    "</ul>\n",
    "<br><br>\n",
    "<center><span style=\"background-color:yellow; font-size:26pt\">Now we can do gradient descent!</span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\textbf{Filter methods}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature selection methods\n",
    "&nbsp;\n",
    "\n",
    "- Filter method\n",
    "    - Evaluate the features <u>independently</u> from the classifier and other features\n",
    "        - No indication of a classifier’s performance on the selected features\n",
    "        - No dependency among the features\n",
    "    - Feasible for very large feature set\n",
    "<br>\n",
    "<center><img src=\"img/page 34.png\" width=\"550\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Selection - Wrappers\n",
    "&nbsp;\n",
    "\n",
    "- Optimizes for a specific learning algorithm\n",
    "- The feature subset selection algorithm is a \"wrapper\" around the learning algorithm\n",
    "    1. Pick a feature subset and pass it in to learning algorithm\n",
    "    2. Create training/test set based on the feature subset\n",
    "    3. Train the learning algorithm with the training set\n",
    "    4. Find accuracy (objective) with validation set\n",
    "    5. Repeat for all feature subsets and pick the feature subset which led to the highest predictive accuracy (or other objective)\n",
    "- Basic approach is simple\n",
    "- Variations are based on how to select the feature subsets, since there are an exponential number of subsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Document frequency\n",
    "&nbsp;\n",
    "\n",
    "- Rare words: non-influential for global prediction, reduce vocabulary size\n",
    "\n",
    "<img src=\"img/page 36.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Information gain\n",
    "&nbsp;\n",
    "\n",
    "- Decrease in entropy of categorical prediction when the feature is present or absent\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "IG(t) = &-\\sum_c{p(c)log{p(c)}} \\\\\n",
    "        &+ p(t)\\sum_c{p(c|t)log{p(c|t)}} \\\\\n",
    "        &+ p(\\bar t)\\sum_c{p(c|\\bar t)log{p(c|\\bar t)}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Information gain\n",
    "&nbsp;\n",
    "\n",
    "- Decrease in entropy of categorical prediction when the feature is present or absent\n",
    "\n",
    "<img src=\"img/page 37.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gini Index\n",
    "&nbsp;\n",
    "\n",
    "Let $p(c | t)$ be the conditional probability that a document belongs to class $c$, given the fact that it contains the term $t$. Therefore, we have:\n",
    "\n",
    "$$\\sum^k_{c=1}{p(c | t)=1}$$\n",
    "\n",
    "Then, the gini-index for the term $t$, denoted by $G(t)$ is defined as:\n",
    "\n",
    "$$G(t) = \\sum^k_{c=1}{p(c | t)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gini Index \n",
    "&nbsp;\n",
    "\n",
    "- The value of the gini-index lies in the range $(1/k, 1)$. \n",
    "\n",
    "- Higher values of the gini-index indicate a greater discriminative power of the term $t$. \n",
    "\n",
    "- If the global class distribution is skewed, the gini-index may not accurately reflect the discriminative power of the underlying attributes.\n",
    "\n",
    "- &#10132; normalized gini-index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Normalized Gini Index \n",
    "&nbsp;\n",
    "\n",
    "- Let $p(c)$ represent the unconditional probability of class $c$. Then, we determine the normalized probability value $p'(c | t)$ as: \n",
    "\n",
    "$$p'(c|t) \\equiv \\frac{p(c|t)/p(c)}{\\sum_{i=1}^k{p(i|t)/p(i)}} $$\n",
    "\n",
    "- Then, the gini-index is computed in terms of these normalized probability values. \n",
    "\n",
    "$$G(t) \\equiv \\sum^k_{c=1}{p'(c|t)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mutual Information\n",
    "&nbsp;\n",
    "\n",
    "- The pointwise mutual information $M_c(t)$ between the term $t$ and the class $c$ is defined on the basis of the level of co-occurrence between the class $c$ and term $t$. Let $p(c)$ be the unconditional probability of class $c$, and $p(c | t)$ be the probability of class $c$, given that the document contains the term $t$. \n",
    "\n",
    "- Let $p(t)$ be the fraction of the documents containing the term $t$, i.e. the unconditional probability of term $t$. \n",
    "\n",
    "- The expected co-occurrence of class $c$ and term $t$ on the basis of mutual independence is given by $p(c) \\cdot p(t)$. The true co-occurrence is of course given by $p(c | t) \\cdot p(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mutual Information \n",
    "&nbsp;\n",
    "\n",
    "- In practice, the value of $p(c | t) \\cdot p(t)$ may be much larger or smaller than $p(c) \\cdot p(t)$, depending upon the level of correlation between the class $c$ and term $t$. The mutual information is defined in terms of the ratio between these two values.\n",
    "\n",
    "$$M_c(t) = log(\\frac{p(c|t) \\cdot p(t)}{p(c) \\cdot p(t)}) = log(\\frac{p(c|t)}{p(c)})$$\n",
    "\n",
    "- Clearly, the term $t$ is positively correlated to the class $c$, when $M_c (t) > 0$, and the term $t$ is negatively correlated to class $c$, when $M_c (t) < 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mutual Information \n",
    "&nbsp;\n",
    "\n",
    "- Note that $M_c(t)$ is specific to a particular class $c$. We need to compute the overall mutual information as a function of the mutual information of the term $t$ with the different classes.\n",
    "\n",
    "$$M_{avg}(t) = \\sum^k_{c=1}{p(c) \\cdot M_c(t)}$$\n",
    "$$M_{max}(t) = \\max_c{\\{M_c(t)\\}}$$\n",
    "\n",
    "- The second measure is particularly useful, when it is more important to determine high levels of positive correlation of the term $t$ with any of the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ${\\chi}^2$-Statistic \n",
    "&nbsp;\n",
    "\n",
    "- The ${\\chi^2}$-statistic is a different way to compute the lack of independence between the term $t$ and a particular class $c$. Let $n$ be the total number of documents, then:\n",
    "\n",
    "$${\\chi}_c^2(t) = \\frac{n \\cdot p(t)^2 \\cdot (p(c|t) - p(c))^2}{p(t) \\cdot (1- p(t)) \\cdot p(c) \\cdot (1 - p(c))}$$\n",
    "\n",
    "- As in the case of the mutual information, we can compute a global ${\\chi^2}$ statistic from the class-specific values. One major advantage of the ${\\chi^2}$-statistic is that it is a normalized value and we can test statistical significance using the ${\\chi^2}$ distribution with one degree of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  ${\\chi}^2$-Statistic \n",
    "&nbsp;\n",
    "\n",
    "- Test whether distributions of two categorical variables are independent of one another\n",
    "  - $H_0$: they are independent\n",
    "  - $H_1$: they are dependent \n",
    "\n",
    "<img src=\"img/page 45_1.png\" width=\"50%\">\n",
    "<img src=\"img/page 45_2.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ${\\chi}^2$-Statistic\n",
    "&nbsp;\n",
    "\n",
    "- Test whether distributions of two categorical variables are independent of one another\n",
    "  \n",
    "  - Degree of freedom = $(\\#col-1) \\times (\\#row-1)$\n",
    "  \n",
    "  - Significance level: $\\alpha$, i.e., $p\\mbox{-}value<\\alpha$ \n",
    "<p><span style=\"color:red\">&#8627;</span> Look into ${\\chi}^2$ distribution table to find the threshold\n",
    "</p>\n",
    "\n",
    "<img src=\"img/page 46.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature scoring metrics\n",
    "&nbsp;\n",
    "\n",
    "- ${\\chi}^2$ statistics\n",
    "\n",
    "  - Test whether distributions of two categorical variables are independent of one another\n",
    "    \n",
    "    - Degree of freedom = $(\\#col-1) \\times (\\#row-1)$\n",
    "  \n",
    "    - Significance level: $\\alpha$, i.e., $p\\mbox{-}value<\\alpha$ \n",
    "\n",
    "    - For the features passing the threshold, rank them by descending order of ${\\chi}^2$ values and choose the top $k$ features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature scoring metrics\n",
    "&nbsp;\n",
    "\n",
    "- ${\\chi}^2$ statistics with multiple categories\n",
    "\n",
    "  - ${\\chi}^2=\\sum_c{p(c) {\\chi}^2(c,t)}$\n",
    "    \n",
    "    - Expectation of ${\\chi}^2$ over all the categories\n",
    "  <br>\n",
    "  <br>\n",
    "  - ${\\chi}^2(t) = \\underset{c}{max}\\ {\\chi}^2(c,t)$\n",
    "\n",
    "    - Strongest dependency between a category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other metrics\n",
    "&nbsp;\n",
    "\n",
    "- Many other metrics (Same trick as in $\\chi^2$ statistics for multi-class cases)\n",
    "    \n",
    "    - Mutual information\n",
    "  \n",
    "        - Relatedness between term $t$ and class $c$\n",
    "  \n",
    "  $$PMI(t;c) = p(t,c)log(\\frac{p(t,c)}{p(t)p(c)})$$\n",
    "\n",
    "    - Odds ratio\n",
    "\n",
    "        - Odds of term $t$ occurring with class $c$ normalized by that without $c$\n",
    "\n",
    "$$Odds(t;c) = \\frac{p(t,c)}{1 - p(t,c)} \\times \\frac{1 - p(t,\\bar{c})}{p(t,\\bar{c})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\textsf{PCA}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Selection and Feature Reduction\n",
    "&nbsp;\n",
    "\n",
    "- Given $n$ original features, it is often advantageous to reduce this to a smaller set of features for actual training\n",
    "    - Can improve/maintain accuracy if we can preserve the most relevant information while discarding the most irrelevant information\n",
    "    - and/or Can make the learning process more computationally and algorithmically manageable by working with less features\n",
    "    - Curse of dimensionality requires an exponential increase in data set size in relation to the number of features to learn without overfit – thus decreasing features can be critical\n",
    "- *Feature Selection* seeks a *subset* of the $n$ original features which retains most of the relevant information\n",
    "    - Filters, Wrappers\n",
    "- *Feature Reduction <u>combines/fuses</u>* the $n$ original features into a smaller set of newly created features which hopefully retains most of the relevant information from *all* the original features - Data fusion (e.g. LDA, PCA, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA – Principal Components Analysis\n",
    "&nbsp;\n",
    "\n",
    "- PCA is one of the most common feature reduction techniques\n",
    "- A linear method for dimensionality reduction\n",
    "- Allows us to combine much of the information contained in $n$ features into $p$ features where $p < n$\n",
    "- PCA is *unsupervised* in that it does not consider the output class/value of an instance – There are other algorithms which do (e.g. Linear Discriminant Analysis)\n",
    "- PCA works well in many cases where data has mostly linear correlations\n",
    "- Non-linear dimensionality reduction is also a successful area and can give much better results for data with significant non-linearities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA Overview\n",
    "&nbsp;\n",
    "\n",
    "- Seek new set of bases which correspond to the highest variance in the data\n",
    "- Transform $n$-dimensional *normalized* data to a new $n$-dimensional basis\n",
    "    - The new dimension with the most variance is the first principal component\n",
    "    - The next is the second principal component, etc.\n",
    "    - Note $z_1$ <u>combines/fuses</u> significant information from both $x_1$ and $x_2$\n",
    "- Drop dimensions for which there is little variance\n",
    "\n",
    "<center><img src=\"img/page 53.png\" width=\"550\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variance and Covariance\n",
    "&nbsp;\n",
    "\n",
    "- Variance is a measure of data spread in one dimension (feature)\n",
    "- Covariance measures how two dimensions (features) vary with respect to each other\n",
    "\n",
    "$$var(X) = \\frac{\\sum_{i = 1}^n{(X_i - \\bar X)(X_i - \\bar X)}}{(n - 1)}$$\n",
    "$$cov(X,Y) = \\frac{\\sum_{i = 1}^n{(X_i - \\bar X)(Y_i - \\bar Y)}}{(n - 1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Covariance and the Covariance Matrix\n",
    "&nbsp;\n",
    "\n",
    "- Considering the sign (rather than exact value) of covariance:\n",
    "    - Positive value means that as one feature increases or decreases the other does also (positively correlated)\n",
    "    - Negative value means that as one feature increases the other decreases and vice versa (negatively correlated)\n",
    "    - A value close to zero means the features are independent\n",
    "    - If highly covariant, are both features necessary?\n",
    "- Covariance matrix is an $n \\times n$ matrix containing the covariance values for all pairs of features in a data set with $n$ features (dimensions)\n",
    "- The diagonal contains the covariance of a feature with itself which is the variance (which is the square of the standard deviation)\n",
    "- The matrix is symmetric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA Example\n",
    "&nbsp;\n",
    "\n",
    "- First step is to center the original data around 0 by subtracting the mean in each dimension\n",
    "\n",
    "<img src=\"img/page 56.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA Example\n",
    "&nbsp;\n",
    "\n",
    "- Second: Calculate the covariance matrix of the centered data\n",
    "- Only $2 \\times 2$ for this case\n",
    "\n",
    "<img src=\"img/page 57.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA Example\n",
    "&nbsp;\n",
    "\n",
    "- Third: Calculate the unit eigenvectors and eigenvalues of the covariance matrix (remember linear algebra)\n",
    "    - Covariance matrix is always square $n \\times n$ and positive semi-definite, thus n non-negative eigenvalues will exist\n",
    "    - All eigenvectors (principal components) are orthogonal to each other and form the new set of bases/dimensions for the data (columns)\n",
    "    - The magnitude of each eigenvalue corresponds to the variance along that new dimension – Just what we wanted!\n",
    "    - We can sort the principal components according to their eigenvalues\n",
    "    - Just keep those dimensions with the largest eigenvalues\n",
    "    \n",
    "<center><img src=\"img/page 58.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA Example\n",
    "&nbsp;\n",
    "\n",
    "- Below are the two eigenvectors overlaying the centered data\n",
    "- Which eigenvector has the largest eigenvalue?\n",
    "- Fourth Step:  Just keep the $p$ eigenvectors with the largest eigenvalues\n",
    "    - Do lose some information, but if we just drop dimensions with small eigenvalues then we lose only a little information, hopefully noise\n",
    "    - We can then have $p$ input features rather than $n$\n",
    "    - The $p$ features contain the most pertinent *combined* information from all $n$ original features\n",
    "    - How many dimensions $p$ should we keep?\n",
    "    \n",
    "<center><img src=\"img/page 59.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA Example\n",
    "&nbsp;\n",
    "\n",
    "- Last Step:  Transform the $n$ features to the $p$ ($< n$) chosen bases (Eigenvectors)\n",
    "- Transformed data (m instances) is a matrix multiply $T =  A \\times B$\n",
    "    - $A$ is a $p \\times n$ matrix with the $p$ principal components in the rows, component one on top\n",
    "    - $B$ is a $n \\times m$ matrix containing the transposed centered original data set\n",
    "    - $T^T$ is a $m \\times p$ matrix containing the transformed data set  \n",
    "- Now we have the new transformed data set with dimensionality $p$\n",
    "- Keep matrix $A$ to transform future centered data instances\n",
    "- Below is shown the transform of both dimensions. Would if we just kept the $1^{st}$ component?\n",
    "\n",
    "<center><img src=\"img/page 60.png\" width=\"700\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA Algorithm Summary\n",
    "&nbsp;\n",
    "\n",
    "1. Center the $n$ normalized TS features (subtract the n means)\n",
    "2. Calculate the covariance matrix of the centered TS\n",
    "3. Calculate the unit eigenvectors and eigenvalues of the covariance matrix\n",
    "4. Keep the $p$ ($< n$) eigenvectors with the largest eigenvalues\n",
    "5. Matrix multiply the p eigenvectors with the centered TS to get a new TS with only $p$ features\n",
    "\n",
    "- Given a novel instance during execution\n",
    "    1. Center the normalized instance (subtract the $n$ means)\n",
    "    2. Do the matrix multiply (step 5 above) to change the new instance from $n$ to $p$ features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis of Text Data\n",
    "&nbsp;\n",
    "\n",
    "- <a href=\"https://www.displayr.com/principal-component-analysis-of-text-data/\">https://www.displayr.com/principal-component-analysis-of-text-data/</a>\n",
    "- <a href=\"https://community.jmp.com/t5/JMP-Blog/Text-analysis-in-the-social-sciences-A-new-spectrum-of/ba-p/45139\">https://community.jmp.com/t5/JMP-Blog/Text-analysis-in-the-social-sciences-A-new-spectrum-of/ba-p/45139</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\textbf{Embedded methods}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Formalism\n",
    "&nbsp;\n",
    "\n",
    "- Many learning algorithms are cast into a minimization of some regularized functional:\n",
    "\n",
    "$$\\min_\\alpha{\\hat{R}(\\alpha, \\sigma)} = \\min_\\alpha{\\sum_{k=1}^m{L(f(\\alpha, \\sigma \\circ x_k), y_k) + \\Omega(\\alpha)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Formalism\n",
    "&nbsp;\n",
    "\n",
    "- Many learning algorithms are cast into a minimization of some regularized functional:\n",
    "\n",
    "<center><img src=\"img/page 64.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Embedded method\n",
    "&nbsp;\n",
    "\n",
    "- Embedded methods are a good inspiration to design new feature selection techniques for your own algorithms:\n",
    "    - Find a functional that represents your prior knowledge about what a good model is.\n",
    "    - Add the s weights into the functional and make sure it’s either differentiable or you can perform a sensitivity analysis efficiently\n",
    "    - Optimize alternatively according to $\\alpha$ and $\\sigma$\n",
    "    - Use early stopping (validation set) or your own stopping criterion to stop and select the subset of features\n",
    "\n",
    "- Embedded methods are therefore not too far from wrapper techniques and can be extended to multiclass, regression, etc…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lasso vs Ridge\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The $l_1$ SVM\n",
    "&nbsp;\n",
    "\n",
    "- A version of SVM where $\\Omega(w) = ||w||^2$ is replaced by the $l_1$ norm $\\Omega(w) = \\sum_i{|w_i|}$ \n",
    "- Can be considered an embedded feature selection method:\n",
    "    - Some weights will be drawn to zero (tend to remove redundant features)\n",
    "    - Difference from the regular SVM where redundant features are included\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The $l_0$ SVM\n",
    "&nbsp;\n",
    "\n",
    "- Replace the regularizer $||w||^2$ by the $l_0$ norm $\\sum_{i=1}^n{1_{w_i \\neq 0}}$\n",
    "\n",
    "- Further replace $\\sum_{i=1}^n{1_{w_i \\neq 0}}$ by $\\sum_i{log{(\\epsilon + |w_i|)}}$\n",
    "\n",
    "- Boils down to the following multiplicative update algorithm:\n",
    "\n",
    "    1. Set $\\sigma = (1, ..., 1)$\n",
    "    2. Get $w^*$ solution of an SVM on data set where each input is scaled by $\\sigma$\n",
    "    3. Set $\\sigma = |w^*| \\circ \\sigma$\n",
    "    4. back to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Menti\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bilevel optimization;\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "    <img src=\"img/page 70_1.png\" width=\"400\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "    Split data into 3 sets: <span style=\"color:red\">training</span>, <span style=\"color:pink\">validation</span>, and <span style=\"color:green\">test set</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bilevel optimization;\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "    <img src=\"img/page 70_2.png\" width=\"400\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "    Split data into 3 sets: <span style=\"color:red\">training</span>, <span style=\"color:pink\">validation</span>, and <span style=\"color:green\">test set</span>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bilevel optimization;\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "    <img src=\"img/page 70_3.png\" width=\"400\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "    Split data into 3 sets: <span style=\"color:red\">training</span>, <span style=\"color:pink\">validation</span>, and <span style=\"color:green\">test set</span>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bilevel optimization;\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "    <img src=\"img/page 70_4.png\" width=\"400\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "    Split data into 3 sets: <span style=\"color:red\">training</span>, <span style=\"color:pink\">validation</span>, and <span style=\"color:green\">test set</span>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bilevel optimization;\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "    <img src=\"img/page 70_4.png\" width=\"400\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "    Split data into 3 sets: <span style=\"color:red\">training</span>, <span style=\"color:pink\">validation</span>, and <span style=\"color:green\">test set</span>.\n",
    "    \n",
    "<ol>\n",
    "    <li>For each feature subset, train predictor on <span style=\"color:red\">training data</span>.</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bilevel optimization;\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "    <img src=\"img/page 70_4.png\" width=\"400\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "    Split data into 3 sets: <span style=\"color:red\">training</span>, <span style=\"color:pink\">validation</span>, and <span style=\"color:green\">test set</span>.\n",
    "    \n",
    "<ol>\n",
    "    <li>For each feature subset, train predictor on <span style=\"color:red\">training data</span>.</li>\n",
    "    <li>\n",
    "        Select the feature subset, which performs best on <span style=\"color:pink\">validation data</span>.\n",
    "        <ul>\n",
    "            <li>Repeat and average if you want to reduce variance (cross-validation).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bilevel optimization;\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "    <img src=\"img/page 70_4.png\" width=\"400\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "    Split data into 3 sets: <span style=\"color:red\">training</span>, <span style=\"color:pink\">validation</span>, and <span style=\"color:green\">test set</span>.\n",
    "    \n",
    "<ol>\n",
    "    <li>For each feature subset, train predictor on <span style=\"color:red\">training data</span>.</li>\n",
    "    <li>\n",
    "        Select the feature subset, which performs best on <span style=\"color:pink\">validation data</span>.\n",
    "        <ul>\n",
    "            <li>Repeat and average if you want to reduce variance (cross-validation).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Test on <span style=\"color:green\">test data</span>.</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\textbf{Summary}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary: what did we learn?\n",
    "&nbsp;\n",
    "\n",
    "- The book: ML for text data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\text {Time for Practical 3!}$$ "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

---
title: "Text Clustering"
author: "Ayoub Bagheri, <a.bagheri@uu.nl>"
output:
  ioslides_presentation:
    logo: logo.png
    smaller: yes
    widescreen: no
---

```{r, include=FALSE}
library(knitr)
library(kableExtra)
```

## Lecture‚Äôs Plan

1. What is text clustering?
2. What are the applications?
3. How to cluster text data?

## Unsupervised learning

```{r, echo=FALSE, out.width="80%", fig.align='center'}
include_graphics("img/page 3.png")
```

## Clustering v.s. Classification

```{r, echo=FALSE, out.width="80%", fig.align='center'}
include_graphics("img/page 4.png")
```

## Clustering

- Discover ‚Äúnatural structure‚Äù of data
    - What is the criterion? 
    - How to identify them?
    - How to evaluate the results?
    
```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 5.png")
```

## Clustering

- Clustering - the process of grouping a set of objects into clusters of similar objects
    - Basic criteria
        - high intra-cluster similarity
        - low inter-cluster similarity
    - No (little) supervision signal about the underlying clustering structure
    - Need similarity/distance as guidance to form clusters

## Applications of text clustering

<div style="float:left;width:50%">
- Organize document collections
  - Automatically identify hierarchical/topical relation among documents
</div>
<div style="float:right;width:50%">
```{r, echo=FALSE, out.width="80%", fig.align='center'}
include_graphics("img/page 7.png")
```
</div>

## Applications of text clustering

<div style="float:left;width:50%">
- Grouping search results
  - Organize documents by topics
  - Facilitate user browsing
</div>
<div style="float:right;width:50%">
```{r, echo=FALSE, out.width="80%", fig.align='center'}
include_graphics("img/page 8.png")
```
</div>

## Applications of text clustering

- Topic modeling
    - Grouping words into topics

```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 9.png")
```

# Distance metric

## Distance metric

- Basic properties
  
  - Positive separation
$$ùê∑(x,y)>0, \forall x \neq y$$
$$ùê∑(x,y)=0, \mathrm{i.f.f.}, x=y$$

  - Symmetry 
$$ùê∑(x,y)=ùê∑(y,x)$$

  - Triangle inequality
$$ùê∑(x,y)‚â§ùê∑(x,z)+ùê∑(z,y)$$

## Typical distance metric

- Minkowski metric

  - $d(x,y) = \sqrt[p]{\sum^V_{i=1}{(x_i-y_i)^p}}$
    
    - When $p=2$, it is <span style="color:red">Euclidean distance</span>

- Cosine metric

  - $ùëë(x,y)=1‚àícosine(x,y)$

    - when $|x|^2=|y|^2=1$, $1‚àícosine(x,y)=\frac{r^2}{2}$

## Typical distance metric

- Edit distance
  
  - Count the minimum number of operations required to transform one string into the other
    
    - Possible operations: insertion, deletion and replacement

<div style="float:left; width:70%">
```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 13.png")
```
</div>
<div style="float:right; width:30%">
  <br> <br> <br> <br> 
  <p style="color:red">&larr; Can be efficiently solved by dynamic programming
   </p>
</div>

## Typical distance metric

- Edit distance

  - Count the minimum number of operations required to transform one string into the other
    
    - Possible operations: insertion, deletion and replacement
  
  - Extent to distance between sentences
    
    - Word similarity as cost of replacement
      
      - ‚Äúterrible‚Äù -> ‚Äúbad‚Äù: low cost <span style="color:red">&rarr; Lexicon or distributional semantics</span>
      
      - ‚Äúterrible‚Äù -> ‚Äúterrific‚Äù: high cost <span style="color:red">&rarr; Lexicon or distributional semantics</span>
    
    - Preserving word order in distance computation

# Clustering algorithms

## Clustering algorithms

1. Partitional clustering algorithms
    - Partition the instances into different groups
    - Flat structure
        - Need to specify the number of classes in advance
        
## Clustering algorithms

<ol start="2">
  <li>Hierarchical clustering algorithms</li>
  <ul>
    <li>Create a hierarchical decomposition of objects</li>
    <li>Rich internal structure</li>
    <ul>
      <li>No need to specify the number of clusters</li>
      <li>Can be used to organize objects</li>
    </ul>
  </ul>
</ol>

## Clustering algorithms

<ol start="3">
  <li>Topic modeling</li>
  <ul>
    <li>Topic models are a suite of algorithms that uncover the hidden thematic structure in document collections. These algorithms help us develop new ways to search, browse and summarize large archives of texts.</li>
    <li>We want to find themes (or topics) in documents</li>
    <li>We don‚Äôt want to do supervised topic classification ‚Äì rather not fix topics in advance nor do manual annotation </li>
    <li>Need an approach which automatically teases out the topics</li>
    <li>This is essentially a clustering problem - can think of both words and documents as being clustered</li>
  </ul>
</ol>

## Hard vs. soft clustering

- Hard clustering: Each document belongs to exactly one cluster
    - More common and easier to do
- Soft clustering: A document can belong to more than one cluster.
    - Makes more sense for applications like creating browsable hierarchies
    - You may want to put a pair of sneakers in two clusters: (i) sports apparel and (ii) shoes
    - You can only do that with a soft clustering approach.

# Partitioning clustering

## Partitioning Algorithms

- Partitioning method: Construct a partition of <span style="color:blue">$n$</span> documents into a set of <span style="color:blue">$K$</span> clusters
- Given: a set of documents and the number <span style="color:blue">$K$</span> 
- Find: a partition of <span style="color:blue">$K$</span> clusters that optimizes the chosen partitioning criterion

  - Globally optimal
    - Intractable for many objective functions
    - Ergo, exhaustively enumerate all partitions
  
  - Effective heuristic methods: <em>K</em>-means and <em>K</em>-medoids algorithms

## Partitioning Algorithms

- Typical partitional clustering algorithms
  
  - <em>k</em>-means clustering
    
    - Partition data by its closest mean
    
```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 22.png")
```

## Partitioning Algorithms

<div style="float:left;width:70%">

<ul>
    <li>Typical partitional clustering algorithms</li>
    <ul>
        <li><em>k</em>-means clustering</li>
        <ul><li>Partition data by its closest mean</li></ul>
        <li>Gaussian Mixture Model</li>
        <ul><li>Consider variance within the cluster as well</li></ul>

</div>
<div style="float:right;width:30%">
```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 23.png")
```
</div>

## <em>K</em>-Means

- Assumes documents are real-valued vectors.

- Clusters based on <span style="font-style:italic;color:red">centroids</span> (aka the <span style="font-style:italic;color:red">center of gravity</span> or mean) of points in a cluster, $c$:

$$\vec \mu(c)=\frac{1}{|c|}\sum_{\vec a \in c}{\vec x}$$

- Reassignment of instances to clusters is based on distance to the current cluster centroids.
  
  - (Or one can equivalently phrase it in terms of similarities)

## K-Means Algorithm

- Select $K$ random docs $\{s_1, s_2,‚Ä¶ s_K\}$ as seeds.
- Until clustering converges (or other stopping criterion):
  
  - For each doc $d_i$: <br>
   
    - Assign $d_i$ to the cluster $c_j$ such that $dist(x_i, s_j)$ is minimal.
  
  - <span style="color:DarkCyan">(Next, update the seeds to the centroid of each cluster)</span>
  
  - For each cluster $c_j$
    - $s_j = \mu(c_j)$

## K Means Example (K=2)

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 26_1.png")
```

## K Means Example (K=2)

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 26_2.png")
```

## K Means Example (K=2)

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 26_3.png")
```

## K Means Example (K=2)

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 26_4.png")
```

## K Means Example (K=2)

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 26_5.png")
```

## K Means Example (K=2)

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 26_6.png")
```

## K Means Example (K=2)

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 26_7.png")
```

## K Means Example (K=2)

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 26_8.png")
```

## Termination conditions

- Several possibilities, e.g.,
  
  - A fixed number of iterations.

  - Doc partition unchanged.

  - Centroid positions don‚Äôt change.

## Convergence

- Why should the <em>K</em>-means algorithm ever reach a <em>fixed point</em>?

  - A state in which clusters don‚Äôt change.

- <em>K</em>-means is a special case of a general procedure known as the <em>Expectation Maximization (EM) algorithm</em>.

  - EM is known to converge.

  - Number of iterations could be large.
    
    - But in practice usually isn‚Äôt

## Seed Choice

- Results can vary based on random seed selection.

- Some seeds can result in poor convergence rate, or convergence to sub-optimal clusterings.
  
  - Select good seeds using a heuristic (e.g., doc least similar to any existing mean)
  
  - <span style="color:blue">Try out multiple starting points</em>
  
  - Initialize with the results of another method.

## <em>K</em>-means issues, variations, etc.

- Recomputing the centroid after every assignment (rather than after all points are re-assigned) can improve speed of convergence of <em>K</em>-means

- Assumes clusters are spherical in vector space
  
  - Sensitive to coordinate changes, weighting etc. 

- Disjoint and exhaustive
  
  - Doesn‚Äôt have a notion of ‚Äúoutliers‚Äù by default
  
  - But can add outlier filtering
  
## MBK

## DBScan

# Hierarchical clustering

## Dendrogram: Hierarchical Clustering

- Build a tree-based hierarchical taxonomy (<em>dendrogram</em>) from a set of documents.

```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 34.png")
```

- One approach: recursive application of a partitional clustering algorithm.

## Dendrogram: Hierarchical Clustering

<div style="float:left;width:50%">
- Clustering obtained by cutting the dendrogram at a desired level: each <b>connected</b> component forms a cluster.
</div>
<div style="float:right;width:50%">
```{r, echo=FALSE, out.width="90%", fig.align='center'}
include_graphics("img/page 35.png")
```
</div>

## Clustering algorithms

- Typical hierarchical clustering algorithms
  
  - Bottom-up agglomerative clustering
    
    - Start with individual objects as separated clusters
    - Repeatedly merge closest pair of clusters   
    
```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 36.png")
```

## Clustering algorithms

- Typical hierarchical clustering algorithms
  
  - Top-down divisive clustering
    
    - Start with all data as one cluster
    
    - Repeatedly splitting the remaining clusters into two

```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 37.png")
```

## Hierarchical Agglomerative Clustering (HAC)

- Starts with each doc in a separate cluster
  
  - then repeatedly joins the <em><u>closest pair</u></em> of clusters, until there is only one cluster.

- The history of merging forms a binary tree or hierarchy.

## Closest pair of clusters

- Many variants to defining closest pair of clusters
  
  - <b>Single-link</b>
    - Similarity of the <em>most</em> cosine-similar (single-link)

  - <b>Complete-link</b>
    - Similarity of the ‚Äúfurthest‚Äù points, the <em>least</em> cosine-similar

  - <b>Centroid</b>
    - Clusters whose centroids (centers of gravity) are the most cosine-similar

  - <b>Average-link</b>
    - Average cosine between pairs of elements

## Single Link Agglomerative Clustering

- Use maximum similarity of pairs:

$$sim(c_i, c_j) = \max_{x \in c_i, y\in c_j}{sim(x,y)}$$

- Can result in ‚Äústraggly‚Äù (long and thin) clusters due to chaining effect.

- After merging $c_i$ and $c_j$, the similarity of the resulting cluster to another cluster, $c_k$, is:

$$sim((c_i \cup c_j), c_k) = max(sim(c_i, c_k), sim(c_j, c_k))$$
\newpage

## Complete Link
&nbsp;

- Use minimum similarity of pairs:

$$sim(c_i, c_j) = \min_{x \in c_i, y\in c_j}{sim(x,y)}$$

- Makes ‚Äútighter,‚Äù spherical clusters that are typically preferable.

- After merging $c_i$ and $c_j$, the similarity of the resulting cluster to another cluster, $c_k$, is:

$$sim((c_i \cup c_j), c_k) = min(sim(c_i, c_k), sim(c_j, c_k))$$

```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 41.png")
```

# Matrix Factorization

## Dimensionality Reduction and Matrix Factorization

$$D \approx UV^T$$

Maximize similarity between entries of $D$ and $UV^T$

&nbsp; &nbsp; &nbsp; subject to:
    
&nbsp; &nbsp; &nbsp; Constraints on $U$ and $V$

## Singular Value Decomposition

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 44.png")
```

## Example SVD

lexicon: lion, tiger, cheetah, jaguar, porsche, ferrari

$$
D = \begin{pmatrix}
    & \text{lion} & \text{tiger} & \text{cheetah} & \text{jaguar} & \text{porsche} & \text{ferrari} \\
   \text{Document-1} & 2 & 2 & 1 & 2 & 0 & 0 \\
   \text{Document-2} & 2 & 3 & 3 & 3 & 0 & 0 \\
   \text{Document-3} & 1 & 1 & 1 & 1 & 0 & 0 \\
   \text{Document-4} & 2 & 2 & 2 & 3 & 1 & 1 \\
   \text{Document-5} & 0 & 0 & 0 & 1 & 1 & 1 \\
   \text{Document-6} & 0 & 0 & 0 & 2 & 1 & 2
  \end{pmatrix}
$$

## Example SVD 

<p style="font-size:16px">
$$
\begin{align}
    D &\approx Q\Sigma P^T \\
    &\approx \begin{pmatrix}
                -0.41 & 0.17 \\
                -0.65 & 0.31 \\
                -0.23 & 0.13 \\
                -0.56 & -0.20 \\
                -0.10 & -0.46 \\
                -0.19 & -0.78
             \end{pmatrix}
             \begin{pmatrix}
                8.4 & 0 \\
                0 & 3.3
             \end{pmatrix}
             \begin{pmatrix}
                -0.41 & -0.49 & -0.44 & -0.61 & -0.10 & -0.12 \\
                0.21 & 0.31 & 0.26 & -0.37 & -0.44 & -0.68
             \end{pmatrix} \\
    &= \begin{pmatrix}
           1.55 & 1.87 & 1.67 & 1.91 & 0.10 & 0.04 \\
           2.46 & 2.98 & 2.66 & 2.95 & 0.10 & -0.03 \\
           0.89 & 1.08 & 0.96 & 1.04 & 0.01 & -0.04 \\
           1.81 & 2.11 & 1.91 & 3.14 & 0.77 & 1.03 \\
           0.02 & -0.05 & -0.02 & 1.06 & 0.74 & 1.11 \\
           0.10 & -0.02 & 0.04 & 1.89 & 1.28 & 1.92
       \end{pmatrix}
\end{align}
$$
</p>

\newpage

## Advantages and Disadvantages of SVD/LSA

1. The orthogonal basis representation of SVD is useful for folding in the reduced representation of new documents not included in the data matrix $D$. For example, if $\vec X$ is a row vector of a new document, then its reduced representation is givenn by the *k*-dimensional vector $\vec XV$. This type of out-of-sample embedding is harder (albeit possible) with other forms of matrix factorization.

2. The SVD solution provides the same error as <em>unconstrained</em> matrix factorization problem. Since most other forms of dimensionality reductiona are constrained matrix factorization problems, one can typically achieve a lower residual error with SVD at the same value of the rank $k$.

## Advantages and Disadvantages of SVD/LSA

<ol start="3">
    <li>The topics of a text collection are often highly overlapping in terms of their vocabulary. As a result, the directions represented by the various topics are naturally not orthogonal, which matches poorly with orthogonal basis vectors. SVD does a poor job at revealing the actual semantic topics (or <em>clusters</em>) in the underlying data. Most forms of nonnegative matrix factorization that do not use orthogonal basis vectors are more adept at representing the clustering structure in the underlying data.</li>
    <li>The representation provided by SVD is not very interpretable and it is hard to match with the semantic concepts in the collection. A key part of the problem is that the eigenvectors contain both positive and negative components that are hard to interpret.</li>
</ol>   

## Nonnegative Matrix Factorization

**lexicon: lion, tiger, cheetah, jaguar, porsche, ferrari**
&nbsp;

**Cats**: lion, tiger, cheetah, jaguar

**Cars**: japuar, porche, ferrari

$$
D = \begin{pmatrix}
    & \text{lion} & \text{tiger} & \text{cheetah} & \text{jaguar} & \text{porsche} & \text{ferrari} \\
   \text{Document-1} & 2 & 2 & 1 & 2 & 0 & 0 \\
   \text{Document-2} & 2 & 3 & 3 & 3 & 0 & 0 \\
   \text{Document-3} & 1 & 1 & 1 & 1 & 0 & 0 \\
   \text{Document-4} & 2 & 2 & 2 & 3 & 1 & 1 \\
   \text{Document-5} & 0 & 0 & 0 & 1 & 1 & 1 \\
   \text{Document-6} & 0 & 0 & 0 & 2 & 1 & 2
  \end{pmatrix}
$$

## Nonnegative Matrix Factorization

```{r, echo=FALSE, out.width="80%", fig.align='center'}
include_graphics("img/page 48.png")
```

## Nonnegative Matrix Factorization

```{r, echo=FALSE, out.width="80%", fig.align='center'}
include_graphics("img/page 49.png")
```

## Advantages and Disadvantages of Nonnegative Matrix Factorization

1. Nonnegativity enables a highly interpretable decomposition because of ability to represent the factorization as a sum of parts.
2. The semantic clusters (or topics) are often captured more accurately by allowing non-orthogonality in the basis vectors. This is because semantic topics are often related.
3. Nonnegative matrix factorization can better address polysemy than SVD.
4. One disadvantage of nonnegative matrix factorization is that it is harder (than SVD) to compute the reduced representations of documents that were not included in the original data matrix $D$. SVD is able to fold in such documents more easily as a simple projection because of its orthogonal basis system.

## Probabilistic Latent Semantic Analysis

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 51.png")
```

## Probabilistic Latent Semantic Analysis

$$
\begin{align}
    \text{Maximize}_{(P,Q,\Sigma)}&[\text{Log likelihood of generating }D \text{ using parameters in matrices }(P,Q,\Sigma)] \\
    &= log(\prod_{i,j}{P(\text{Adding one occurnce of term }j \text{ in document }i)^{D_{ij}}}) \\
    &= \sum_{i=1}^n\sum_{j=1}^d{D_{ij}} \begin{matrix} \underbrace{ log(P(\vec X_i, t_j)) } \\ \text{Parametrized by }P,Q,\Sigma \end{matrix} \\
    &\text{subject to:} \\
    &P,Q,\Sigma \geq 0 \\
    &\text{Entries in each column of } P \text{ sum to 1} \\
    &\text{Entries in each column of } Q \text{ sum to 1} \\
    &\Sigma \text{ is a diagonal matrix that sums to 1}
\end{align}
$$

```{r, echo=FALSE, out.width="60%", fig.align='center'}
include_graphics("img/page 52.png")
```

## Comparison with SVD

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 53.png")
```


## Example of PLSA
 
```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 54.png")
```

# Topic Modeling

## Topic models

<div style="float:left;width:60%">
<ul>
    <li>Three concepts: words, topics, and documents</li>
    <li>Documents are a collection of words and have a probability distribution over topics</li>
    <li>Topics have a probability distribution over words</li>
    <li>Model:</li>
    <ul><li>Topics made up of words used to generate documents</li></ul>
</div>
<div style="float:right;width:40%">
```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 56.png")
```
</div>

## Topic models (lda or gensim) | Reality: Documents observed, infer  topics

```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 57.png")
```

## Probabilistic modeling

1. Treat data as observations that arise from a generative probabilistic process that includes hidden variables: For documents, the hidden variables reflect the thematic structure of the collection.

2. Infer the hidden structure using posterior inference: What are the topics that describe this collection?

3. Situate new data into the estimated model: How does this query or new document fit into the estimated topic structure?

## 

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 59.png")
```

## Intro to Latent Dirichlet Allocation (LDA)

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 60.png")
```

## 

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 61.png")
```

## 

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 62.png")
```

## 

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 63.png")
```

## 

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 64.png")
```

## 

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 65.png")
```

## 

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 66.png")
```

## 

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 67.png")
```

## 

```{r, echo=FALSE, out.width="100%", fig.align='center'}
include_graphics("img/page 68.png")
```

# Cluster Validation

## Desirable properties of clustering algorithms 

- Scalability
  
  - Both in time and space

- Ability to deal with various types of data
  
  - No/less assumption about input data
  
  - Minimal requirement about domain knowledge

- Interpretability and usability

## Cluster validation

- Criteria to determine whether the clusters are meaningful
  
  - Internal validation
    
    - Stability and coherence
  
  - External validation
    
    - Match with known categories

## Internal validation

- Coherence

  - Inter-cluster similarity v.s. intra-cluster similarity
  
  - Davies‚ÄìBouldin index
    
    - $DB = \frac{1}{k}\sum_{i=1}^k{\underset{j \neq i}{\operatorname{max}}{(\frac{\sigma_i + \sigma_j}{d(c_i,c_j)})}}$  <span style="color:red">&larr;</span> <em>Evaluate every pair of clusters</em>

      - where $k$ is total number of clusters, $\sigma_i$ is average distance of all elements in cluster $i$, $d(c_i, c_j)$ is the distance between cluster centroid $c_i$ and $c_j$.

<p style="color:red">We prefer smaller DB-index!</p>

## Internal validation

- Coherence

  - Inter-cluster similarity v.s. intra-cluster similarity
  
  - Davies‚ÄìBouldin index
    
    - $DB = \frac{1}{k}\sum_{i=1}^k{\underset{j \neq i}{\operatorname{max}}{(\frac{\sigma_i + \sigma_j}{d(c_i,c_j)})}}$  <span style="color:red">&larr;</span> <em>Evaluate every pair of clusters</em>

      - where $k$ is total number of clusters, $\sigma_i$ is average distance of all elements in cluster $i$, $d(c_i, c_j)$ is the distance between cluster centroid $c_i$ and $c_j$.

<p style="color:red">We prefer smaller DB-index!</p>

## External validation
&nbsp;

- Given class label $\Omega$ (<span style="color:blue; font-size: 18px">Required, might need extra cost</span>) on each instance

  - Purity: correctly clustered documents in each cluster
    
    - $purity(\Omega,C) = \frac{1}{N}\sum_{i=1}^k{\underset{j}{\operatorname{max}}|c_i \cap w_j|}$   <span style="color:red; font-size:18px">&larr; Not a good metric if we assign each document into a single cluster</span>
      - where $c_i$ is a set of documents in cluster $i$, and $w_j$ is a set of documents in class $j$
      
<div style="float: left; width:30%">
$$purity(\Omega, C) = \\ \frac{1}{17}(5 + 4 + 3)$$
</div>
<div style="float: right; width:70%">
```{r, echo=FALSE, out.width="80%", fig.align='center'}
include_graphics("img/page 74.png")
```
</div>

## External validation

- Given class label Œ© on each instance
  
  - Normalized mutual information (NMI)
    
    - $NMI(\Omega, C) = \frac{I(\Omega, C)}{[H(\Omega)+H(C)]/2}$ <span style="color:red; font-size:18px">&swarr; Normalization by entropy will penalize too many clusters</span>
      - where $I(\Omega, C) = \sum_i \sum_j P(w_i \cap c_j)log{\frac{P(w_i \cap c_j)}{P(w_i)P(c_j)}} \\ H(\Omega) = - \sum_i{P(w_i) logP(w_i)} \ \ \ \mathrm{and} \ \ \ H(C) = - \sum_j{P(c_j)logP(c_j)}$ 
    
    - Indicate the increase of knowledge about classes when we know the clustering results

## External validation

- Given class label $\Omega$ on each instance

  - Rand index
    
    - Idea: we want to assign two documents to the same cluster if and only if they are from the same class
    
    - $RI = \frac{TP + TN}{TP + FP + FN + TN}$ <span style="color:red;font-size:18px">&larr; Essentially it is like classification accuracy</span>
    
```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 76.png")
```

## External validation

- Given class label $\Omega$ on each instance
  - Rand index
    
```{r, echo=FALSE, out.width="80%", fig.align='center'}
include_graphics("img/page 77.png")
```

## External validation

- Given class label $\Omega$ on each instance
  
  - Precision/Recall/F-measure
     
     - Based on the contingency table, we can also define precision/recall/F-measure of clustering quality
     
```{r, echo=FALSE, out.width="80%", fig.align='center'}
include_graphics("img/page 78.png")
```

## Group Average

- Similarity of two clusters = average similarity of all pairs within merged cluster.

$$sim(c_i, c_j) = \frac{1}{|c_i \cup c_j|(|c_i \cup c_j| - 1)}\sum_{\vec x \in (c_i \cup c_j)} \sum_{\vec y \in (c_i \cup c_j): \vec y \neq \vec x}{sim(\vec x, \vec y)}$$

- Compromise between single and complete link.

- Two options:
 
  - Averaged across all ordered pairs in the merged cluster 
 
  - Averaged over all pairs between the two original clusters

- No clear difference in efficacy

## Computing Group Average Similarity

- Always maintain sum of vectors in each cluster.

$$\vec s(c_j) = \sum_{\vec x \in c_j}{\vec x}$$
- Compute similarity of clusters in constant time:

$$sim(c_i, c_j) = \frac{\vec s(c_i) + \vec s(c_j) \cdot (\vec s(c_j) + \vec s(c_j)) - (|c_i| + |c_j|)}{(|c_i| + |c_j|)(|c_i| + |c_j| - 1)}$$

## What Is A Good Clustering?

- Internal criterion: A good clustering will produce high quality clusters in which:

  - the <u>intra-class</u> (that is, intra-cluster) similarity is high
  
  - the <u>inter-class</u> similarity is low

  - The measured quality of a clustering depends on both the document representation and the similarity measure used
  
## External criteria for clustering quality

- Quality measured by its ability to discover some or all of the hidden patterns or latent classes in gold standard data

- Assesses a clustering with respect to <u>ground truth</u> ‚Ä¶ requires <span style="color:green; font-style:italic">labeled data</span>

- Assume documents with $C$ gold standard classes, while our clustering algorithms produce $K$ clusters, $\omega_1$, $\omega_2$, ‚Ä¶, $\omega_K$  with $n_i$ members.

## External Evaluation of Cluster Quality

- Simple measure: <u>purity</u>, the ratio between the dominant class in the cluster $\pi_i$ and the size of cluster $\omega_i$

$$Purity(\omega_i) = \frac{1}{n_i}\max_j(n_{ij}) \ \ \ j \in C$$

- Biased because having n clusters maximizes purity

- Others are entropy of classes in clusters (or mutual information between classes and clusters)

## Rand index and Cluster F-measure

$$RI = \frac{A + D}{A + B + C + D}$$
<center style="color:darkred">Compare with standard Precision and Recall:</center>

$$P= \frac{A}{A + B} \ \ \ \ \ \ P = \frac{A}{A + C}$$
<center style="color:darkred">People also define and use a cluster F-measure, which is probably a better measure</center>

# Summary

## Summary: what did we learn?

- Text clustering

- In clustering, clusters are inferred from the data without human input (unsupervised learning)

- However, in practice, it‚Äôs a bit less clear: there are many ways of influencing the outcome of clustering: number of clusters, similarity measure, representation of documents

- Evaluation

# Time for Practical 4!









{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Utrecht Summer School: Applied Text Mining  \n",
    "&nbsp;\n",
    "\n",
    "\n",
    "Ayoub Bagheri, <a.bagheri@uu.nl>  \n",
    "Berit Janssen, <b.d.janssen@uu.nl>  \n",
    "Dong Nguyen, <d.p.nguyen@uu.nl>  \n",
    "\n",
    "<img src=\"img/uu_logo.png\" style=\"float: right;\" width=\"100\" height=\"100\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text mining in an example\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float: left; width: 20%;\">\n",
    "<img src=\"img/Garry.png\" style=\"float: right;\" width=\"80\" height=\"80\">\n",
    "</div>\n",
    "\n",
    "<div style=\"float: right; width: 80%;\">\n",
    "<ul>\n",
    "    <li><b>Garry</b> works at <span style=\"color:blue\">Bol.com</span> (a webshop in the Netherlands)</li>\n",
    "    <li>He works in the dep of <b>Customer relationship management</b></li>\n",
    "    <li>He reads <span style=\"color:blue\">customers’ reviews</span> (comments), extracts <span style=\"color:blue\">aspects</span> they wrote their reviews on, and identifies their <span style=\"color:blue\">sentiments</span>.</li>\n",
    "    <li>Curious about his job? See two examples!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/littleprince.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float: left; with: 20%\">\n",
    "<center><img src=\"img/Garry.png\" style=\"float:center\" width=\"100\" height=\"100\"></center>\n",
    "</div>\n",
    "\n",
    "<div style=\"float: right; width: 80%\">\n",
    "    <ul>\n",
    "        <li>Garry likes his job a lot, but sometimes it is frustrating!</li>\n",
    "        <li>This is mainly because their company is expanding quickly!</li>\n",
    "        <li>Garry decides to hire <b>Larry</b> as his assistant.</li>\n",
    "    </ul>\n",
    "\n",
    "<img src=\"img/page4_person2.png\" style=\"float: right\" width=\"200\" height=\"240\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float: left; width: 20%\">\n",
    "\n",
    "<center><img src=\"img/Garry.png\" style=\"float:center\" width=\"70\" height=\"70\"></center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center><img src=\"img/page4_person2.png\" style=\"float:center\" width=\"170\" height=\"190\"></center>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"float: right; width: 80%\">\n",
    "\n",
    "<ul>\n",
    "<li>Still, a lot to do for two people!</li>\n",
    "\n",
    "<li>Garry has some budget left to hire another assistant for couple of years!</li>\n",
    "\n",
    "<li>He decides to hire <b>Harry</b> too!</li>\n",
    "\n",
    "<li>Still, manual labeling is labor-intensive!</li>\n",
    "</ul>\n",
    "    \n",
    "<center><img src=\"img/page5_person3.png\" style=\"float:center\" width=\"150\" height=\"170\"></center>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges?\n",
    "&nbsp;\n",
    "\n",
    "- What are the challenges Garry, Larry, and Harry encounter in doing their job, when working with text data?\n",
    "\n",
    "  - Go to <a href=\"www.menti.com\">www.menti.com</a> and use the code 22 07 62 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges with text data\n",
    "&nbsp;\n",
    "\n",
    "- Huge amount of data\n",
    "\n",
    "- High dimensional but sparse\n",
    "\n",
    "  - all possible word and phrase types in the language!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges with text data\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float: left; width: 60%\">\n",
    "\n",
    "<ul><li>Ambiguity</li></ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"float: left; width: 40%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Challenges with text data\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float: left; width: 60%\">\n",
    "\n",
    "<ul><li>Ambiguity</li></ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"float: left; width: 40%\">\n",
    "\n",
    "<img src=\"img/page 8.png\" width=\"150\" height=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges with text data\n",
    "&nbsp;\n",
    "\n",
    "- Noisy data\n",
    "  \n",
    "  - Examples: Abbreviations, spelling errors, short text\n",
    "\n",
    "- Complex relationships between words\n",
    "  \n",
    "  - “Hema merges with Intertoys”\n",
    "  \n",
    "  - “Intertoys is bought by Hema”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Back to story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float: left; width: 20%\">\n",
    "\n",
    "<img src=\"img/Garry.png\" width=\"100\" height=\"100\">\n",
    "\n",
    "<img src=\"img/page 11.png\" width=\"150\" height=\"150\">\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"float: right; width: 80%\">\n",
    "  \n",
    "<ul>\n",
    "<li>During one of the coffee moments at the company, <b>Garry</b> was talking about their situation at the dep of Customer relationship management.</li>\n",
    "\n",
    "<li>When <b>Carrie</b>, her colleague from the <b>IT department</b>, hears the situation, she offers Garry to use Text Mining!!</li>\n",
    "\n",
    "<li>She says: “<span style=\"color:blue\">Text mining is your friend; it can help you to make the faster by filtering and recommending possible words…</span>”</li>\n",
    "\n",
    "<li>She continues : “Text mining is a subfield of AI and NLP and is related to data science, data mining and machine learning. It will make the process faster and cuts some of the expenses!”</li>\n",
    "\n",
    "<li>After consulting with Larry and Harry, They decide to give text mining a try!</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text mining definition?\n",
    "&nbsp;\n",
    "\n",
    "- Which can be a part of Text Mining definition?\n",
    "  - The discovery by computer of new, previously unknown information from textual data\n",
    "  - Automatically extracting information from text\n",
    "  - Text mining is about looking for patterns in text\n",
    "  - Text mining describes a set of techniques that model and structure the information content of textual sources\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"color:green\">(You can choose multiple choices)</span>\n",
    "\n",
    "Go to <a href=\"www.menti.com\">www.menti.com</a> and use the code 22 07 62 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Access\n",
    "&nbsp;\n",
    "\n",
    "- You can view my screen on your machine:\n",
    "  \n",
    "  - <a href=\"join.me/TMPython\">join.me/TMPython</a>\n",
    "\n",
    "- Access the course materials from:\n",
    "\n",
    "  - <a href=\"www.ayoubbagheri.nl/TMPython\">www.ayoubbagheri.nl/TMPython</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Program\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 17.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goal of the course\n",
    "&nbsp;\n",
    "\n",
    "- The course teaches students the basic and advanced text mining techniques using Python on a variety of applications in many domains of science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python?\n",
    "&nbsp;\n",
    "\n",
    "- From 1 to 5 how familiar are you with Python?\n",
    "\n",
    "    - Go to <a hre=\"www.menti.com\">www.menti.com</a> and use the code 81 09 57 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python IDE?!\n",
    "&nbsp;\n",
    "\n",
    "- Which Python IDE do you mostly use?\n",
    "    - Go to <a href=\"www.menti.com\">www.menti.com</a> and use the code 81 09 57 8\n",
    "\n",
    "- From 1 to 10 how familiar are you with Google Colab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python\n",
    "&nbsp;\n",
    "\n",
    "- Latest: <a href=\"https://www.python.org/downloads/release/python-391/\">Python 3.9.1</a>\n",
    "\n",
    "- Python For Beginners\n",
    "    - <a href=\"https://www.python.org/about/gettingstarted/\">https://www.python.org/about/gettingstarted/</a>\n",
    "\n",
    "- The Python Language Reference\n",
    "    - <a href=\"https://docs.python.org/3/reference/\">https://docs.python.org/3/reference/</a>\n",
    "\n",
    "- Python 3.9.1 documentation\n",
    "    - <a href=\"https://docs.python.org/3/\">https://docs.python.org/3/</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Google Colab\n",
    "&nbsp;\n",
    "\n",
    "- Colaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser, with\n",
    "    - Zero configuration required\n",
    "    - Free access to GPUs\n",
    "    - Easy sharing\n",
    "\n",
    "- <a href=\"https://colab.research.google.com/notebooks/intro.ipynb\">https://colab.research.google.com/notebooks/intro.ipynb</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Google Colab?!\n",
    "&nbsp;\n",
    "\n",
    "- From 1 to 5 how familiar are you with Google Colab?\n",
    "    - Go to <a href=\"www.menti.com\">www.menti.com</a> and use the code 81 09 57 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python: Quick & Easy to Learn!\n",
    "&nbsp;\n",
    "\n",
    "Code in Colab\n",
    "\n",
    "<img src=\"img/page 24.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python: Quick & Easy to Learn!\n",
    "&nbsp;\n",
    "\n",
    "Code in Colab\n",
    "\n",
    "<img src=\"img/page 24.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Do you have any questions?\n",
    "&nbsp;\n",
    "\n",
    "- **During the lecture**\n",
    "\n",
    "  - Post your question to the chat; we will read them during a break.\n",
    "\n",
    "- **During the computer lab**\n",
    "\n",
    "  - Post your question in general; we will answer them.\n",
    "\n",
    "- **After the lecture**\n",
    "\n",
    "  - Feel free to send me an email or text me in Microsoft Teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction! (continue)\n",
    "&nbsp;\n",
    "\n",
    "Ayoub Bagheri, a.bagheri@uu.nl <br>\n",
    "Utrecht Summer School: Applied Text Mining\n",
    "\n",
    "<img src=\"img/uu_logo.png\" style=\"float: right;\" width=\"100\" height=\"100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another TM definition\n",
    "\n",
    "<img src=\"img/page 28.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text mining process\n",
    "\n",
    "<img src=\"img/page 29.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/page 30.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text mining tasks\n",
    "&nbsp;\n",
    "\n",
    "- Text classification\n",
    "- Text clustering\n",
    "\n",
    "We will also cover:\n",
    "    - Sentiment analysis\n",
    "    - Feature selection\n",
    "    - Topic modelling\n",
    "    - Word embedding\n",
    "    - Deep learning models\n",
    "    - Responsible text mining\n",
    "    - Text summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text classification\n",
    "&nbsp;\n",
    "\n",
    "- Supervised learning\n",
    "- Human experts annotate a set of text data\n",
    "    - Training set\n",
    "- Learn a classification model\n",
    "\n",
    "<img src=\"img/page 32.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text classification?\n",
    "&nbsp;\n",
    "\n",
    "- Which problem is not a text classification task? (less likely to be)\n",
    "    - Author's gender detection from text\n",
    "    - Finding about the smoking conditions of patients from clinical letters\n",
    "    - Grouping news articles into political vs non-political news\n",
    "    - Classifying reviews into positive and negative sentiment\n",
    "    \n",
    "Go to <a href=\"www.menti.com\">www.menti.com</a> and use the code 86 08 86 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text clustering\n",
    "\n",
    "- Unsupervised learning\n",
    "- Finding Groups of Similar Documents\n",
    "- No labeled data\n",
    "\n",
    "<img src=\"img/page 34.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text clustering?\n",
    "&nbsp;\n",
    "\n",
    "- Which problem is not a text clustering task? (less likely to be)\n",
    "    - Grouping similar news articles\n",
    "    - Grouping discharge letters in two categories: heart disease vs cancer\n",
    "    - Grouping tweets which support Trump into three undefined subgroups\n",
    "    - Grouping online books of a library in 10 categories\n",
    "\n",
    "\n",
    "Go to <a href=\"www.menti.com\">www.menti.com</a> and use the code 86 08 86 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector space model\n",
    "&nbsp;\n",
    "\n",
    "- Represent documents by concept vectors\n",
    "    - Each concept defines one dimension\n",
    "    - $k$ concepts define a high-dimensional space\n",
    "    - Element of vector corresponds to concept weight\n",
    "        - E.g., $d=(x_1,…,x_k)$, $x_i$ is “importance” of concept $i$ in $d$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vector space model\n",
    "&nbsp;\n",
    "\n",
    "- Represent documents by concept vectors\n",
    "    - Each concept defines one dimension\n",
    "    - $k$ concepts define a high-dimensional space\n",
    "    - Element of vector corresponds to concept weight\n",
    "        - E.g., $d=(x_1,…,x_k)$, $x_i$ is “importance” of concept $i$ in $d$\n",
    "\n",
    "- Distance between the vectors in this concept space\n",
    "    - Relationship among documents  \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An illustration of VS model\n",
    "&nbsp;\n",
    "\n",
    "- All documents are projected into this concept space\n",
    "\n",
    "<img src=\"img/page 38.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## An illustration of VS model\n",
    "&nbsp;\n",
    "\n",
    "- All documents are projected into this concept space\n",
    "\n",
    "<img src=\"img/page 38_1.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag-of-Words representation\n",
    "\n",
    "- Term as the basis for vector space\n",
    "\n",
    "<img src=\"img/page 39.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization\n",
    "&nbsp;\n",
    "\n",
    "- Break a stream of text into meaningful units\n",
    "    - Tokens: words, phrases, symbols\n",
    "        - Input: It’s not straight-forward to perform so-called “tokenization.”  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenization\n",
    "&nbsp;\n",
    "\n",
    "- Break a stream of text into meaningful units\n",
    "    - Tokens: words, phrases, symbols\n",
    "        - Input: It’s not straight-forward to perform so-called “tokenization.”  \n",
    "        - Output(1): 'It’s', 'not', 'straight-forward', 'to', 'perform', 'so-called', '“tokenization.”' \n",
    "        - Output(2): 'It', '’', 's', 'not', 'straight', '-', 'forward, 'to', 'perform', 'so', '-', 'called', ‘“', 'tokenization', '.', '”‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenization\n",
    "&nbsp;\n",
    "\n",
    "- Break a stream of text into meaningful units\n",
    "    - Tokens: words, phrases, symbols\n",
    "        - Input: It’s not straight-forward to perform so-called “tokenization.”  \n",
    "        - Output(1): 'It’s', 'not', 'straight-forward', 'to', 'perform', 'so-called', '“tokenization.”' \n",
    "        - Output(2): 'It', '’', 's', 'not', 'straight', '-', 'forward, 'to', 'perform', 'so', '-', 'called', ‘“', 'tokenization', '.', '”‘\n",
    "    - Definition depends on language, corpus, or even context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization\n",
    "&nbsp;\n",
    "\n",
    "- Solutions\n",
    "    - Regular expressions\n",
    "        - [\\w]+: so-called -> ‘so’, ‘called’\n",
    "        - [\\S]+: It’s -> ‘It’s’ instead of ‘It’, ‘’s’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenization\n",
    "&nbsp;\n",
    "\n",
    "- Solutions\n",
    "    - Regular expressions\n",
    "        - [\\w]+: so-called -> ‘so’, ‘called’\n",
    "        - [\\S]+: It’s -> ‘It’s’ instead of ‘It’, ‘’s’\n",
    "    - Statistical methods\n",
    "        - Explore rich features to decide where the boundary of a word is\n",
    "            - Apache OpenNLP (<a href=\"http://opennlp.apache.org/\">http://opennlp.apache.org/</a>)\n",
    "            - Stanford NLP Parser (<a href=\"http://nlp.stanford.edu/software/lex-parser.shtml\">http://nlp.stanford.edu/software/lex-parser.shtml</a>) \n",
    "        - Online Demo\n",
    "            - Stanford (<a href=\"http://nlp.stanford.edu:8080/parser/index.jsp\">http://nlp.stanford.edu:8080/parser/index.jsp</a>) \n",
    "            - UIUC (<a href=\"http://nlp.stanford.edu:8080/parser/index.jsp\">http://cogcomp.cs.illinois.edu/curator/demo/index.html</a>) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag-of-Words with N-grams\n",
    "&nbsp;\n",
    "\n",
    "- N-grams: a contiguous sequence of N tokens from a given piece of text\n",
    "    - E.g., *‘Text mining is to identify useful information.’*\n",
    "    - Bigrams: *‘text_mining’, ‘mining_is’, ‘is_to’, ‘to_identify’, ‘identify_useful’, ‘useful_information’, ‘information_.’*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bag-of-Words with N-grams\n",
    "&nbsp;\n",
    "\n",
    "- N-grams: a contiguous sequence of N tokens from a given piece of text\n",
    "    - E.g., *‘Text mining is to identify useful information.’*\n",
    "    - Bigrams: *‘text_mining’, ‘mining_is’, ‘is_to’, ‘to_identify’, ‘identify_useful’, ‘useful_information’, ‘information_.’*\n",
    "\n",
    "- Pros: capture local dependency and order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bag-of-Words with N-grams\n",
    "&nbsp;\n",
    "\n",
    "- N-grams: a contiguous sequence of N tokens from a given piece of text\n",
    "    - E.g., *‘Text mining is to identify useful information.’*\n",
    "    - Bigrams: *‘text_mining’, ‘mining_is’, ‘is_to’, ‘to_identify’, ‘identify_useful’, ‘useful_information’, ‘information_.’*\n",
    "\n",
    "- Pros: capture local dependency and order\n",
    "\n",
    "- Cons: a purely statistical view, increase the vocabulary size $O(V^N)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic document representation\n",
    "&nbsp;\n",
    "\n",
    "- Represent a document with all the occurring words\n",
    "    - Pros\n",
    "        - Preserve all information in the text (hopefully)\n",
    "        - Fully automatic\n",
    "    - Cons\n",
    "        - Vocabulary gap: cars v.s., car, talk v.s., talking\n",
    "        - Large storage: N-grams needs $𝑂(𝑉^𝑁)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Automatic document representation\n",
    "&nbsp;\n",
    "\n",
    "- Represent a document with all the occurring words\n",
    "    - Pros\n",
    "        - Preserve all information in the text (hopefully)\n",
    "        - Fully automatic\n",
    "    - Cons\n",
    "        - Vocabulary gap: cars v.s., car, talk v.s., talking\n",
    "        - Large storage: N-grams needs $𝑂(𝑉^𝑁)$\n",
    "    - Solution\n",
    "        - Construct controlled vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A statistical property of language\n",
    "&nbsp;\n",
    "\n",
    "- Zipf’s law\n",
    "    - Frequency of any word is inversely proportional to its rank in the frequency table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A statistical property of language\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 44_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A statistical property of language\n",
    "&nbsp;\n",
    "\n",
    "- Zipf’s law\n",
    "    - Frequency of any word is inversely proportional to its rank in the frequency table\n",
    "    - Formally\n",
    "        - $f(k;s,N)=\\frac{1/k^S}{\\sum_{n=1}^N{1/n^S}}$\n",
    "        \n",
    "        where $k$ is rank of the word; $N$ is the vocabulary size; $s$ is language-specific parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A statistical property of language\n",
    "&nbsp;\n",
    "\n",
    "- Zipf’s law\n",
    "    - Frequency of any word is inversely proportional to its rank in the frequency table\n",
    "    - Formally\n",
    "        - $f(k;s,N)=\\frac{1/k^S}{\\sum_{n=1}^N{1/n^S}}$\n",
    "        \n",
    "        where $k$ is rank of the word; $N$ is the vocabulary size; $s$ is language-specific parameter\n",
    "    - Simply: $f(k;s,N) \\propto 1/k^s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A statistical property of language\n",
    "&nbsp;\n",
    "\n",
    "- Zipf’s law\n",
    "    - Frequency of any word is inversely proportional to its rank in the frequency table\n",
    "    - Formally\n",
    "        - $f(k;s,N)=\\frac{1/k^S}{\\sum_{n=1}^N{1/n^S}}$\n",
    "        \n",
    "        where $k$ is rank of the word; $N$ is the vocabulary size; $s$ is language-specific parameter\n",
    "    - Simply: $f(k;s,N) \\propto 1/k^s$\n",
    "    \n",
    "<img src=\"img/page 44_2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A statistical property of language\n",
    "&nbsp;\n",
    "\n",
    "<p style=\"color:red\">Discrete version of power law</p>\n",
    "\n",
    "<img src=\"img/page 44_1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Menti\n",
    "&nbsp;\n",
    "\n",
    "- In a large Spanish text corpus, if we know the most popular word’s frequency is 145,872, what is your best estimate of its second most popular word’s frequency? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization/Segmentation\n",
    "&nbsp;\n",
    "- Split text into words and sentences\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenization/Segmentation\n",
    "&nbsp;\n",
    "- Split text into words and sentences\n",
    "    \n",
    "<img src=\"img/page 46_1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenization/Segmentation\n",
    "&nbsp;\n",
    "\n",
    "- Split text into words and sentences\n",
    "    \n",
    "<img src=\"img/page 46_2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenization/Segmentation\n",
    "\n",
    "- Split text into words and sentences\n",
    "    - Task: what is the most <span style=\"color:red\">likely</span> segmentation /tokenization?\n",
    "    \n",
    "<img src=\"img/page 46_2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named entity recognition\n",
    "&nbsp;\n",
    "\n",
    "- Determine text mapping to proper names\n",
    "\n",
    "<img src=\"img/page 48_1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Named entity recognition\n",
    "&nbsp;\n",
    "\n",
    "- Determine text mapping to proper names\n",
    "\n",
    "<img src=\"img/page 48_2.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Named entity recognition\n",
    "&nbsp;\n",
    "\n",
    "- Determine text mapping to proper names\n",
    "    - Task: what is the most <span style=\"color:red\">likely</span> mapping \n",
    "\n",
    "<img src=\"img/page 48_2.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part Of Speech (POS) Tagging\n",
    "\n",
    "- Annotate each word in a sentence with a part-of-speech.\n",
    "\n",
    "<img src=\"img/page 50.png\" width=\"400\">\n",
    "\n",
    "- Useful for subsequent syntactic parsing and word sense disambiguation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Zipf’s law tells us\n",
    "&nbsp;\n",
    "\n",
    "- Head words take large portion of occurrences, but they are semantically meaningless\n",
    "    - E.g., the, a, an, we, do, to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Zipf’s law tells us\n",
    "&nbsp;\n",
    "\n",
    "- Head words take large portion of occurrences, but they are semantically meaningless\n",
    "    - E.g., the, a, an, we, do, to\n",
    "- Tail words take major portion of vocabulary, but they rarely occur in documents\n",
    "    - E.g., *sesquipedalianism*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Zipf’s law tells us\n",
    "&nbsp;\n",
    "\n",
    "- Head words take large portion of occurrences, but they are semantically meaningless\n",
    "    - E.g., the, a, an, we, do, to\n",
    "- Tail words take major portion of vocabulary, but they rarely occur in documents\n",
    "    - E.g., *sesquipedalianism*\n",
    "- The rest is most representative\n",
    "    - To be included in the controlled vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic document representation\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 53_1.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Automatic document representation\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 53_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Automatic document representation\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 53_3.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normalization\n",
    "&nbsp;\n",
    "\n",
    "- Convert different forms of a word to a normalized form in the vocabulary\n",
    "    - U.S.A. -> USA, St. Louis -> Saint Louis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normalization\n",
    "&nbsp;\n",
    "\n",
    "- Convert different forms of a word to a normalized form in the vocabulary\n",
    "    - U.S.A. -> USA, St. Louis -> Saint Louis\n",
    "- Solution\n",
    "    - Rule-based\n",
    "        - Delete periods and hyphens\n",
    "        - All in lower cases\n",
    "    - Dictionary-based\n",
    "        - Construct equivalent class \n",
    "            - Car -> “automobile, vehicle”\n",
    "            - Mobile phone -> “cellphone”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normalization\n",
    "&nbsp;\n",
    "\n",
    "- Convert different forms of a word to a normalized form in the vocabulary\n",
    "    - U.S.A. -> USA, St. Louis -> Saint Louis\n",
    "- Solution\n",
    "    - Rule-based\n",
    "        - Delete periods and hyphens\n",
    "        - All in lower cases\n",
    "    - Dictionary-based\n",
    "        - Construct equivalent class <span style=\"color:red\">&#x2190; We will come back to this later</span>\n",
    "            - Car -> “automobile, vehicle”\n",
    "            - Mobile phone -> “cellphone”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming\n",
    "&nbsp;\n",
    "\n",
    "- Reduce inflected or derived words to their root form \n",
    "    - Plurals, adverbs, inflected word forms\n",
    "        - E.g., ladies -> lady, referring -> refer, forgotten -> forget\n",
    "    - Bridge the vocabulary gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stemming\n",
    "&nbsp;\n",
    "\n",
    "- Reduce inflected or derived words to their root form \n",
    "    - Plurals, adverbs, inflected word forms\n",
    "        - E.g., ladies -> lady, referring -> refer, forgotten -> forget\n",
    "    - Bridge the vocabulary gap\n",
    "    - Solutions (for English)\n",
    "        - Porter stemmer: patterns of vowel-consonant sequence\n",
    "        - Krovetz stemmer: morphological rules \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stemming\n",
    "&nbsp;\n",
    "\n",
    "- Reduce inflected or derived words to their root form \n",
    "    - Plurals, adverbs, inflected word forms\n",
    "        - E.g., ladies -> lady, referring -> refer, forgotten -> forget\n",
    "    - Bridge the vocabulary gap\n",
    "    - Solutions (for English)\n",
    "        - Porter stemmer: patterns of vowel-consonant sequence\n",
    "        - Krovetz stemmer: morphological rules \n",
    "    - Risk: lose precise meaning of the word\n",
    "        - E.g., lay -> lie (a false statement? or be in a horizontal position?) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stopwords\n",
    "&nbsp;\n",
    "\n",
    "- Useless words for document analysis\n",
    "    - Not all words are informative\n",
    "    - Remove such words to reduce vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stopwords\n",
    "&nbsp;\n",
    "\n",
    "- Useless words for document analysis\n",
    "    - Not all words are informative\n",
    "    - Remove such words to reduce vocabulary size\n",
    "    - No universal definition\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stopwords\n",
    "&nbsp;\n",
    "\n",
    "- Useless words for document analysis\n",
    "    - Not all words are informative\n",
    "    - Remove such words to reduce vocabulary size\n",
    "    - No universal definition\n",
    "    - Risk: break the original meaning and structure of text\n",
    "        - E.g., this is not a good option -> option\n",
    "           <br> to be or not to be -> null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: a statistical property of language\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 60.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recap: a statistical property of language\n",
    "&nbsp;\n",
    "\n",
    "<p style=\"color:red\">Discrete version of power law</p>\n",
    "<img src=\"img/page 60.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constructing a VSM representation\n",
    "&nbsp;\n",
    "\n",
    "<center>D1: <em>‘Text mining is to identify useful information.’</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constructing a VSM representation\n",
    "&nbsp;\n",
    "\n",
    "<center>D1: <em>‘Text mining is to identify useful information.’</em>\n",
    "</center>\n",
    "\n",
    "<ol>\n",
    "<li><span style=\"color:red;font-weight:bold\">Tokenization:</span></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constructing a VSM representation\n",
    "&nbsp;\n",
    "\n",
    "<center>D1: <em>‘Text mining is to identify useful information.’</em>\n",
    "</center>\n",
    "\n",
    "<ol>\n",
    "<li><span style=\"color:red;font-weight:bold\">Tokenization:</span></li>\n",
    "\n",
    "D1: <em>‘Text’, ‘mining’, ‘is’, ‘to’, ‘identify’, ‘useful’, ‘information’, ‘.’</em>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constructing a VSM representation\n",
    "&nbsp;\n",
    "\n",
    "<center>D1: <em>‘Text mining is to identify useful information.’</em>\n",
    "</center>\n",
    "\n",
    "<ol>\n",
    "<li><span style=\"color:red;font-weight:bold\">Tokenization:</span></li>\n",
    "\n",
    "D1: <em>‘Text’, ‘mining’, ‘is’, ‘to’, ‘identify’, ‘useful’, ‘information’, ‘.’</em>\n",
    "\n",
    "<li><span style=\"color:red;font-weight:bold\">Stemming/normalization:</span></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constructing a VSM representation\n",
    "&nbsp;\n",
    "\n",
    "<center>D1: <em>‘Text mining is to identify useful information.’</em>\n",
    "</center>\n",
    "\n",
    "<ol>\n",
    "<li><span style=\"color:red;font-weight:bold\">Tokenization:</span></li>\n",
    "\n",
    "D1: <em>‘Text’, ‘mining’, ‘is’, ‘to’, ‘identify’, ‘useful’, ‘information’, ‘.’</em>\n",
    "\n",
    "<li><span style=\"color:red;font-weight:bold\">Stemming/normalization:</span></li>\n",
    "\n",
    "D1: <em>‘text’, ‘mine’, ‘is’, ‘to’, ‘identify’, ‘use’, ‘inform’, ‘.’</em>\n",
    "\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constructing a VSM representation\n",
    "&nbsp;\n",
    "\n",
    "<center>D1: <em>‘Text mining is to identify useful information.’</em>\n",
    "</center>\n",
    "\n",
    "<ol>\n",
    "<li><span style=\"color:red;font-weight:bold\">Tokenization:</span></li>\n",
    "\n",
    "D1: *‘Text’, ‘mining’, ‘is’, ‘to’, ‘identify’, ‘useful’, ‘information’, ‘.’*\n",
    "\n",
    "<li><span style=\"color:red;font-weight:bold\">Stemming/normalization:</span></li>\n",
    "\n",
    "D1: *‘text’, ‘mine’, ‘is’, ‘to’, ‘identify’, ‘use’, ‘inform’, ‘.’*\n",
    "\n",
    "<li><span style=\"color:red;font-weight:bold\">N-gram construction:</span></li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constructing a VSM representation\n",
    "&nbsp;\n",
    "\n",
    "<center>D1: <em>‘Text mining is to identify useful information.’</em>\n",
    "</center>\n",
    "\n",
    "<ol>\n",
    "<li><span style=\"color:red;font-weight:bold\">Tokenization:</span></li>\n",
    "\n",
    "D1: <em>‘Text’, ‘mining’, ‘is’, ‘to’, ‘identify’, ‘useful’, ‘information’, ‘.’</em>\n",
    "\n",
    "<li><span style=\"color:red;font-weight:bold\">Stemming/normalization:</span></li>\n",
    "\n",
    "D1: <em>‘text’, ‘mine’, ‘is’, ‘to’, ‘identify’, ‘use’, ‘inform’, ‘.’</em>\n",
    "\n",
    "<li><span style=\"color:red;font-weight:bold\">N-gram construction:</span></li>\n",
    "\n",
    "D1: <em>‘text-mine’, ‘mine-is’, ‘is-to’, ‘to-identify’, ‘identify-use’, ‘use-inform’, ‘inform-.’</em>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constructing a VSM representation\n",
    "&nbsp;\n",
    "\n",
    "<center>D1: <em>‘Text mining is to identify useful information.’</em>\n",
    "</center>\n",
    "\n",
    "<ol>\n",
    "<li><span style=\"color:red;font-weight:bold\">Tokenization:</span></li>\n",
    "\n",
    "D1: <em>‘Text’, ‘mining’, ‘is’, ‘to’, ‘identify’, ‘useful’, ‘information’, ‘.’</em>\n",
    "\n",
    "<li><span style=\"color:red;font-weight:bold\">Stemming/normalization:</span></li>\n",
    "\n",
    "D1: <em>‘text’, ‘mine’, ‘is’, ‘to’, ‘identify’, ‘use’, ‘inform’, ‘.’</em>\n",
    "\n",
    "<li><span style=\"color:red;font-weight:bold\">N-gram construction:</span></li>\n",
    "\n",
    "D1: <em>‘text-mine’, ‘mine-is’, ‘is-to’, ‘to-identify’, ‘identify-use’, ‘use-inform’, ‘inform-.’</em>\n",
    "\n",
    "<li><span style=\"color:red;font-weight:bold\">Stopword/controlled vocabulary filtering::</span></li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constructing a VSM representation\n",
    "&nbsp;\n",
    "\n",
    "<center>D1: <em>‘Text mining is to identify useful information.’</em>\n",
    "</center>\n",
    "\n",
    "<ol>\n",
    "<li><span style=\"color:red;font-weight:bold\">Tokenization:</span></li>\n",
    "\n",
    "D1: <em>‘Text’, ‘mining’, ‘is’, ‘to’, ‘identify’, ‘useful’, ‘information’, ‘.’</em>\n",
    "\n",
    "<li><span style=\"color:red;font-weight:bold\">Stemming/normalization:</span></li>\n",
    "\n",
    "D1: <em>‘text’, ‘mine’, ‘is’, ‘to’, ‘identify’, ‘use’, ‘inform’, ‘.’</em>\n",
    "\n",
    "<li><span style=\"color:red;font-weight:bold\">N-gram construction:</span></li>\n",
    "\n",
    "D1: <em>‘text-mine’, ‘mine-is’, ‘is-to’, ‘to-identify’, ‘identify-use’, ‘use-inform’, ‘inform-.’</em>\n",
    "\n",
    "<li><span style=\"color:red;font-weight:bold\">Stopword/controlled vocabulary filtering::</span></li>\n",
    "\n",
    "D1: <em>‘text-mine’, ‘to-identify’, ‘identify-use’, ‘use-inform’</em>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to assign weights?\n",
    "&nbsp;\n",
    "\n",
    "- <u>Important!</u>\n",
    "- Why?\n",
    "    - Corpus-wise: some terms carry more information about the document content\n",
    "    - Document-wise: not all terms are equally important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to assign weights?\n",
    "&nbsp;\n",
    "\n",
    "- <u>Important!</u>\n",
    "- Why?\n",
    "    - Corpus-wise: some terms carry more information about the document content\n",
    "    - Document-wise: not all terms are equally important\n",
    "- How? \n",
    "    - Two basic <u>heuristics</u>\n",
    "        - TF (Term Frequency) = Within-doc-frequency\n",
    "        - IDF (Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term frequency\n",
    "&nbsp;\n",
    "\n",
    "- Idea: a term is more important if it occurs more frequently in a document\n",
    "- TF Formulas\n",
    "    - Let $c(t,d)$ be the frequency count of term $t$ in doc $d$\n",
    "    - Raw TF:  $tf(t,d) =c(t,d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Term frequency\n",
    "&nbsp;\n",
    "\n",
    "- Idea: a term is more important if it occurs more frequently in a document\n",
    "- TF Formulas\n",
    "    - Let $c(t,d)$ be the frequency count of term $t$ in doc $d$\n",
    "    - Raw TF:  $tf(t,d) =c(t,d)$\n",
    "    \n",
    "<img src=\"img/page 64.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TF normalization\n",
    "&nbsp;\n",
    "\n",
    "- Two views of document length\n",
    "    - A doc is long because it is verbose\n",
    "    - A doc is long because it has more content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TF normalization\n",
    "&nbsp;\n",
    "\n",
    "- Two views of document length\n",
    "    - A doc is long because it is verbose\n",
    "    - A doc is long because it has more content\n",
    "- Raw TF is inaccurate\n",
    "    - Document length variation\n",
    "    - “Repeated occurrences” are less informative than the “first occurrence”\n",
    "    - Information about semantic does not increase proportionally with number of term occurrence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TF normalization\n",
    "&nbsp;\n",
    "\n",
    "- Two views of document length\n",
    "    - A doc is long because it is verbose\n",
    "    - A doc is long because it has more content\n",
    "- Raw TF is inaccurate\n",
    "    - Document length variation\n",
    "    - “Repeated occurrences” are less informative than the “first occurrence”\n",
    "    - Information about semantic does not increase proportionally with number of term occurrence\n",
    "- Generally penalize long document, but avoid over-penalizing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TF normalization\n",
    "&nbsp;\n",
    "\n",
    "- Two views of document length\n",
    "    - A doc is long because it is verbose\n",
    "    - A doc is long because it has more content\n",
    "- Raw TF is inaccurate\n",
    "    - Document length variation\n",
    "    - “Repeated occurrences” are less informative than the “first occurrence”\n",
    "    - Information about semantic does not increase proportionally with number of term occurrence\n",
    "- Generally penalize long document, but avoid over-penalizing\n",
    "    - Pivoted length normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TF normalization\n",
    "&nbsp;\n",
    "\n",
    "- Maximum TF scaling\n",
    "    - $tf(t,d) = \\alpha + (1 - \\alpha)\\frac{c(t,d}{\\underset{t}{\\operatorname{max}} \n",
    "{c(t,d)}}$, if $c(t,d)>0$\n",
    "    - Normalize by the most frequent word in this doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TF normalization\n",
    "&nbsp;\n",
    "\n",
    "- Maximum TF scaling\n",
    "    - $tf(t,d) = \\alpha + (1 - \\alpha)\\frac{c(t,d}{\\underset{t}{\\operatorname{max}} \n",
    "{c(t,d)}}$, if $c(t,d)>0$\n",
    "    - Normalize by the most frequent word in this doc\n",
    "\n",
    "<img src=\"img/page 66.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TF normalization\n",
    "&nbsp;\n",
    "\n",
    "- Sub-linear TF scaling\n",
    "    - $$tf(t,d)= \\left\\{  \n",
    "\\begin{array}{**rcl**}\n",
    "    1 + logc(t,d), if \\ c(t,d)>0 & \\\\\n",
    "    0, \\ \\ otherwise&   \n",
    "\\end{array}\n",
    "\\right. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document frequency\n",
    "&nbsp;\n",
    "\n",
    "- Idea: a term is more discriminative if it occurs only in fewer documents\n",
    "\n",
    "<img src=\"img/page 69.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverse document frequency\n",
    "&nbsp;\n",
    "\n",
    "- Solution\n",
    "    - Assign higher weights to rare terms\t\n",
    "    - Formula\n",
    "        - $IDF(t) = 1 + log(\\frac{N}{df(t)})$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Inverse document frequency\n",
    "&nbsp;\n",
    "\n",
    "- Solution\n",
    "    - Assign higher weights to rare terms\t\n",
    "    - Formula\n",
    "        - $IDF(t) = 1 + log(\\frac{N}{df(t)})$\n",
    "        \n",
    "        <span style=\"color:red\">$log$: Non-linear scaling; $N$: total number of documents; $df(t)$: number of docs containing term $t$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Inverse document frequency\n",
    "&nbsp;\n",
    "\n",
    "- Solution\n",
    "    - Assign higher weights to rare terms\t\n",
    "    - Formula\n",
    "        - $IDF(t) = 1 + log(\\frac{N}{df(t)})$\n",
    "        \n",
    "        <span style=\"color:red\">$log$: Non-linear scaling; $N$: total number of documents; $df(t)$: number of docs containing term $t$</span>\n",
    "    - A corpus-specific property\n",
    "        - Independent of a single document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Menti\n",
    "&nbsp;\n",
    "\n",
    "- If we remove one document from the corpus, how would it affect the IDF of words in the vocabulary?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Menti\n",
    "&nbsp;\n",
    "\n",
    "- If we remove one document from the corpus, how would it affect the IDF of words in the vocabulary?\n",
    "- If we add one document from the corpus, how would it affect the IDF of words in the vocabulary?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why document frequency\n",
    "&nbsp;\n",
    "\n",
    "- How about total term frequency?\n",
    "    - $ttf(t) = \\sum_d{c(t,d)}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why document frequency\n",
    "&nbsp;\n",
    "\n",
    "- How about total term frequency?\n",
    "    - $ttf(t) = \\sum_d{c(t,d)}$\n",
    "    \n",
    "    <img src=\"img/page 73.png\" width=\"550\">\n",
    "\n",
    "    - Cannot recognize words frequently occurring in a subset of documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TF-IDF weighting \n",
    "&nbsp;\n",
    "\n",
    "- Combining TF and IDF \n",
    "    - Common in doc &#x2192; high tf &#x2192; high weight\n",
    "    - Rare in collection &#x2192; high idf &#x2192; high weight\n",
    "    - $w(t,d)=TF(t,d) \\times IDF(t)$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TF-IDF weighting \n",
    "&nbsp;\n",
    "\n",
    "- Combining TF and IDF \n",
    "    - Common in doc &#x2192; high tf &#x2192; high weight\n",
    "    - Rare in collection &#x2192; high idf &#x2192; high weight\n",
    "    - $w(t,d)=TF(t,d) \\times IDF(t)$\n",
    "    \n",
    "- Most well-known document representation schema in IR! (G Salton et al. 1983)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TF-IDF weighting \n",
    "&nbsp;\n",
    "\n",
    "- Combining TF and IDF \n",
    "    - Common in doc &#x2192; high tf &#x2192; high weight\n",
    "    - Rare in collection &#x2192; high idf &#x2192; high weight\n",
    "    - $w(t,d)=TF(t,d) \\times IDF(t)$\n",
    "    \n",
    "- Most well-known document representation schema in IR! (G Salton et al. 1983)\n",
    "\n",
    "<img src=\"img/page 74_1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TF-IDF weighting \n",
    "&nbsp;\n",
    "\n",
    "- Combining TF and IDF \n",
    "    - Common in doc &#x2192; high tf &#x2192; high weight\n",
    "    - Rare in collection &#x2192; high idf &#x2192; high weight\n",
    "    - $w(t,d)=TF(t,d) \\times IDF(t)$\n",
    "    \n",
    "- Most well-known document representation schema in IR! (G Salton et al. 1983)\n",
    "\n",
    "<img src=\"img/page 74_2.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to define a good similarity metric?\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 75_1.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to define a good similarity metric?\n",
    "&nbsp;\n",
    "\n",
    "- Euclidean distance?\n",
    "\n",
    "<img src=\"img/page 75_2.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to define a good similarity metric?\n",
    "&nbsp;\n",
    "\n",
    "- Euclidean distance\n",
    "    - $dist(d_i, d_j) = \\sqrt{\\sum_{t\\in V}{[tf(t,d_i)idf(t) - tf(t, d_j)idf(t)]^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to define a good similarity metric?\n",
    "&nbsp;\n",
    "\n",
    "- Euclidean distance\n",
    "    - $dist(d_i, d_j) = \\sqrt{\\sum_{t\\in V}{[tf(t,d_i)idf(t) - tf(t, d_j)idf(t)]^2}}$\n",
    "    - Longer documents will be penalized by the extra words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to define a good similarity metric?\n",
    "&nbsp;\n",
    "\n",
    "- Euclidean distance\n",
    "    - $dist(d_i, d_j) = \\sqrt{\\sum_{t\\in V}{[tf(t,d_i)idf(t) - tf(t, d_j)idf(t)]^2}}$\n",
    "    - Longer documents will be penalized by the extra words\n",
    "    - We care more about how these two vectors are overlapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From distance to angle\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 77_1.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## From distance to angle\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 77_2.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## From distance to angle\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 77_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## From distance to angle\n",
    "&nbsp;\n",
    "\n",
    "- Angle: how vectors are overlapped\n",
    "    - Cosine similarity – projection of one vector onto another\n",
    "\n",
    "\n",
    "<img src=\"img/page 77_3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cosine similarity\n",
    "&nbsp;\n",
    "\n",
    "- Angle between two vectors \n",
    "    - $cosine(d_i, d_j) = \\frac{V_{d_i}^TV_{d_j}}{|V_{d_i}|_2 \\times |V_{d_j}|_2}$ <span style=\"color:red\">&#x2190; TF-IDF vector</span>\n",
    "    \n",
    "<img src=\"img/page 78_1.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cosine similarity\n",
    "&nbsp;\n",
    "\n",
    "- Angle between two vectors \n",
    "    - $cosine(d_i, d_j) = \\frac{V_{d_i}^TV_{d_j}}{|V_{d_i}|_2 \\times |V_{d_j}|_2}$ <span style=\"color:red\">&#x2190; TF-IDF vector</span>\n",
    "    \n",
    "    <img src=\"img/page 78_1.png\" width=\"200\">\n",
    "    \n",
    "    - Documents are normalized by length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cosine similarity\n",
    "&nbsp;\n",
    "\n",
    "- Angle between two vectors \n",
    "    - $cosine(d_i, d_j) = \\frac{V_{d_i}^TV_{d_j}}{|V_{d_i}|_2 \\times |V_{d_j}|_2}$ <span style=\"color:red\">&#x2190; TF-IDF vector</span>\n",
    "    \n",
    "    <img src=\"img/page 78_1.png\" width=\"200\">\n",
    "    \n",
    "    - Documents are normalized by length\n",
    "\n",
    "<img src=\"img/page 78_2.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cosine similarity\n",
    "&nbsp;\n",
    "\n",
    "- Angle between two vectors \n",
    "    - $cosine(d_i, d_j) = \\frac{V_{d_i}^TV_{d_j}}{|V_{d_i}|_2 \\times |V_{d_j}|_2}$ <span style=\"color:red\">&#x2190; TF-IDF vector</span>\n",
    "    \n",
    "    <img src=\"img/page 78_1.png\" width=\"100\">\n",
    "    \n",
    "    - Documents are normalized by length\n",
    "\n",
    "<img src=\"img/page 78_3.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages and disadvantages of VS model\n",
    "&nbsp;\n",
    "\n",
    "- Empirically effective! \n",
    "- Intuitive\n",
    "- Easy to implement\n",
    "- Well-studied/mostly evaluated\n",
    "- The Smart system\n",
    "    - Developed at Cornell: 1960-1999\n",
    "    - Still widely used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Advantages and disadvantages of VS model\n",
    "&nbsp;\n",
    "\n",
    "- Empirically effective! \n",
    "- Intuitive\n",
    "- Easy to implement\n",
    "- Well-studied/mostly evaluated\n",
    "- The Smart system\n",
    "    - Developed at Cornell: 1960-1999\n",
    "    - Still widely used \n",
    "- <span style=\"color:red\">Warning: many variants of TF-IDF!</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Disadvantages of VS model\n",
    "&nbsp;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Disadvantages of VS model\n",
    "&nbsp;\n",
    "\n",
    "- Assume term independence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Disadvantages of VS model\n",
    "&nbsp;\n",
    "\n",
    "- Assume term independence\n",
    "- Lack of “predictive adequacy” \n",
    "    - Arbitrary term weighting\n",
    "    - Arbitrary similarity measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Disadvantages of VS model\n",
    "&nbsp;\n",
    "\n",
    "- Assume term independence\n",
    "- Lack of “predictive adequacy” \n",
    "    - Arbitrary term weighting\n",
    "    - Arbitrary similarity measure\n",
    "- Lots of parameter tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Menti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other examples (from different disciplines)\n",
    "&nbsp;\n",
    "\n",
    "- SALTClass\n",
    "- ICD Classification\n",
    "- ASReview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/page 84.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/page 85.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/page 86.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/page 87.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary: what did we learn?\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\text {Time for Practical 1!}$$ "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impractical 1: Text Pre-processing\n",
    "#### Ayoub Bagheri\n",
    "<img src=\"img/uu_logo.png\" alt=\"logo\" align=\"right\" title=\"UU\" width=\"50\" height=\"20\" />\n",
    "\n",
    "\n",
    "#### Applied Text Mining - Utrecht Summer School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical, we are first going to get acquainted with Python in Google Colab, then we will do some text preprocessing! Are you looking for Python documentation to refresh you knowledge of programming? If so, you can check https://docs.python.org/3/reference/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser, with:\n",
    "* Zero configuration required\n",
    "* Free access to GPUs\n",
    "* Easy sharing\n",
    "\n",
    "Colab notebooks are Jupyter notebooks that are hosted by Colab. Here you can find links to more detailed introductions to Colab: https://colab.research.google.com/notebooks/intro.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing simple texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. **Open Colab and create a new empty notebook to work with Python 3!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to https://colab.research.google.com/ and login with your account. Then click on \"File $\\rightarrow$ New notebook\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. **Text is also known as a string variable, or as an array of characters. Create a variable _a_ with the text value of \"Hello @Text Mining World! I'm here to learn everything, right?\", and then print it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. **Since this is an array, print the first and last character of your variable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. **Use _\"!pip install\"_ command and install the packages: nltk, spacy, gensim, string, numpy.**\n",
    "\n",
    "Generally, you only need to install each package once on your computer and load it again, however, in Colab you may need to reinstall a package once you are reconnecting to the network.\n",
    "\n",
    "NB: nltk comes with many corpora, toy grammars, trained models, etc. A complete list is posted at: http://nltk.org/nltk_data/\n",
    "To install the data, after installing nltk, use nltk’s data downloader as \"nltk.download()\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: If you want to stop your code in Jupyter and it does not stop from running:\n",
    "- raise SystemExit(\"Stop right there!\")\n",
    "- Or: One simple trick to get rid of this problem, is to press \"ctrl+a\" to select all the code of that particular cell you want to stop the execution of and press \"ctrl+x\" to cut the entire cell code. Now the cell is empty and just the empty cell is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\programdata\\miniconda3\\lib\\site-packages (1.18.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement string (from versions: none)\n",
      "ERROR: No matching distribution found for string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\miniconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk) (0.14.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk) (4.36.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: gensim in c:\\programdata\\miniconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\programdata\\miniconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Collecting spacy\n",
      "  Using cached https://files.pythonhosted.org/packages/04/11/09c1fcbd0ef9741142f4a990406a2adddbc9f951163b49cbd7170bb539be/spacy-3.1.1-cp37-cp37m-win_amd64.whl\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Using cached https://files.pythonhosted.org/packages/7a/ec/f8ff3ccfc4e59ce619a66a0bf29dc3b49c2e8c07de29d572e191c006eaa2/tqdm-4.61.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.11.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (8.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (41.4.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\programdata\\miniconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\miniconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in c:\\programdata\\miniconda3\\lib\\site-packages (from catalogue<2.1.0,>=2.0.4->spacy) (3.1.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.6)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Installing collected packages: tqdm, spacy\n",
      "  Found existing installation: tqdm 4.36.1\n",
      "    Uninstalling tqdm-4.36.1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\miniconda3\\\\lib\\\\site-packages\\\\tqdm-4.36.1.dist-info\\\\entry_points.txt'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install string\n",
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. **Import (load) the nltk package and use the function _lower_ to convert the characters in string _a_ to their lowercase form and save it into a new variable _b_.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. **Use the _string_ package to print the list of punctuations.**\n",
    "\n",
    "Punctuations can separate characters, words, phrases, or sentences. In some applications they are very important to the task at hand, in others they are redundant and should be removed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. **Use the punctuation list to remove the punctuations from the lowercase form of our example string _a_. Name your variable _c_.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. **Use _word_tokenize_ function from _nltk_ and tokenize string _b_. Compare that with the tokenization of string _c_.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the main difference is in punctuations, however, we also see that some words are now combined togehter in the tokenization of string c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. **Use the function _Regexptokenizer_ from _nltk_ to tokenize string _b_ whilst removing punctuations. This way you will avoid unnecessary concatenations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this tokenizer, you get the same output as with tokenizing the string c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. **Use _sent_tokenize_ function from the _nltk_ package and split string _b_ into sentences. Compare that with the sentence tokenization of string _c_.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization. Imagine you need to count average words per sentence, how you will calculate? For accomplishing such a task, you need both NLTK sentence tokenizer as well as NLTK word tokenizer to calculate the ratio. Such output serves as an important feature for machine training as the answer would be numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing a text corpus (data set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing a data set is similar to pre-processing simple text strings. First, we need to get some data. For this, we can use our own data set, or we can scrape data from web or use social media APIs. There are also some websites with publicly available data sets:\n",
    "- CLARIN Resource Families: https://www.clarin.eu/portal\n",
    "- UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets.php?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table\n",
    "- Kaggle: https://www.kaggle.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we want to analyze and pre-process the Taylor Swift song lyrics data from all her albums. We downloaded this data set from the Kaggle website and put that already in the data folder. Here is the link to the original data set: https://www.kaggle.com/PromptCloudHQ/taylor-swift-song-lyrics-from-all-the-albums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11\\. **Read the “taylor_swift.csv” data set from the data folder. Check the head and tail functions with your dataframe.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12\\. **Add a new column to the dataframe and name it _Preprocessed_ _Lyrics_ , then fill the column out with the preprocessed text including the steps in this and the following questions. First replace the '\\n' notations with a space character.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13\\. **Write another custom function to remove the punctuations. You can use the previous method or make use of the function maketrans from the string package.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14\\. **Change all the characters to their lower forms. Think about why and when we need this step in our analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15\\. **List the 20 most frequent terms in this dataframe.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16\\. **Plot a wordcloud with max 50 words using the WordCloud function from the wordcloud package. Use the command _?WordCloud_ to check the help for this function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\programdata\\miniconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\miniconda3\\lib\\site-packages (from wordcloud) (3.1.3)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\miniconda3\\lib\\site-packages (from wordcloud) (7.2.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from wordcloud) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\miniconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\miniconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (41.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "?WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17\\. Use the English stop word list from the nltk package to remove the stop words. Check the stop words and update them with your optional list of words, for example: \"im\", \"youre\", \"id\", \"dont\", \"cant\", \"didnt\", \"ive\", \"ill\", \"hasnt\". Show the 20 most frequent terms and plot the wordcould of 50 words again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18\\. We can apply stemming or lemmatization on our text data. Apply a lemmatizer from nltk and save the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PorterStemmer is for English language. If we are working with other languages, we can use snowball stemmer. The supported languages for snowball stemmer are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector space and BOW models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19\\. **Use the CountVectorizer from the sklearn package and build a bag of words model on _Preprocessed Lyrics_ based on term frequency. Check the shape of the output matrix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20\\. **Inspect the first 100 terms in the vocabulary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21\\. **Using TfidfVectorizer, you can create a model based on tfidf. Apply a TfidfVectorizer to your text data. Does the shape of the output matrix differ from dtm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22\\. **Use the TfidfVectorizer to create an n-gram based model with n = 1 and 2. Use the ngram_range argument to determine the lower and upper boundary of the range of n-values for different n-grams to be extracted. (tip: use ?TfidfVectorizer)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23\\. **We want to compare the lyrics of Friends theme song with the lyrics of Taylor Swift's songs and find the most similar one. Use the string below, first, apply the pre-processing steps and then transform the text into count and tfidf vectors. Do the bag of words models agree on the most similar song to Friends theme song?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So no one told you life was going to be this way. Your job's a joke, you're broke, you're love life's DOA. It's like you're always stuck in second gear, When it hasn't been your day, your week, your month, or even your year. But, I'll be there for you, when the rain starts to pour. I'll be there for you, like I've been there before. I'll be there for you, cause you're there for me too.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends_theme_lyrics = \"So no one told you life was going to be this way. Your job's a joke, you're broke, you're love life's DOA. It's like you're always stuck in second gear, When it hasn\\'t been your day, your week, your month, or even your year. But, I\\'ll be there for you, when the rain starts to pour. I\\'ll be there for you, like I\\'ve been there before. I\\'ll be there for you, cause you\\'re there for me too.\"\n",
    "friends_theme_lyrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

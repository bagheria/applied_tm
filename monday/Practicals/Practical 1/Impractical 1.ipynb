{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2Z6Sxt2_e_n"
      },
      "source": [
        "# Practical 1: Text Pre-processing\n",
        "#### Ayoub Bagheri\n",
        "<img src=\"https://github.com/bagheria/applied_tm/blob/master/monday/Practicals/Practical%201/img/uu_logo.png?raw=1\" alt=\"logo\" align=\"right\" title=\"UU\" width=\"50\" height=\"20\" />\n",
        "\n",
        "\n",
        "#### Applied Text Mining - Utrecht Summer School"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOQ6TOUO_e_o"
      },
      "source": [
        "In this practical, we are first going to get acquainted with Python in Google Colab, then we will do some text preprocessing! Are you looking for Python documentation to refresh you knowledge of programming? If so, you can check https://docs.python.org/3/reference/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLMIpUpz_e_o"
      },
      "source": [
        "Google Colaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser, with:\n",
        "* Zero configuration required\n",
        "* Free access to GPUs\n",
        "* Easy sharing\n",
        "\n",
        "Colab notebooks are Jupyter notebooks that are hosted by Colab. You can find more detailed introductions to Colab [here](https://colab.research.google.com/notebooks/intro.ipynb), but we will also cover the basics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0PMYm66_e_p"
      },
      "source": [
        "### Pre-processing simple texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC4oi5E-_e_p"
      },
      "source": [
        "### Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_Y8cIjk_e_p"
      },
      "source": [
        "**Here we are going to introduce Python and Google Colab a bit. If you are familiar with Python, start with question 11.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbB5pUmQ_e_q"
      },
      "source": [
        "1\\. **Open Colab and create a new empty notebook to work with Python 3!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvABBTqJ_e_q"
      },
      "source": [
        "Go to https://colab.research.google.com/ and login with your account. Then click on \"File $\\rightarrow$ New notebook\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y2rmdmqD7Ex"
      },
      "source": [
        "If you want to insert a new code chunk below of the cell you are currently in, press `Alt + Enter`.\n",
        "\n",
        "If you want to stop your code from running in Colab:\n",
        "- Interrupt execution by pressing `ctrl + M I` or simply click the stop button\n",
        "- Or: Press `ctrl + A` to select all the code of that particular cell, press `ctrl + X` to cut the entire cell code. Now the cell is empty and can be deleted by using `ctrl + M D` or by pressing the delete button. You can paste your code in a new code chunk and adjust it.\n",
        "\n",
        "NB: On Macbooks, use `cmd` instead of `ctrl` in shortcuts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvumQnvm_e_q"
      },
      "source": [
        "2\\. **Text is also known as a string variable, or as an array of characters. Create a variable `a` with the text value of `\"Hello @Text Mining World! I'm here to learn everything, right?\"`, and then print it!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9aDlvid_e_s"
      },
      "source": [
        "3\\. **Since this is an array, print the first and last character of your variable.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUDe4m-Q_e_t"
      },
      "source": [
        "4\\. **Use the `!pip install` command and install the packages: `numpy`, `nltk`, `gensim`, and `spacy`.**\n",
        "\n",
        "Generally, you only need to install each package once on your computer and load it again, however, in Colab you may need to reinstall a package once you are reconnecting to the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOiqy5IW_e_u"
      },
      "outputs": [],
      "source": [
        "!pip install -q numpy\n",
        "!pip install -q nltk\n",
        "!pip install -q gensim\n",
        "!pip install -q spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuFbeRZS_e_u"
      },
      "source": [
        "5\\. **Import (load) the `nltk` package and use the function `lower()` to convert the characters in string `a` to their lowercase form and save it into a new variable `b`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTDVccwQLg-v"
      },
      "source": [
        "NB: `nltk` comes with many corpora, toy grammars, trained models, etc. A complete list is posted at: http://nltk.org/nltk_data/\n",
        "\n",
        "To install the data, after installing `nltk`, you could use the `nltk.download()` data downloader. We will make use of this in Question 8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhUcakjL_e_v"
      },
      "source": [
        "6\\. **Use the `string` package to print the list of punctuations.**\n",
        "\n",
        "Punctuations can separate characters, words, phrases, or sentences. In some applications they are very important to the task at hand, in others they are redundant and should be removed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xulG_2Dt_e_v"
      },
      "source": [
        "7\\. **Use the punctuation list to remove the punctuations from the lowercase form of our example string `a`. Name your variable `c`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_OgCGqM_e_w"
      },
      "source": [
        "8\\. **Use the function `word_tokenize()` function from `nltk` and tokenize string `b`. Compare that with the tokenization of string `c`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i8d0AZa_e_w"
      },
      "source": [
        "We see that the main difference is in punctuations, however, we also see that some words are now combined togehter in the tokenization of string `c`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHVc16DE_e_w"
      },
      "source": [
        "9\\. **Use the function `Regexptokenizer()` from `nltk` to tokenize the string `b` whilst removing punctuations. This way you will avoid unnecessary concatenations.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnn45ert_e_w"
      },
      "source": [
        "With this tokenizer, you get similar output as with tokenizing the string `c`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n2lr76s_e_x"
      },
      "source": [
        "10\\. **Use funtion `sent_tokenize()` from the `nltk` package and split the string `b` into sentences. Compare that with the sentence tokenization of string `c`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WwXvBm0_e_x"
      },
      "source": [
        "An obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization. Imagine you need to count average words per sentence. How would you calculate it? For accomplishing such a task, you need both the NLTK sentence tokenizer as well as the NLTK word tokenizer to calculate the ratio. Such output serves as an important feature for machine training as the answer would be numeric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X4r7iwi_e_x"
      },
      "source": [
        "### Pre-processing a text corpus (dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSEqZl0P_e_x"
      },
      "source": [
        "Pre-processing a dataset is similar to pre-processing simple text strings. First, we need to get some data. For this, we can use our own dataset, or we can scrape data from web or use social media APIs. There are also some websites with publicly available datasets:\n",
        "- [CLARIN Resource Families](https://www.clarin.eu/portal)\n",
        "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table)\n",
        "- [Kaggle](https://www.kaggle.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1DCngUZ_e_x"
      },
      "source": [
        "Here, we want to analyze and pre-process the Taylor Swift song lyrics data from all her albums. The dataset can be downloaded from the course [website](https://ayoubbagheri.nl/applied_tm/monday/Practicals/Practical%201/data.zip) or alternatively from [Kaggle](https://www.kaggle.com/PromptCloudHQ/taylor-swift-song-lyrics-from-all-the-albums).\n",
        "\n",
        "Upload `taylor_swift_lyrics.csv` to Google Colab. You can do this by clicking on the Files button on the very left side of Colab and drag and drop the data there or click the upload button. Alternatively you can mount Google Drive and upload the dataset there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noUphbO__e_x"
      },
      "source": [
        "11\\. **Read the `taylor_swift.csv` dataset. Check the dataframe using `head()` and `tail()` functions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjRIVlyZ_e_z"
      },
      "source": [
        "12\\. **Add a new column to the dataframe and name it `Preprocessed Lyrics` , then fill the column with the preprocessed text including the steps in this and the following questions. First replace the `\\n` sequences with a space character.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMrLTU6R_e_z"
      },
      "source": [
        "13\\. **Write another custom function to remove the punctuations. You can use the previous method or make use of the function `maketrans()` from the `string` package.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7O1J3dD_e_z"
      },
      "source": [
        "14\\. **Change all the characters to their lower forms. Think about why and when we need this step in our analysis.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMvNJTTb_e_0"
      },
      "source": [
        "15\\. **List the 20 most frequent terms in this dataframe.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SGVk2iT_e_0"
      },
      "source": [
        "You see that these are mainly stop words. Before removing them let's plot a wordcloud of our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81zPVUPE_e_0"
      },
      "source": [
        "16\\. **Plot a wordcloud with max 50 words using the `WordCloud()` function from the `wordcloud` package. Use the command `?WordCloud` to check the help for this function.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww7Q4LTI_e_1"
      },
      "source": [
        "17\\. **Use the English stop word list from the `nltk` package to remove the stop words. Check the stop words and update them with your optional list of words, for example: \"im\", \"youre\", \"id\", \"dont\", \"cant\", \"didnt\", \"ive\", \"ill\", \"hasnt\". Show the 20 most frequent terms and plot the wordcould of 50 words again.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glmBDTA8_e_2"
      },
      "source": [
        "18\\. **We can apply stemming or lemmatization on our text data. Apply a lemmatizer from `nltk` and save the results.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxjncheH_e_2"
      },
      "source": [
        "And here is the code for stemming:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdX-47X9_e_2"
      },
      "source": [
        "The `PorterStemmer()` is for English language. If we are working with other languages, we can use other stemmers such as the `SnowballStemmer()` which supports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wUpLYm6_e_2",
        "outputId": "cbf93a16-185c-4d45-917d-b04e716e15b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "SnowballStemmer.languages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thsgfRTe_e_2"
      },
      "source": [
        "### Vector space and BOW models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmtbWZRj_e_3"
      },
      "source": [
        "19\\. **Use `CountVectorizer()` from the `sklearn` package and build a bag of words model on `Preprocessed Lyrics` based on term frequency. Check the shape of the output matrix.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXfX2Lhm_e_3"
      },
      "source": [
        "20\\. **Inspect the first 100 terms in the vocabulary.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjLkOUWZ_e_3"
      },
      "source": [
        "21\\. **Using `TfidfVectorizer()`, you can create a model based on tfidf. Apply this vectorizer to your text data. Does the shape of the output matrix differ from dtm?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt1kTxaC_e_4"
      },
      "source": [
        "22\\. **Use the `TfidfVectorizer()` to create an n-gram based model with n = 1 and 2. Use the `ngram_range` argument to determine the lower and upper boundary of the range of n-values for different n-grams to be extracted. (tip: use `?TfidfVectorizer`)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zahiDP-Y_e_4"
      },
      "source": [
        "23\\. **We want to compare the lyrics of Friends theme song with the lyrics of Taylor Swift's songs and find the most similar one. Use the string below. First, apply the pre-processing steps and then transform the text into count and tfidf vectors.\n",
        "\n",
        "Do the bag of words models agree on the most similar song to Friends theme song?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "I1yQJhBJ_e_4",
        "outputId": "7bf0554e-ecb1-4b98-c444-6b211fc648b2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"So no one told you life was going to be this way. Your job's a joke, you're broke, you're love life's DOA. It's like you're always stuck in second gear, When it hasn't been your day, your week, your month, or even your year. But, I'll be there for you, when the rain starts to pour. I'll be there for you, like I've been there before. I'll be there for you, cause you're there for me too.\""
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "friends_theme_lyrics = \"So no one told you life was going to be this way. Your job's a joke, you're broke, you're love life's DOA. It's like you're always stuck in second gear, When it hasn\\'t been your day, your week, your month, or even your year. But, I\\'ll be there for you, when the rain starts to pour. I\\'ll be there for you, like I\\'ve been there before. I\\'ll be there for you, cause you\\'re there for me too.\"\n",
        "friends_theme_lyrics"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

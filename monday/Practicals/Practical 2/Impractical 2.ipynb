{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22p-xE9-7owv"
      },
      "source": [
        "# Genre classification with `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNNqoJYC77Yg"
      },
      "source": [
        "1\\. **Upload the \"book_reviews.csv\" from your machine, following the [Colab documentation](https://colab.research.google.com/notebooks/io.ipynb). This file contains 10,000 English language book reviews from Goodreads, with genre, age and star rating labels. Uploading may take a minute or so.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nmIG604iAfO"
      },
      "source": [
        "2\\. **Load the .csv file into a [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html?highlight=read_csv#pandas.read_csv). This makes it easy to acess and filter data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GM1bjeQ9GSS"
      },
      "source": [
        "3\\. **Now you can construct the document-term matrix. The [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class counts how often each word occurs in each document. Optionally, you can also pass `ngram_range` as a parameter, to see if combinations of multiple words are better predictors for ratings. Define the output of the `fit_transform` function on `'tokenised_text'` as your feature matrix `X`, and the star ratings (`'rating_no'`) as the variable `y` you're trying to predict.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uIibvNSF-mHi"
      },
      "source": [
        "To inspect the words in the document-term matrix, you can call `get_feature_names_out()` on the vectorizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wLQif1t-0qw"
      },
      "source": [
        "Alternatively, you could also use a [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer): this class counts how often a word occurs in a document and weighs it against how often the word occurs in the whole corpus. This is a way to eliminate words that are frequent but not very meaningful. You can play around with different vectorizers to see how they affect your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H19ZRObVd2Ec"
      },
      "source": [
        "4\\. **Now we can define a baseline model: use the `DummyClassifier` to always predict the most frequent genre in the dataset.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do9U5e_nAJ6y"
      },
      "source": [
        "5\\. **After defining your document-term matrix, you can split the data into train- and test sets. Note that `random_state` is used so that the split will be the same for everyone in the group, such that different random selections don't cause slightly different results.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU_7jXDlEOJP"
      },
      "source": [
        "6\\. **Now pick one of the following classifiers:**\n",
        "- [K-Nearest Neighbor classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
        "- [Multionimal Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)\n",
        "- [Support Vector Machine](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
        "- [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
        "- [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2soMKGJzGCHq"
      },
      "source": [
        "7\\. **Find the parameters which lead to best results. You can also automatate this with [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), as shown below.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJsZ7nBwJ4uL"
      },
      "source": [
        "8\\. **Try combining multiple classifiers, for instance with a [Voting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) Can you get a better result?**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

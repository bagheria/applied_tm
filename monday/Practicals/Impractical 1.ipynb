{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impractical 1\n",
    "\n",
    "#### Applied Text Mining - Utrecht Summer School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical, we are first going to get acquainted with Python in Google Colab, then we will do some text preprocessing! Are you looking for Python documentation to refresh you knowledge of programming? If so, you can check https://docs.python.org/3/reference/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser, with:\n",
    "* Zero configuration required\n",
    "* Free access to GPUs\n",
    "* Easy sharing\n",
    "\n",
    "Colab notebooks are Jupyter notebooks that are hosted by Colab. Here you can find links to more detailed introductions to Colab: https://colab.research.google.com/notebooks/intro.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. **Open Colab and create a new empty notebook to work with Python 3!** We are going to work with the Python libraries NLTK, Gensim, and spaCy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to https://colab.research.google.com/ and login with your account. Then click on \"File $\\rightarrow$ New notebook\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. **Text is also known as a string variable, or as an array of characters. Create a variable with the text value of \"Hello @Text Mining World! I'm here to learn everything, right?\", and then print it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. **Since this is an array, print the first and last character of your variable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. **Use _\"!pip install\"_ command and install the packages: nltk, spacy, gensim, string, numpy.**\n",
    "\n",
    "Generally, you only need to install each package once on your computer and load it again, however, in Colab you may need to reinstall a package once you are reconnecting to the network.\n",
    "\n",
    "NB: nltk comes with many corpora, toy grammars, trained models, etc. A complete list is posted at: http://nltk.org/nltk_data/\n",
    "To install the data, after installing nltk, use nltkâ€™s data downloader as \"nltk.download()\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: Solution if you want to stop your code and it does not stop from running: \n",
    "Either:  raise SystemExit(\"Stop right there!\")\n",
    "\n",
    "Or: One simple trick to get rid of this problem, is to press \"ctrl+a\" to select all the code of that particular cell you want to stop the execution of and press \"ctrl+x\" to cut the entire cell code. Now the cell is empty and just the empty cell is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. **Import (load) the nltk package and use the function _lower_ to convert the characters in string _a_ to their lowercase form.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. **Use the _string_ package to print the list of punctuations.**\n",
    "\n",
    "Punctuations can separate characters, words, phrases, or sentences. In some applications they are very important to the task at hand, in others they are redundant and should be removed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. **Use the punctuation list to remove the punctuations from the lowercase form of our example string _a_. Name your variable _c_.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. **Use _word_tokenize_ function from _nltk_ and tokenize string _b_. Compare that with the tokenization of string _c_.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the main difference is in punctuations, however, we also that some words are now combined togehter in the tokenization of string c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. **Use the function _Regexptokenizer_ from _nltk_ to tokenize string _b_ whilst removing punctuations. This way you will avoid unnecessary concatenations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. **Use _sent_tokenize_ function from _nltk_ and split string _b_ into sentences. Compare that with the sentence tokenization of string _c_.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization. Imagine you need to count average words per sentence, how you will calculate? For accomplishing such a task, you need both NLTK sentence tokenizer as well as NLTK word tokenizer to calculate the ratio. Such output serves as an important feature for machine training as the answer would be numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11\\. Load a data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12\\. Calculate average word per sentence in the data set. Can you extend this ratio for each document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13\\. Plot wordclounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14\\. Remove stop words and calculate the ratio word per sentence again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15\\. Find all the context-related words in your data set and plot . Let's define a word context-related only if it appears in more than 90 percent of documents. First plot a word cloud of these words in your data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16\\. Modify the list of stop words by adding context-related words. Remove the list of modified stop words from your data and analyze the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17\\. What percentage of documents in the data set are not written in English?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18\\. Find all the dates and create a dataframe of sentences versus dates if any."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

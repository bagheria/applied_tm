{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dELWpyou789C"
      },
      "source": [
        "# Practical 5: Word Embeddings\n",
        "#### Dong Nguyen\n",
        "<img src=\"img/uu_logo.png\" alt=\"logo\" align=\"right\" title=\"UU\" width=\"50\" height=\"20\" />\n",
        "\n",
        "#### Applied Text Mining - Utrecht Summer School"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu6FkzSFrp1f"
      },
      "source": [
        "# Installation\n",
        "\n",
        "If you're using Google Colab, then you should be able to run these commands directly. Otherwise, make sure you have `sklearn`, `matplotlib` and `numpy` installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hX_spMIk9BQF"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from numpy import linalg as LA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgw0Ld6kv9Ku"
      },
      "source": [
        "# First install the `gensim` library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0tEUKDfxJ5Y"
      },
      "source": [
        "In this practical session we're going to use the [`gensim`](https://radimrehurek.com/gensim/) library. This library offers a variety of methods to read\n",
        "in pre-trained word embeddings as well as train your own.\n",
        "\n",
        "The website contains a lot of documentation, for example here: https://radimrehurek.com/gensim/auto_examples/index.html#documentation\n",
        "\n",
        "If gensim isn't installed yet, you can use the following command:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUHdt_DtvrA3"
      },
      "outputs": [],
      "source": [
        "#!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zfk4o1Cebn-J"
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import datapath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKUfWy0nwG0U"
      },
      "source": [
        "# Reading in a pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79Mb6_c9wwvt"
      },
      "source": [
        "First we load in a pre-trained GloVe model. Note: this can take around five minutes.\n",
        "\n",
        "See https://github.com/RaRe-Technologies/gensim-data for an overview of the models you can try. For example\n",
        "\n",
        "*   word2vec-google-news-300: word2vec trained on Google news. 1662 MB.\n",
        "*   glove-twitter-200: trained on Twitter: 758 MB\n",
        "\n",
        "We're going to start with `glove-wiki-gigaword-300` which\n",
        "is 376.1MB to download. These embeddings are trained on\n",
        "Wikipedia (2014) and the Gigaword corpus, a large collection\n",
        "of newswire text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs4LeLkav_xy",
        "outputId": "8e6e6f49-fb7d-4dd3-e2da-9a5bec5a9b55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[=================================================-] 99.7% 374.8/376.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('glove-wiki-gigaword-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91pmPOTX4bmE"
      },
      "source": [
        "# Exploring the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luAJI20EZfVM"
      },
      "source": [
        "How many words does the vocabulary contain?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCYQx2rzamAe"
      },
      "source": [
        "Is '*utrecht*' in the vocabulary?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWqjHy7JaCAq"
      },
      "source": [
        "Print a word embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hYuk_2KaLO-"
      },
      "source": [
        "How many dimensions does this embedding have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02IVqpksawKL"
      },
      "source": [
        "**Question:** Explore the embeddings for a few other words. Can you find words that are *not* in the vocabulary?\n",
        "\n",
        "(For example, think of uncommon words, misspellings, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omCh8m2k1kMd"
      },
      "source": [
        "# Vector arithmethics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmIVSe-c1pOA"
      },
      "source": [
        "We can calculate the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between two words in this way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1vVWsuO1m9G",
        "outputId": "33d4eb2e-d19f-4845-9389-725063fc8aea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5970514"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv.similarity('university', 'student')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB1u026w1y4y"
      },
      "source": [
        "*Optional*: cosine similarity is the same as the dot product between the normalized word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-UHTcX229Qn",
        "outputId": "ada0503f-70f6-4528-e979-6b643e575c92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5970514"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv_university_norm = wv['university']/ LA.norm(wv['university'], 2)\n",
        "wv_student_norm = wv['student'] / LA.norm(wv['student'], 2)\n",
        "\n",
        "wv_university_norm.dot(wv_student_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfd3Gy5O30_U"
      },
      "source": [
        "A normalized embedding has a L2 norm (length) of 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dr0qHc24B5D",
        "outputId": "dd784f03-36f3-4837-eb32-de4212650977"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LA.norm(wv_student_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2qWG2llbX6Q"
      },
      "source": [
        "# Similarity analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSFauHoA4xF9"
      },
      "source": [
        "Print the top 5 most similar words to car"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YljWEl6W44jN"
      },
      "source": [
        "**Question**: What are the top 5 most similar words to *cat*?  And to *king*? And to *fast*? What kind of words often appear in the top?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SVBB1MoOczHp"
      },
      "source": [
        "Now calculate the similarities between two words:\n",
        "\n",
        "* buy, purchase\n",
        "* cat, dog\n",
        "* car, green"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTy98KEIdn1S"
      },
      "source": [
        "We can calculate the cosine similarity between a list of word pairs and correlate these with human ratings. One such dataset with human ratings is called WordSim353.\n",
        "\n",
        "**Goto** https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/wordsim353.tsv to get a sense of the data.\n",
        "\n",
        "\n",
        "Gensim already implements a method to evaluate a word embedding model using this data.\n",
        "* It calculates the cosine similarity between each word pair\n",
        "* It calculates both the Spearman and Pearson correlation coefficient between the cosine similarities and human judgements\n",
        "\n",
        "See https://radimrehurek.com/gensim/models/keyedvectors.html for a description of the methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS7w7qmKcUoh",
        "outputId": "3c229ae0-adaf-473c-a11f-c6e06d59f866"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PearsonRResult(statistic=0.6040760940127656, pvalue=1.752303459427883e-36),\n",
              " SignificanceResult(statistic=0.6085349998820805, pvalue=3.879629536780527e-37),\n",
              " 0.0)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv.evaluate_word_pairs(datapath('wordsim353.tsv'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3hbA_Pd5m2Y"
      },
      "source": [
        "# Analogies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wpOgoulf_mh"
      },
      "source": [
        "Man is to woman as king is to. ..?\n",
        "\n",
        "This can be converted into vector arithmethics:\n",
        "\n",
        "```\n",
        "king - ? = man - woman.\n",
        "\n",
        "king - man + woman = ?\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QpczXwlgUGW"
      },
      "source": [
        "france - paris + amsterdam = ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_qTUAGjgbdP"
      },
      "source": [
        "Note that it we would just retrieve the most similar words to '*amsterdam*' we would receive a different result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plZLOUP2giIk"
      },
      "source": [
        "cat is to cats as girl is to ?\n",
        "\n",
        "```\n",
        "girl - ? = cat - cats\n",
        "girl - cat + cats = ?\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fsH7_DEh8Zt"
      },
      "source": [
        "Compare against a baseline. What if we would just have retrieved the most similar words to '*girl*'?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdTPXa2M8svY"
      },
      "source": [
        "**Question**: Try a few of your own analogies, do you get the expected answer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z_2LyUAiHm8"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDnfe0x_9izp"
      },
      "source": [
        "We can't visualize embeddings in their raw format, because of their\n",
        "high dimensionality. However, we can use dimensionality reduction\n",
        "techniques such as PCA to project them onto a 2D space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOVm0bj8AKtY"
      },
      "source": [
        "**Question**: What do you notice in this plot? Do the distances between the words make sense? Any surprises? Feel free to add your own words!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viKrIg1LAlnW"
      },
      "source": [
        "# Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m02uJ_taNhbk"
      },
      "source": [
        "Is *math* more associated with male or female words?\n",
        "\n",
        "Compute the average cosine similarity between the target word\n",
        "and the set of attribute words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLBpdjSON1UW"
      },
      "source": [
        "What about *poetry*?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fof7yee6KNz4"
      },
      "source": [
        "#Next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYkoAJXvKb4U"
      },
      "source": [
        "Repeat this analysis but now with the GloVe model trained on Twitter data,\n",
        "e.g. `glove-twitter-50`.\n",
        "\n",
        "* Which model obtains better performance on the word similarity task? (WordSim353?)\n",
        "* What other differences do you observe? (e.g. think of the vocabulary, biases, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg5NPxBoOtia"
      },
      "source": [
        "# FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKg2ssDeRi93"
      },
      "source": [
        "(only if you have time, this can take a while (11 min?))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiQ9AbR6RmFc"
      },
      "source": [
        "Load in a fastText model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0OSH3WpOute",
        "outputId": "99519001-d4ff-48f0-ad54-5812d00b86a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[=================================================-] 100.0% 958.0/958.4MB downloaded\n"
          ]
        }
      ],
      "source": [
        "wv_f = api.load('fasttext-wiki-news-subwords-300')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tL4pC2stRuNE"
      },
      "source": [
        "Usage is very similar to before. For example, we can calculate the similarity\n",
        "between two words:\n",
        "\n",
        "* dog, dogs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvADpGHIR2GB"
      },
      "source": [
        "**Question** How does this compare to the similarity scores you obtained\n",
        "with the other models you tried?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

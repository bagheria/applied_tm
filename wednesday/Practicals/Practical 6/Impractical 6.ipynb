{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw-nOs7K7BtQ"
      },
      "source": [
        "# Practical 6: Deep Learning for Multiclass Text Classification\n",
        "#### Ayoub Bagheri\n",
        "<img src=\"img/uu_logo.png\" alt=\"logo\" align=\"right\" title=\"UU\" width=\"50\" height=\"20\" />\n",
        "\n",
        "#### Applied Text Mining - Utrecht Summer School"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdGWs-9Z7BtT"
      },
      "source": [
        "In this practical, we will apply various deep learning models for multiclass text classification. We will work with the famous 20 Newsgroups dataset from the `sklearn` library and apply deep learning models using the `keras` library.\n",
        "\n",
        "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. It was originally collected by Ken Lang, and it has become a popular data set for experiments in text applications of machine learning techniques.\n",
        "\n",
        "Also, we will use the `keras` library, which is a deep learning and neural networks API by Fran√ßois Chollet's team capable of running on top of Tensorflow (Google), Theano or CNTK (Microsoft)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnM7UAKh7BtT"
      },
      "source": [
        "Today we will use the following libraries. Take care to have them installed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZr3d7EM7BtU"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras import layers, utils\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL-8UbJ97BtU"
      },
      "source": [
        "### Let's get started!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng1HxqXS7BtU"
      },
      "source": [
        "1\\. **Load the train and test subsets of the 20 Newsgroups data set from `sklearn` datasets. Remove the headers, footers and qoutes from the news article when loading data sets. Use number 321 for `random_state`.**\n",
        "**In order to get faster execution times for this practical we will work on a partial data set with only 5 categories out of the 20 available in the data set: `'rec.sport.hockey'`, `'talk.politics.mideast'`, `'soc.religion.christian'`, `'comp.graphics'`, and `'sci.med'`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-oBGDEK7BtV"
      },
      "source": [
        "2\\. **Find out about the number of news articles in the train and test sets.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8HjyJbL7BtY"
      },
      "source": [
        "3\\. **Covert the train and test to dataframes.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4cJ2eNL7BtZ"
      },
      "source": [
        "### Train a neural network a with document-term matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH99Eyqk7BtZ"
      },
      "source": [
        "4\\. **In order to feed predictive deep learning models with text data, first you need to turn the text into vectors of numerical values suitable for statistical analysis. Use the binary representation with `TfidfVectorizer` and create document-term matrices for test and train (name them `X_train` and `X_test`).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIgRkX-S7Bta"
      },
      "source": [
        "5\\. **Use the `LabelEncoder` to create `y_train` and `y_test` from `df_train.label.values` and `df_test.label.values`, respectively.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODTSpeUx7Btb"
      },
      "source": [
        "6\\. **Use the sequential API in `keras` and create a one-hidden-layer neural network. So, the first layer will be the input layer with the number of features in your X_train, followed by a single hidden layer, and an output layer. Set the number of neurons in the hidden layer to 5, and activation function as `relu`. For the output layer you can use a `softmax` activation function.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqbWpEjR7Btb"
      },
      "source": [
        "The sequential API (https://www.tensorflow.org/guide/keras/sequential_model) allows you to create models layer by layer. It is limited in that it does not allow to create models that share layers or have multiple inputs or outputs.\n",
        "\n",
        "The functional API (https://www.tensorflow.org/guide/keras/functional) allows you to create models that have a lot more flexibility as you can define models where layers connect to more than just the previous and next layers. In this way, you can connect layers to (literally) any other layer. As a result, creating complex networks such as Siamese neural networks and residual neural networks become possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVOswMnx7Btc"
      },
      "source": [
        "7\\. **The `compile` function defines the `loss` function, the `optimizer` and the evaluation `metrics`. Call this function for your neural network model with `loss='binary_crossentropy'`, `optimizer='adam'`, `metrics=['accuracy']`. Check the summary of the model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J4RDUKg7Btc"
      },
      "source": [
        "| Task           | Output type                       | Last-layer activation | Loss function            | Metric(s) |\n",
        "| ---            | ---                               | ---                   | ---                      | ---       |\n",
        "| Regression     | Numerical                         | Linear                | meanSquaredError (MSE),<br> meanAbsoluteError (MAE) | Same as loss |\n",
        "| Classification | Binary                            | Sigmoid               | binary_crossentropy      | Accuracy, precision, recall, sensitivity, <br> TPR, FPR, ROC, AUC |\n",
        "| Classification | Single label, Multiple classes    | Softmax               | categorical_crossentropy | Accuracy, confusion matrix |\n",
        "| Classification | Multiple labels, Multiple classes | Sigmoid               | binary_crossentropy      | Accuracy, precision, recall, sensitivity, <br> TPR, FPR, ROC, AUC |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzZthw6z7Btc"
      },
      "source": [
        "8\\. **Time to train your model! Train your model in 20 iterations. What does `batch_size` represent?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMvt-_yq7Btd"
      },
      "source": [
        "Note that if you rerun the `fit()` method, you will start off with the computed weights from the previous training. Make sure to call `clear_session()` before you start training the model again:\n",
        "\n",
        "<br>\n",
        "\n",
        "**`from keras.backend import clear_session` <br>\n",
        "`clear_session()`**\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2Db0lQN7Btd"
      },
      "source": [
        "9\\. **Plot the accuracy and loss of your trained model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__SQlBJn7Bte"
      },
      "source": [
        "10\\. **Evaluate the accuracy of your trained model on the test set. Compare that with the accuracy on train.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkYntDUZ7Bte"
      },
      "source": [
        "# The embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SvzxqH37Btf"
      },
      "source": [
        "11\\. **Use the tokenizer from Keras with 20,000 words and create `X-train` and `X_test` sequences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyv-P1FY7Btg"
      },
      "source": [
        "12\\. **Use the `pad_sequence()` function to pad each text sequence with zeros, so that each vector has the same length of 100 words.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDFK8yUO7Bth"
      },
      "source": [
        "13\\. **Now it is time to create a neural network model using an embedding layer as input. Take the output of the embedding layer (`embedding_dim = 50`) and plug it into a Dense layer with 10 neurons, and the `relu` activation function. In order to do this, you have to add a Flatten layer in between that prepares the sequential input for the Dense layer. Note that in the Embedding layer, `input_dim` is the size of the vocabulary, `output_dim` is the size of the embedding vector, and `input_length` is the length of the text sequence.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRhP8Qt17Bti"
      },
      "source": [
        "# Pretrained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmE_ubpj7Bti"
      },
      "source": [
        "14\\. **Pretrained word embeddings are the embeddings learned in one task that are used for solving another similar task. These embeddings are trained on large data sets, saved, and then used for solving other tasks. Here, we are going to use the GloVe embeddings which are precomputed word embeddings simply trained on a large corpus of text. For this purpose, we wrote the following fuction to apply on the pretrained word embeddings and use the corresponding word vectors for words in our vocabulary. Download one of the GloVe embeddings (e.g. `glove.6B.50d.txt`) and create the embedding matrix using the provided function. (Link to download: https://nlp.stanford.edu/projects/glove/)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U5qhCIE7Bti"
      },
      "source": [
        "15\\. **Build your previous neural network model again, but this time with the initial weights from the pretrained word embeddings. Set the `trainable` argument `False` so that your embedding layer does not learn the word vectors anymore, and then again back to `True`. How does the performances change?**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

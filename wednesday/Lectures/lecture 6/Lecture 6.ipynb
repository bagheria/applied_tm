{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning for Text 1 \n",
    "&nbsp;\n",
    "\n",
    "\n",
    "Ayoub Bagheri, <a.bagheri@uu.nl>  \n",
    "\n",
    "<img src=\"img/uu_logo.png\" style=\"float: right;\" width=\"100\" height=\"100\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture’s Plan\n",
    "&nbsp;\n",
    "\n",
    "1. Feed-forward neural net\n",
    "2. Recurrent neural net\n",
    "    1. SRN\n",
    "    2. LSTM\n",
    "    3. GRU\n",
    "    4. Bi-LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Deep Learning (DL) ? \n",
    "&nbsp;\n",
    "\n",
    "A machine learning subfield of learning <span style=\"color:orange;\">representations</span> of data. Exceptional effective at <span style=\"color:orange;\">learning patterns</span>.\n",
    "\n",
    "Deep learning algorithms attempt to learn (multiple levels of) representation by using a <span style=\"color:orange;\">hierarchy of multiple layers</span>\n",
    "\n",
    "If you provide the system <span style=\"color:orange;\">tons of information</span>, it begins to understand it and respond in useful ways.\n",
    "\n",
    "<center><img src=\"img/page 3.png\" width=\"500\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-Layer Feed-Forward Networks\n",
    "&nbsp;\n",
    "\n",
    "- Multi-layer networks can represent arbitrary functions, but an effective learning algorithm for such networks was thought to be difficult.\n",
    "- A typical multi-layer network consists of an input, hidden and output layer, each fully connected to the next, with activation feeding forward.\n",
    "\n",
    "<center><img src=\"img/page 4.png\" width=\"500\"></center>\n",
    "\n",
    "- The weights determine the function computed. Given an arbitrary number of hidden units, any boolean function can be computed with a single hidden layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network Intro\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "    <img src=\"img/page 5_1.png\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "    <span style=\"color:blue\">$$h = \\sigma(W_1x + b_1)$$</span>\n",
    "    <span style=\"color:green\">$$y = \\sigma(W_2h + b_2)$$</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Network Intro\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "    <img src=\"img/page 5_1.png\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "    <span style=\"color:blue\">$$h = \\sigma(W_1x + b_1)$$</span>\n",
    "    <span style=\"color:green\">$$y = \\sigma(W_2h + b_2)$$</span>\n",
    "    <br> <br> <br> <br>\n",
    "    \n",
    "4 + 2 = 6 neurons (not counting inputs)\n",
    "<br>\n",
    "    \n",
    "[3 x 4] + [4 x 2] = 20 weights \n",
    "<br>\n",
    "    \n",
    "4 + 2 = 6 biases\n",
    "<br>\n",
    "    \n",
    "<span style=\"color:orange\">26 learnable <b>parameters</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Network Intro\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 5_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Network Intro\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 5_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Network Intro\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 5_4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training\n",
    "&nbsp;\n",
    "\n",
    "<center><span style=\"font-size: 12px\">https://medium.com/@ramrajchandradevan/the-evolution-of-gradient-descend-optimization-algorithm-4106a6702d39</span></center>\n",
    "\n",
    "<center><img src=\"img/page 6_1.png\" width=\"700\"></center>\n",
    "\n",
    "<div style=\"float:left;width:75%\">\n",
    "Optimize (min. or max.) <span style=\"color:orange;font-weight:bold\">objective/cost function <b>$J$</b>$(\\theta)$</span>\n",
    "<br>\n",
    "\n",
    "Generate <span style=\"color:orange;font-weight:bold\">error signal</span> that measures difference between predictions and target values\n",
    "<br> &nbsp; \n",
    "    \n",
    "<br> &nbsp;\n",
    "\n",
    "</div>\n",
    "<div style=\"float:right;width:25%\">\n",
    "<img src=\"img/page 6_2.gif\" width=\"300\">\n",
    "</div>\n",
    "<br> <br>\n",
    "\n",
    "<div style=\"float:left;width:30%\">\n",
    "<img src=\"img/page 6_3.png\" width=\"300\">\n",
    "</div>\n",
    "<div style=\"float:right;width:70%\">\n",
    "Use error signal to change the <span style=\"color:orange;font-weight:bold\">weights</span> and get more accurate predictions \n",
    "<br> <br>\n",
    "    \n",
    "Subtracting a fraction of the <span style=\"color:orange;font-weight:bold\">gradient</span> moves you towards the <span style=\"color:orange;font-weight:bold\">(local) minimum of the cost function</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "&nbsp;\n",
    "\n",
    "- Define objective to minimize error:\n",
    "$$E(W) = \\sum_{d \\in D} \\sum_{k \\in K} (t_{kd} - O_{kd})^2$$\n",
    "\n",
    "    where $D$ is the set of training examples, $K$ is the set of output units, $t_{kd}$ and $o_{kd}$ are, respectively, the teacher and current output for unit $k$ for example $d$.\n",
    "- The derivative of a sigmoid unit with respect to net input is:\n",
    "$$\\frac{\\partial{o_j}}{\\partial{net_j}} = o_j(1-o_j)$$    \n",
    "- Learning rule to change weights to minimize \n",
    "$$\\Delta w_{ji} = - \\eta \\frac{\\partial{E}}{\\partial{w_{ji}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation Learning Rule\n",
    "&nbsp;\n",
    "\n",
    "- Each weight changed by:\n",
    "$$\\Delta w_{ji} = \\eta \\delta_j o_i $$\n",
    "$$\\delta_j = o_j(1 - o_j)(t_j - o_j) \\ \\ \\ \\text{if } j \\text{ is an output unit}$$\n",
    "$$\\delta_j = o_j(1 - o_j)\\sum_k{\\delta_k w_{kj}} \\ \\ \\ \\text{if } j \\text{ is a hidden unit}$$\n",
    "    \n",
    "    where $\\eta$ is a constant called the learning rate\n",
    "    \n",
    "    $t_j$ is the correct teacher output for unit $j$\n",
    "    \n",
    "    $\\delta_j$ is the error measure for unit $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "&nbsp;\n",
    "\n",
    "<span style=\"color:orange;font-weight:bold\">objective/cost function <b>$J$</b>$(\\theta)$</span>\n",
    "\n",
    "Update each element of $\\theta$:\n",
    "\n",
    "$$\\theta^{new}_j = \\theta^{old}_j - \\alpha \\frac{d}{\\theta^{old}_j} J(\\theta)$$ \n",
    "\n",
    "Matrix notation for all parameters (<span style=\"color:red\">$\\alpha$: learning rate</span>):\n",
    "\n",
    "$$\\theta^{new}_j = \\theta^{old}_j - \\alpha \\nabla _{\\theta}J(\\theta)$$  \n",
    "\n",
    "<center><img src=\"img/page 9.png\" width=\"300\"></center>\n",
    "\n",
    "Recursively apply <span style=\"color:orange;font-weight:bold\">chain rule</span> though each node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "&nbsp;\n",
    "\n",
    "<span style=\"color:orange;font-weight:bold\">objective/cost function <b>$J$</b>$(\\theta)$</span> &nbsp;&nbsp;&nbsp;&nbsp; <span style=\"color:darkred\"><u>Review of backpropagation</u></span>\n",
    "\n",
    "Update each element of $\\theta$:\n",
    "\n",
    "$$\\theta^{new}_j = \\theta^{old}_j - \\alpha \\frac{d}{\\theta^{old}_j} J(\\theta)$$ \n",
    "\n",
    "Matrix notation for all parameters (<span style=\"color:red\">$\\alpha$: learning rate</span>):\n",
    "\n",
    "$$\\theta^{new}_j = \\theta^{old}_j - \\alpha \\nabla _{\\theta}J(\\theta)$$  \n",
    "\n",
    "<center><img src=\"img/page 9.png\" width=\"300\"></center>\n",
    "\n",
    "Recursively apply <span style=\"color:orange;font-weight:bold\">chain rule</span> though each node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>First calculate error of output units and use this to change the top layer of weights.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 10_1.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>First calculate error of output units and use this to change the top layer of weights.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "Current output: $o_j=0.2$\n",
    "\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 10_1.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>First calculate error of output units and use this to change the top layer of weights.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "Current output: $o_j=0.2$\n",
    "<br>\n",
    "    \n",
    "Correct output: $t_j=1.0$\n",
    "    \n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 10_1.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>First calculate error of output units and use this to change the top layer of weights.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "Current output: $o_j=0.2$\n",
    "<br>\n",
    "    \n",
    "Correct output: $t_j=1.0$\n",
    "<br>\n",
    "    \n",
    "Error $\\delta_j = o_j(1–o_j)(t_j–o_j)$\n",
    "<br>\n",
    "    \n",
    "$0.2(1–0.2)(1–0.2)=0.128$\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 10_1.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>First calculate error of output units and use this to change the top layer of weights.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "Current output: $o_j=0.2$\n",
    "<br>\n",
    "    \n",
    "Correct output: $t_j=1.0$\n",
    "<br>\n",
    "    \n",
    "Error $\\delta_j = o_j(1–o_j)(t_j–o_j)$\n",
    "<br>\n",
    "    \n",
    "$0.2(1–0.2)(1–0.2)=0.128$\n",
    "<br>\n",
    "    \n",
    "Update weights into $j$\n",
    "    \n",
    "$$\\Delta w_{ji} = \\eta \\delta_j o_i$$\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 10_1.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>First calculate error of output units and use this to change the top layer of weights.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "Current output: $o_j=0.2$\n",
    "<br>\n",
    "    \n",
    "Correct output: $t_j=1.0$\n",
    "<br>\n",
    "    \n",
    "Error $\\delta_j = o_j(1–o_j)(t_j–o_j)$\n",
    "<br>\n",
    "    \n",
    "$0.2(1–0.2)(1–0.2)=0.128$\n",
    "<br>\n",
    "    \n",
    "Update weights into $j$\n",
    "<br>  \n",
    "\n",
    "$$\\Delta w_{ji} = \\eta \\delta_j o_i$$\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 10_2.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>First calculate error of output units and use this to change the top layer of weights.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "Current output: $o_j=0.2$\n",
    "<br>  \n",
    "\n",
    "Correct output: $t_j=1.0$\n",
    "<br>\n",
    "    \n",
    "Error $\\delta_j = o_j(1–o_j)(t_j–o_j)$\n",
    "<br>\n",
    "    \n",
    "$0.2(1–0.2)(1–0.2)=0.128$\n",
    "<br>\n",
    "    \n",
    "Update weights into $j$\n",
    "<br>\n",
    "    \n",
    "$$\\Delta w_{ji} = \\eta \\delta_j o_i$$\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 10_3.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>Next calculate error for hidden units based on errors on the output units it feeds into.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 11_1.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>Next calculate error for hidden units based on errors on the output units it feeds into.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "\n",
    "$$\\delta_j = o_j(1-o_j)\\sum_k{\\delta_kw_{kj}}$$\n",
    "    \n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 11_1.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>Next calculate error for hidden units based on errors on the output units it feeds into.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "\n",
    "$$\\delta_j = o_j(1-o_j)\\sum_k{\\delta_kw_{kj}}$$\n",
    "    \n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 11_2.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>Next calculate error for hidden units based on errors on the output units it feeds into.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "\n",
    "$$\\delta_j = o_j(1-o_j)\\sum_k{\\delta_kw_{kj}}$$\n",
    "    \n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 11_3.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>Finally update bottom layer of weights based on errors calculated for hidden units.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "\n",
    "$$\\delta_j = o_j(1-o_j)\\sum_k{\\delta_kw_{kj}}$$ \n",
    "\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 12_1.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>Finally update bottom layer of weights based on errors calculated for hidden units.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "\n",
    "$$\\delta_j = o_j(1-o_j)\\sum_k{\\delta_kw_{kj}}$$ \n",
    "\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 12_2.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>Finally update bottom layer of weights based on errors calculated for hidden units.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "\n",
    "$$\\delta_j = o_j(1-o_j)\\sum_k{\\delta_kw_{kj}}$$\n",
    "\n",
    "Update weights into $j$\n",
    "    \n",
    "$$\\Delta w_{ji} = \\eta \\delta_j o_i$$    \n",
    "\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 12_2.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error Backpropagation\n",
    "&nbsp;\n",
    "\n",
    "<ul><li>Finally update bottom layer of weights based on errors calculated for hidden units.</li></ul>\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "\n",
    "$$\\delta_j = o_j(1-o_j)\\sum_k{\\delta_kw_{kj}}$$\n",
    "\n",
    "Update weights into $j$\n",
    "    \n",
    "$$\\Delta w_{ji} = \\eta \\delta_j o_i$$    \n",
    "\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "<img src=\"img/page 12_3.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation Training Algorithm\n",
    "&nbsp;\n",
    "\n",
    "- Create the 3-layer network with $H$ hidden units with full connectivity between layers. Set weights to small random real values.\n",
    "\n",
    "- Until all training examples produce the correct value (within $\\epsilon$), or mean squared error ceases to decrease, or other termination criteria:\n",
    "    \n",
    "    - Begin epoch\n",
    "       \n",
    "    - For each training example, $d$, do:\n",
    "\n",
    "        - Calculate network output for $d$’s input values \n",
    "        - Compute error between current output and correct output for $d$\n",
    "        - Update weights by backpropagating error and using learning rule\n",
    "    - End epoch\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comments on Training Algorithm\n",
    "&nbsp;\n",
    "\n",
    "- Not guaranteed to converge to zero training error, may converge to local optima or oscillate indefinitely.\n",
    "- However, in practice, does converge to low error for many large networks on real data.\n",
    "- Many epochs (thousands) may be required, hours or days of training for large networks.\n",
    "- To avoid local-minima problems, run several trials starting with different random weights (*random restarts*).\n",
    "    - Take results of trial with lowest training set error.\n",
    "    - Build a committee of results from multiple trials (possibly weighting votes by training set accuracy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hidden Unit Representations\n",
    "&nbsp;\n",
    "\n",
    "- Trained hidden units can be seen as newly constructed features that make the target concept linearly separable in the transformed space.\n",
    "- On many real domains, hidden units can be interpreted as representing meaningful features such as vowel detectors or edge detectors, etc..\n",
    "- However, the hidden layer can also become a distributed representation of the input in which each individual unit is not easily interpretable as a meaningful feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One forward pass\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 16.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation functions\n",
    "&nbsp;\n",
    "\n",
    "Non-linearities needed to learn complex (non-linear) representations of data, otherwise the NN would be just a linear function $W_1W_2x = Wx$\n",
    "\n",
    "<center><img src=\"img/page 17.png\" width=\"500\"></center>\n",
    "\n",
    "More layers and neurons can approximate more complex functions\n",
    "\n",
    "Full list: https://en.wikipedia.org/wiki/Activation_function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation: Sigmoid\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "<img src=\"img/page 18.png\" width=\"250\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "Takes a real-valued number and “squashes” it into range between 0 and 1. \n",
    "$$R^n \\rightarrow [0,1]$$\n",
    "</div>\n",
    "\n",
    "- $+$ Nice interpretation as the <span style=\"color:orange;font-weight:bold\">firing rate</span> of a neuron\n",
    "    - 0 = not firing at all \n",
    "    - 1 = fully firing\n",
    "\n",
    "- $-$ Sigmoid neurons <span style=\"color:orange;font-weight:bold\">saturate</span> and <span style=\"color:orange;font-weight:bold\">kill gradients</span>, thus NN will barely learn\n",
    "    - when the neuron’s activation are 0 or 1 (saturate)\n",
    "        - gradient at these regions almost zero \n",
    "        - almost no signal will flow to its weights \n",
    "        - if initial weights are too large then most neurons would saturate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation: Tanh\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "<img src=\"img/page 19.png\" width=\"300\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "Takes a real-valued number and “squashes” it into range between -1 and 1. \n",
    "$$R^n \\rightarrow [-1,1]$$\n",
    "</div>\n",
    "\n",
    "- Like sigmoid, tanh neurons <span style=\"color:orange;font-weight:bold\">saturate</span>\n",
    "- Unlike sigmoid, output is <span style=\"color:orange;font-weight:bold\">zero-centered</span>\n",
    "- Tanh is a <span style=\"color:orange;font-weight:bold\">scaled sigmoid</span>: $tanh⁡(x)=2sigm(2x)−1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation: Tanh\n",
    "&nbsp;\n",
    "\n",
    "Most Deep Networks use ReLU nowadays \n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "<img src=\"img/page 20.png\" width=\"250\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "Takes a real-valued number and thresholds it at zero $f(x) = max(0,x)$\n",
    "$$R^n \\rightarrow R^n_+$$\n",
    "</div>\n",
    "\n",
    "- $+$ Trains much <span style=\"color:orange;font-weight:bold\">faster</span>\n",
    "    - accelerates the convergence of SGD\n",
    "    - due to linear, non-saturating form \n",
    "- $+$ Less expensive operations\n",
    "    - compared to sigmoid/tanh (exponentials etc.)\n",
    "    - implemented by simply thresholding a matrix at zero\n",
    "- $+$ More <span style=\"color:orange;font-weight:bold\">expressive </span>\n",
    "- $+$ Prevents the <span style=\"color:orange;font-weight:bold\">gradient vanishing problem</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting\n",
    "&nbsp;\n",
    "\n",
    "Learned hypothesis may <span style=\"color:orange;font-weight:bold\">fit</span> the training data very well, even outliers (<span style=\"color:orange;font-weight:bold\">noise</span>) but fail to <span style=\"color:orange;font-weight:bold\">generalize</span> to new examples (test data)\n",
    "\n",
    "<div style=\"float:left;width:50%\">\n",
    "    <img src=\"img/page 21_1.png\" width=\"500\">\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "    <img src=\"img/page 21_2.png\" width=\"500\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting Prevention\n",
    "&nbsp;\n",
    "\n",
    "- Running too many epochs can result in over-fitting.\n",
    "\n",
    "<center><img src=\"img/page 22.png\"></center>\n",
    "\n",
    "- Keep a hold-out validation set and test accuracy on it after every epoch. Stop training when additional epochs actually increase validation error.\n",
    "- To avoid losing training data for validation:\n",
    "    - Use internal 10-fold CV on the training set to compute the average number of epochs that maximizes generalization accuracy.\n",
    "    - Train final network on complete training set for this many epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left;width:60%\">\n",
    "<span style=\"font-weight:bold;color:orange\">Dropout</span>\n",
    "<ul>\n",
    "    <li>Randomly drop units (along with their connections) during training</li>\n",
    "    <li>Each unit retained with fixed probability $p$, independent of other units </li>\n",
    "    <li><span style=\"color:orange\">Hyper-parameter</span> $p$ to be chosen (tuned)</li>\n",
    "</ul>\n",
    "</div>\n",
    "<div style=\"float:right;width:40%\">\n",
    "<img src=\"img/page 23.png\" width=\"500\">    \n",
    "</div>\n",
    "\n",
    "<br> <br> <br> <br> \n",
    "\n",
    "<span style=\"font-weight:bold;color:orange\">L2 = weight decay</span>\n",
    "<ul>\n",
    "    <li>Regularization term that penalizes big weights, added to the objective $J_{reg}(\\theta) = J(\\theta) + \\lambda\\sum_k{\\theta_k^2}$ </li>\n",
    "    <li>Weight decay value determines how dominant regularization is during gradient computation</li>\n",
    "    <li>Big weight decay coefficient &rarr big penalty for big weights</li>\n",
    "</ul>\n",
    "\n",
    "<span style=\"font-weight:bold;color:orange\">Early-stopping</span>\n",
    "<ul>\n",
    "    <li>Use validation error to decide when to stop training</li>\n",
    "    <li>Stop when monitored quantity has not improved after n subsequent epochs</li>\n",
    "    <li>$n$ is called patience</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tuning hyper-parameters\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"float:left;width:60%\">\n",
    "$$g(x) \\approx g(x) + h(y)$$\n",
    "    \n",
    "$g(x)$ shown in green\n",
    "\n",
    "$h(y)$ is shown in yellow\n",
    "</div>\n",
    "<div style=\"float:right;width:40%\">\n",
    "<img src=\"img/page 24.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "“Grid and random search of 9 trials for optimizing function $g(x) \\approx g(x) + h(y)$\n",
    "With grid search, nine trials only test g(x) in three distinct places. \n",
    "With random search, all nine trials explore distinct values of $g$. ”\n",
    "<br>\n",
    "\n",
    "Both try configurations randomly and <span style=\"font-weight:bold;color:orange\">blindly</span>\n",
    "\n",
    "Next trial is independent to all the trials done before\n",
    "<br>\n",
    "\n",
    "<span style=\"font-weight:bold;color:darkred\"><u>Bayesian optimization for hyper-parameter tuning:</u></span>\n",
    "\n",
    "Make smarter choice for the next trial, minimize the number of trials\n",
    "\n",
    "<ol>\n",
    "    <li>Collect the performance at several configurations</li>\n",
    "    <li>Make inference and decide what configuration to try next</li>\n",
    "</ol>\n",
    "\n",
    "<span style=\"color:darkred\"><u>Library available!</u></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss functions and output\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 25.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Determining the Best <br> Number of Hidden Units\n",
    "&nbsp;\n",
    "\n",
    "- Too few hidden units prevents the network from adequately fitting the data.\n",
    "- Too many hidden units can result in over-fitting.\n",
    "\n",
    "<center><img src=\"img/page 26.png\" width=\"500\"></center>\n",
    "\n",
    "- Use internal cross-validation to empirically determine an optimal number of hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks (RNN)\n",
    "&nbsp;\n",
    "\n",
    "- Add feedback loops where some units’ current outputs determine some future network inputs.\n",
    "- RNNs can model dynamic finite-state machines, beyond the static combinatorial circuits modeled by feed-forward networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Recurrent Network (SRN)\n",
    "&nbsp;\n",
    "\n",
    "- Initially developed by Jeff Elman (“*Finding structure in time*,” 1990).\n",
    "- Additional input to hidden layer is the state of the hidden layer in the previous time step.\n",
    "\n",
    "<center><img src=\"img/page 29.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unrolled RNN\n",
    "&nbsp;\n",
    "\n",
    "- Behavior of RNN is perhaps best viewed by “unrolling” the network over time.\n",
    "\n",
    "<center><img src=\"img/page 30.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training RNN’s\n",
    "&nbsp;\n",
    "\n",
    "- RNNs can be trained using “backpropagation through time.”\n",
    "- Can viewed as applying normal backprop to the unrolled network.\n",
    "\n",
    "<center><img src=\"img/page 31.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vanishing/Exploding Gradient Problem\n",
    "&nbsp;\n",
    "\n",
    "- Backpropagated errors multiply at each layer, resulting in exponential decay (if derivative is small) or growth (if derivative is large).\n",
    "- Makes it very difficult train deep networks, or simple recurrent networks over many time steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Long Distance Dependencies\n",
    "&nbsp;\n",
    "\n",
    "- It is very difficult to train SRNs to retain information over many time steps\n",
    "- This make is very difficult to learn SRNs that handle long-distance dependencies, such as subject-verb agreement.\n",
    "\n",
    "<img src=\"img/page 34.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Long Short Term Memory\n",
    "&nbsp;\n",
    "\n",
    "- LSTM networks, add additional gating units in each memory cell.\n",
    "    - Forget gate\n",
    "    - Input gate\n",
    "    - Output gate\n",
    "- Prevents vanishing/exploding gradient problem and allows network to retain state information over longer periods of time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM Network Architecture\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"img/page 36.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cell State \n",
    "&nbsp;\n",
    "\n",
    "- Maintains a vector $C_t$ that is the same dimensionality as the hidden state, $h_t$\n",
    "- Information can be added or deleted from this state vector via the forget and input gates.\n",
    "\n",
    "<img src=\"img/page 37.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cell State Example\n",
    "&nbsp;\n",
    "\n",
    "- Want to remember person & number of a subject noun so that it can be checked to agree with the person & number of verb when it is eventually encountered.\n",
    "- Forget gate will remove existing information of a prior subject when a new one is encountered.\n",
    "- Input gate \"adds\" in the information for the new subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forget Gate\n",
    "&nbsp;\n",
    "\n",
    "- Forget gate computes a 0-1 value using a logistic sigmoid output function from the input, $x_t$, and the current hidden state, $h_t$:\n",
    "- Multiplicatively combined with cell state, \"forgetting\" information where the gate outputs something close to 0.\n",
    "\n",
    "<center><img src=\"img/page 39.png\" width=\"500\"></center>\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperbolic Tangent Units\n",
    "&nbsp;\n",
    "\n",
    "- Tanh can be used as an alternative nonlinear function to the sigmoid logistic (0-1) output function.\n",
    "- Used to produce thresholded output between –1 and 1.\n",
    "\n",
    "<center><img src=\"img/page 40.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Input Gate\n",
    "&nbsp;\n",
    "\n",
    "- First, determine which entries in the cell state to update by computing 0-1 sigmoid output.\n",
    "- Then determine what amount to add/subtract from these entries by computing a tanh output (valued –1 to 1) function of the input and hidden state.\n",
    "\n",
    "<center><img src=\"img/page 41.png\" width=\"500\"></center>\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "$$\\tilde{C}_t = tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Updating the Cell State\n",
    "&nbsp;\n",
    "\n",
    "- Cell state is updated by using component-wise vector multiply to \"forget\" and vector addition to \"input\" new information.\n",
    "\n",
    "<center><img src=\"img/page 42.png\" width=\"500\"></center>\n",
    "\n",
    "$$C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Output Gate\n",
    "&nbsp;\n",
    "\n",
    "- Hidden state is updated based on a \"filtered\" version of the cell state, scaled to –1 to 1 using tanh.\n",
    "- Output gate computes a sigmoid function of the input and current hidden state to determine which elements of the cell state to \"output\".\n",
    "\n",
    "<center><img src=\"img/page 43.png\" width=\"500\"></center>\n",
    "\n",
    "$$o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o)$$\n",
    "$$h_t = o_t * tanh(C_t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overall Network Architecture\n",
    "&nbsp;\n",
    "\n",
    "- Single or multilayer networks can compute LSTM inputs from problem inputs and problem outputs from LSTM outputs.\n",
    "\n",
    "<img src=\"img/page 44.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM Training\n",
    "&nbsp;\n",
    "\n",
    "- Trainable with backprop derivatives such as:\n",
    "    - Stochastic gradient descent (randomize order of examples in each epoch) with momentum (bias weight changes to continue in same direction as last update).\n",
    "    - ADAM optimizer (Kingma & Ma, 2015)\n",
    "- Each cell has many parameters ($W_f$, $W_i$, $W_C$, $W_o$)\n",
    "    - Generally requires lots of training data.\n",
    "    - Requires lots of compute time that exploits GPU clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Problems Solved with LSTMs\n",
    "&nbsp;\n",
    "\n",
    "- Sequence labeling \n",
    "    - Train with supervised output at each time step computed using a single or multilayer network that maps the hidden state ($h_t$) to an output vector ($O_t$).\n",
    "- Language modeling\n",
    "    - Train to predict next input ($O_t =I_{t+1}$)\n",
    "- Sequence (e.g. text) classification\n",
    "    - Train a single or multilayer network that maps the final hidden state ($h_n$) to an output vector ($O$). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence to Sequence <br> Transduction (Mapping)\n",
    "&nbsp;\n",
    "\n",
    "- Encoder/Decoder framework maps one sequence to a \"deep vector\" then another LSTM maps this vector to an output sequence. \n",
    "\n",
    "<center><img src=\"img/page 47.png\"></center>\n",
    "\n",
    "- Train model \"end to end\" on I/O pairs of sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of <br> LSTM Application Architectures\n",
    "\n",
    "<img src=\"img/page 48.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Successful Applications of LSTMs\n",
    "&nbsp;\n",
    "\n",
    "- Speech recognition: Language and acoustic modeling\n",
    "- Sequence labeling\n",
    "    - POS Tagging \n",
    "    \n",
    "    https://www.aclweb.org/aclwiki/index.php?title=POS_Tagging_(State_of_the_art)\n",
    "    \n",
    "    - NER\n",
    "    - Phrase Chunking \n",
    "- Neural syntactic and semantic parsing\n",
    "- Image captioning: CNN output vector to sequence\n",
    "- Sequence to Sequence\n",
    "    - Machine Translation (Sustkever, Vinyals, & Le, 2014)\n",
    "    - Video Captioning (input sequence of CNN frame outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bi-directional LSTM (Bi-LSTM)\n",
    "&nbsp;\n",
    "\n",
    "- Separate LSTMs process sequence forward and backward and hidden layers at each time step are concatenated to form the cell output.\n",
    "\n",
    "<img src=\"img/page 50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gated Recurrent Unit (GRU)\n",
    "- Alternative RNN to LSTM that uses fewer gates (Cho, et al., 2014)\n",
    "    - Combines forget and input gates into “update” gate.\n",
    "    - Eliminates cell state vector\n",
    "\n",
    "<center><img src=\"img/page 51.png\" width=\"500\"></center>\n",
    "\n",
    "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$$\n",
    "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$$\n",
    "$$\\tilde{h}_t = tanh(W \\cdot [r_t * h_{t-1}, x_t])$$\n",
    "$$h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde(h)_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GRU vs. LSTM\n",
    "&nbsp;\n",
    "\n",
    "- GRU has significantly fewer parameters and trains faster.\n",
    "- Experimental results comparing the two are still inconclusive, many problems they perform the same, but each has problems on which they work better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Menti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention\n",
    "&nbsp;\n",
    "\n",
    "- For many applications, it helps to add “attention” to RNNs.\n",
    "- Allows network to learn to attend to different parts of the input at different time steps, shifting its attention to focus on different aspects during its processing.\n",
    "- Used in image captioning to focus on different parts of an image when generating different parts of the output sentence.\n",
    "- In MT, allows focusing attention on different parts of the source sentence when generating different parts of the translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention Mechanism\n",
    "&nbsp;\n",
    "\n",
    "<center><img src=\"img/page 55.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention - Scoring\n",
    "&nbsp;\n",
    "\n",
    "<center><img src=\"img/page 56_1.png\" width=\"500\"></center>\n",
    "\n",
    "$$score(h_{t-1},\\bar{h}_s) = h_t^T\\bar{h}_s$$\n",
    "\n",
    "<span style=\"color:orange;font-weight:bold\">Compare</span> target and source hidden states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention - Scoring\n",
    "&nbsp;\n",
    "\n",
    "<center><img src=\"img/page 56_2.png\" width=\"500\"></center>\n",
    "\n",
    "$$score(h_{t-1},\\bar{h}_s) = h_t^T\\bar{h}_s$$\n",
    "\n",
    "<span style=\"color:orange;font-weight:bold\">Compare</span> target and source hidden states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention - Normalization\n",
    "&nbsp;\n",
    "\n",
    "<center><img src=\"img/page 57.png\" width=\"500\"></center>\n",
    "\n",
    "$$a_t(s) = \\frac{e^{score(s)}}{\\sum_{s'}{e^{score(s')}}}$$\n",
    "\n",
    "<span style=\"color:orange;font-weight:bold\">Convert</span> into alignment weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention - Context\n",
    "&nbsp;\n",
    "\n",
    "<center><img src=\"img/page 58.png\" width=\"500\"></center>\n",
    "\n",
    "$$c_t = \\sum_s{a_t(s)\\bar{h}_s}$$\n",
    "\n",
    "Build <span style=\"color:orange;font-weight:bold\">context</span> vector: weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention - Context\n",
    "&nbsp;\n",
    "\n",
    "<center><img src=\"img/page 59.png\" width=\"500\"></center>\n",
    "\n",
    "$$h_t = f(h_{t-1}, c_t, e_t)$$\n",
    "\n",
    "Compute <span style=\"color:orange;font-weight:bold\">next</span> hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary: what did we learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\text {Time for Practical 6!}$$ "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
